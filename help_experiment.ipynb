{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "from Model import *\n",
    "from itertools import chain\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from synthetic import simulate_lorenz_96, simulate_var\n",
    "from utils import build_flags, time_split, save_result, evaluate_result, count_accuracy, loss_sparsity, loss_divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = build_flags()\n",
    "args = parser.parse_args(args=[])\n",
    "args.seed = 2\n",
    "args.num_nodes = 20\n",
    "args.dims = 1\n",
    "args.threshold = 0.5\n",
    "args.time_length = 500\n",
    "args.time_step = 10\n",
    "args.epochs = 3000\n",
    "args.batch_size = 128\n",
    "args.lr = 1e-3\n",
    "args.weight_decay = 1e-3\n",
    "args.alpha = 0.02\n",
    "args.beta_sparsity = 1   #log_sum\n",
    "args.beta_kl = 0.1        #JS散度\n",
    "args.hidden = 15\n",
    "args.dropout = 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_np, GC = simulate_lorenz_96(p=20, F=10, T=500, seed=2)\n",
    "X_np_ori = X_np\n",
    "X_np = X_np.transpose(1, 0)\n",
    "X_np = X_np[:, :, np.newaxis]\n",
    "X_np = np.array(time_split(X_np, step=10))\n",
    "X_np = torch.FloatTensor(X_np)\n",
    "data = X_np\n",
    "data_loader = DataLoader(data, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jing_xuzijian/crf/Intrer_VAE/Model.py:20: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n",
      "  nn.init.xavier_normal(m.weight.data, gain=1.414)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature: 0001 Epoch: 0000 Loss: 35.3649740219 MSE_Loss: 26.9113945961 Sparsity_loss: 8.4376585484 KL_loss: 0.1592129655 time: 0.1109s\n",
      "Feature: 0001 Epoch: 0100 Loss: 16.5646128654 MSE_Loss: 13.7968297005 Sparsity_loss: 2.7627740502 KL_loss: 0.0500849960 time: 0.0192s\n",
      "Feature: 0001 Epoch: 0200 Loss: 11.2060635090 MSE_Loss: 9.8673775196 Sparsity_loss: 1.3362399042 KL_loss: 0.0244614482 time: 0.0184s\n",
      "Feature: 0001 Epoch: 0300 Loss: 8.6542916298 MSE_Loss: 7.4600173235 Sparsity_loss: 1.1922337711 KL_loss: 0.0204063314 time: 0.0192s\n",
      "Feature: 0001 Epoch: 0400 Loss: 7.2643269300 MSE_Loss: 6.0709468126 Sparsity_loss: 1.1913311481 KL_loss: 0.0204906212 time: 0.0184s\n",
      "Feature: 0001 Epoch: 0500 Loss: 6.0865253210 MSE_Loss: 5.1390103102 Sparsity_loss: 0.9462039471 KL_loss: 0.0131115990 time: 0.0183s\n",
      "Feature: 0001 Epoch: 0600 Loss: 5.6702266932 MSE_Loss: 4.7516417503 Sparsity_loss: 0.9173251390 KL_loss: 0.0125989979 time: 0.0252s\n",
      "Feature: 0001 Epoch: 0700 Loss: 5.8422636986 MSE_Loss: 4.8944616318 Sparsity_loss: 0.9464702457 KL_loss: 0.0133180441 time: 0.0267s\n",
      "Feature: 0001 Epoch: 0800 Loss: 5.6352660656 MSE_Loss: 4.7182536125 Sparsity_loss: 0.9157465994 KL_loss: 0.0126580326 time: 0.0269s\n",
      "Feature: 0001 Epoch: 0900 Loss: 5.6633565426 MSE_Loss: 4.7344459295 Sparsity_loss: 0.9276149273 KL_loss: 0.0129550255 time: 0.0266s\n",
      "Feature: 0001 Epoch: 1000 Loss: 5.8067158461 MSE_Loss: 4.9147675037 Sparsity_loss: 0.8907242417 KL_loss: 0.0122412010 time: 0.0266s\n",
      "Feature: 0001 Epoch: 1100 Loss: 5.5301685333 MSE_Loss: 4.6237668991 Sparsity_loss: 0.9051103592 KL_loss: 0.0129132287 time: 0.0269s\n",
      "Feature: 0001 Epoch: 1200 Loss: 5.2828472853 MSE_Loss: 4.4074496031 Sparsity_loss: 0.8741855472 KL_loss: 0.0121206483 time: 0.0271s\n",
      "Feature: 0001 Epoch: 1300 Loss: 5.2609269619 MSE_Loss: 4.3645014167 Sparsity_loss: 0.8951601088 KL_loss: 0.0126538686 time: 0.0269s\n",
      "Feature: 0001 Epoch: 1400 Loss: 5.2040543556 MSE_Loss: 4.3197905421 Sparsity_loss: 0.8830194920 KL_loss: 0.0124428826 time: 0.0269s\n",
      "Feature: 0001 Epoch: 1500 Loss: 5.1592196226 MSE_Loss: 4.2950368524 Sparsity_loss: 0.8629860282 KL_loss: 0.0119676681 time: 0.0269s\n",
      "Feature: 0001 Epoch: 1600 Loss: 5.0301805735 MSE_Loss: 4.1577655673 Sparsity_loss: 0.8711856753 KL_loss: 0.0122946161 time: 0.0270s\n",
      "Feature: 0001 Epoch: 1700 Loss: 4.9159182310 MSE_Loss: 4.0358351469 Sparsity_loss: 0.8788268119 KL_loss: 0.0125627420 time: 0.0269s\n",
      "Feature: 0001 Epoch: 1800 Loss: 4.9980027676 MSE_Loss: 4.1427944899 Sparsity_loss: 0.8540177047 KL_loss: 0.0119054762 time: 0.0270s\n",
      "Feature: 0001 Epoch: 1900 Loss: 4.8538552523 MSE_Loss: 3.9882525206 Sparsity_loss: 0.8643913269 KL_loss: 0.0121136229 time: 0.0266s\n",
      "Feature: 0001 Epoch: 2000 Loss: 4.7222609520 MSE_Loss: 3.8592840433 Sparsity_loss: 0.8617476225 KL_loss: 0.0122917637 time: 0.0269s\n",
      "Feature: 0001 Epoch: 2100 Loss: 4.7767792940 MSE_Loss: 3.9185324907 Sparsity_loss: 0.8570455313 KL_loss: 0.0120123322 time: 0.0270s\n",
      "Feature: 0001 Epoch: 2200 Loss: 4.8290804625 MSE_Loss: 3.9918265939 Sparsity_loss: 0.8360890001 KL_loss: 0.0116501965 time: 0.0268s\n",
      "Feature: 0001 Epoch: 2300 Loss: 4.6101520061 MSE_Loss: 3.7661588192 Sparsity_loss: 0.8428111523 KL_loss: 0.0118212616 time: 0.0269s\n",
      "Feature: 0001 Epoch: 2400 Loss: 4.6322388649 MSE_Loss: 3.7843098044 Sparsity_loss: 0.8467247784 KL_loss: 0.0120442852 time: 0.0260s\n",
      "Feature: 0001 Epoch: 2500 Loss: 4.5522676706 MSE_Loss: 3.6981356740 Sparsity_loss: 0.8529212326 KL_loss: 0.0121069609 time: 0.0268s\n",
      "Feature: 0001 Epoch: 2600 Loss: 4.6120964289 MSE_Loss: 3.7570245266 Sparsity_loss: 0.8538420498 KL_loss: 0.0123001933 time: 0.0269s\n",
      "Feature: 0001 Epoch: 2700 Loss: 4.5638091564 MSE_Loss: 3.6989848614 Sparsity_loss: 0.8635712415 KL_loss: 0.0125300842 time: 0.0271s\n",
      "Feature: 0001 Epoch: 2800 Loss: 4.4316134453 MSE_Loss: 3.5883374810 Sparsity_loss: 0.8420839608 KL_loss: 0.0119207217 time: 0.0269s\n",
      "Feature: 0001 Epoch: 2900 Loss: 4.4390966892 MSE_Loss: 3.6004302502 Sparsity_loss: 0.8374847025 KL_loss: 0.0118174576 time: 0.0267s\n",
      "Feature: 0001 Epoch: 3000 Loss: 4.4790841341 MSE_Loss: 3.6352318525 Sparsity_loss: 0.8426544815 KL_loss: 0.0119779925 time: 0.0265s\n",
      "Feature: 0001 Epoch: 3100 Loss: 4.4207651019 MSE_Loss: 3.5756916404 Sparsity_loss: 0.8438776582 KL_loss: 0.0119579709 time: 0.0268s\n",
      "Feature: 0001 Epoch: 3200 Loss: 4.4370590448 MSE_Loss: 3.6020817757 Sparsity_loss: 0.8337944150 KL_loss: 0.0118291080 time: 0.0269s\n",
      "Feature: 0001 Epoch: 3300 Loss: 4.3990435600 MSE_Loss: 3.5487296581 Sparsity_loss: 0.8490954489 KL_loss: 0.0121843182 time: 0.0262s\n",
      "Feature: 0001 Epoch: 3400 Loss: 4.3055300713 MSE_Loss: 3.4526881576 Sparsity_loss: 0.8516223282 KL_loss: 0.0121952849 time: 0.0268s\n",
      "Feature: 0001 Epoch: 3500 Loss: 4.4210700989 MSE_Loss: 3.5674824715 Sparsity_loss: 0.8523590863 KL_loss: 0.0122852342 time: 0.0269s\n",
      "Feature: 0001 Epoch: 3600 Loss: 4.3543309569 MSE_Loss: 3.5035890937 Sparsity_loss: 0.8495407104 KL_loss: 0.0120119448 time: 0.0269s\n",
      "Feature: 0001 Epoch: 3700 Loss: 4.4511222839 MSE_Loss: 3.5991054773 Sparsity_loss: 0.8507975787 KL_loss: 0.0121921913 time: 0.0268s\n",
      "Feature: 0001 Epoch: 3800 Loss: 4.2436410189 MSE_Loss: 3.3832505941 Sparsity_loss: 0.8591620624 KL_loss: 0.0122828016 time: 0.0265s\n",
      "Feature: 0001 Epoch: 3900 Loss: 4.1234143972 MSE_Loss: 3.2517755032 Sparsity_loss: 0.8703643531 KL_loss: 0.0127443848 time: 0.0270s\n",
      "Feature: 0001 Epoch: 4000 Loss: 4.1515501738 MSE_Loss: 3.2902416587 Sparsity_loss: 0.8600792289 KL_loss: 0.0122924098 time: 0.0268s\n",
      "Feature: 0001 Epoch: 4100 Loss: 4.1449688673 MSE_Loss: 3.2670413852 Sparsity_loss: 0.8766709268 KL_loss: 0.0125643981 time: 0.0268s\n",
      "Feature: 0001 Epoch: 4200 Loss: 4.1560079455 MSE_Loss: 3.2852113843 Sparsity_loss: 0.8695565462 KL_loss: 0.0124002625 time: 0.0271s\n",
      "Feature: 0001 Epoch: 4300 Loss: 4.0614921451 MSE_Loss: 3.1910211444 Sparsity_loss: 0.8692436069 KL_loss: 0.0122738353 time: 0.0268s\n",
      "Feature: 0001 Epoch: 4400 Loss: 4.0873553753 MSE_Loss: 3.2051541805 Sparsity_loss: 0.8809168637 KL_loss: 0.0128439504 time: 0.0263s\n",
      "Feature: 0001 Epoch: 4500 Loss: 4.1661337018 MSE_Loss: 3.2899128199 Sparsity_loss: 0.8750094771 KL_loss: 0.0121147395 time: 0.0271s\n",
      "Feature: 0001 Epoch: 4600 Loss: 4.1755996943 MSE_Loss: 3.2863553166 Sparsity_loss: 0.8879746795 KL_loss: 0.0126976245 time: 0.0269s\n",
      "Feature: 0001 Epoch: 4700 Loss: 4.0225661993 MSE_Loss: 3.1565379500 Sparsity_loss: 0.8648417294 KL_loss: 0.0118664845 time: 0.0266s\n",
      "Feature: 0001 Epoch: 4800 Loss: 4.0148242116 MSE_Loss: 3.1291596293 Sparsity_loss: 0.8844355643 KL_loss: 0.0122912424 time: 0.0250s\n",
      "Feature: 0001 Epoch: 4900 Loss: 3.9061318040 MSE_Loss: 3.0165911317 Sparsity_loss: 0.8883128613 KL_loss: 0.0122777424 time: 0.0258s\n"
     ]
    }
   ],
   "source": [
    "idx = 0\n",
    "save_file = '/home/jing_xuzijian/crf/Intrer_VAE_result/help_experiment/Inter_help_coef2.pt'\n",
    "Inter_encoder = encoder(args.dims, args.hidden, args.dims, args.time_step - 1, args.num_nodes, args.dropout, args.alpha)\n",
    "Inter_encoder = Inter_encoder.cuda()\n",
    "Inter_decoder = decoder(args.dims, args.hidden, args.time_step - 1, args.num_nodes, args.dropout, args.alpha)\n",
    "Inter_decoder = Inter_decoder.cuda()\n",
    "optimizer = optim.Adam(params = chain(Inter_encoder.parameters(), Inter_decoder.parameters()), lr=args.lr, weight_decay=args.weight_decay)\n",
    "loss_mse = nn.MSELoss()\n",
    "best_loss = np.inf\n",
    "for epoch in range(5000):\n",
    "    t = time.time()\n",
    "    Loss = []\n",
    "    MSE_loss = []\n",
    "    SPA_loss = []\n",
    "    KL_loss = []\n",
    "    for batch_idx, data in enumerate(data_loader):\n",
    "        optimizer.zero_grad()\n",
    "        data = data.cuda()\n",
    "        target = data[:, idx, 1:, :]\n",
    "        inputs = data[:, :, :-1, :]\n",
    "\n",
    "        mu, log_var = Inter_encoder(inputs)  #Inter_encoder(inputs, adj)\n",
    "        sigma = torch.exp(log_var / 2)\n",
    "        gamma = torch.randn(size = mu.size()).cuda()\n",
    "        gamma = mu + sigma * gamma\n",
    "        mask = torch.sigmoid(gamma) #* torch.sigmoid(theta + gamma)\n",
    "\n",
    "        inputs = mask_inputs(mask, inputs)\n",
    "        pred = Inter_decoder(inputs, idx)   #Inter_decoder(inputs, adj, idx)\n",
    "\n",
    "\n",
    "\n",
    "        mse_loss = loss_mse(pred, target)\n",
    "        spa_loss = loss_sparsity(mask, 'log_sum')\n",
    "        kl_loss = loss_divergence(mask, 'JS')\n",
    "\n",
    "        loss = mse_loss + args.beta_sparsity * spa_loss + args.beta_kl * kl_loss\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        Loss.append(loss.item())\n",
    "        MSE_loss.append(mse_loss.item())\n",
    "        SPA_loss.append(spa_loss.item())\n",
    "        KL_loss.append(kl_loss.item())\n",
    "    \n",
    "    if epoch == 500:\n",
    "        optimizer.param_groups[0]['lr'] = args.lr/10\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print(  'Feature: {:04d}'.format(idx + 1),\n",
    "                'Epoch: {:04d}'.format(epoch),\n",
    "                'Loss: {:.10f}'.format(np.mean(Loss)),\n",
    "                'MSE_Loss: {:.10f}'.format(np.mean(MSE_loss)),\n",
    "                'Sparsity_loss: {:.10f}'.format(np.mean(SPA_loss)),\n",
    "                'KL_loss: {:.10f}'.format(np.mean(KL_loss)),\n",
    "                'time: {:.4f}s'.format(time.time() - t))\n",
    "            \n",
    "    if np.mean(Loss) < best_loss:\n",
    "        best_loss = np.mean(Loss)\n",
    "            # M[idx, :] = adj[idx, :]\n",
    "            # gamma_matrix[idx, :] = gamma\n",
    "            # theta_matrix[idx, :] = theta\n",
    "        torch.save({\n",
    "                    'encoder_state_dict': Inter_encoder.state_dict(),\n",
    "                    'decoder_state_dict': Inter_decoder.state_dict(),\n",
    "                        # 'adj' : adj\n",
    "\n",
    "                    }, save_file)\n",
    "            # np.save(save_file + str(idx) + '.npy', mask.cpu().detach().numpy())\n",
    "\n",
    "        # print('Feature: {:04d}'.format(idx + 1),\n",
    "        #       'Epoch: {:04d}'.format(epoch),\n",
    "        #       'Loss: {:.10f}'.format(np.mean(Loss)),\n",
    "        #       'mse_loss: {:.10f}'.format(np.mean(MSE_loss)),\n",
    "        #       'Sparsity_loss: {:.10f}'.format(np.mean(SPA_loss)),\n",
    "        #       'KL_loss: {:.10f}'.format(np.mean(KL_loss)),\n",
    "        #       #'mmd_loss: {:.10f}'.format(np.mean(mmd_loss)),\n",
    "        #       # 'time: {:.4f}s'.format(time.time() - t), file=log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 3.6874e+00,  1.5899e-01,  4.2904e-02, -4.9666e-02, -3.7316e-02,\n",
      "         1.5473e-03,  1.3185e-02, -1.2098e-02,  9.3266e-03, -6.6691e-03,\n",
      "         1.7035e-03,  2.3604e-02,  1.6201e-02, -1.5379e-02,  9.8692e-03,\n",
      "         6.3649e-03,  2.5113e-02,  1.4795e-02, -3.8504e-01, -7.8076e-02],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([[-0.0245,  0.1380, -0.8680,  1.0253, -2.4208, -2.8583,  1.6075, -0.5942,\n",
      "          0.0422, -5.0181, -4.2502, -1.2880,  3.8038, -3.3679, -2.8081, -1.0867,\n",
      "          1.1769,  0.8595,  0.1737, -2.9083],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000]], device='cuda:0')\n",
      "[1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "print(Inter_decoder.adj[idx, :])\n",
    "print(Inter_decoder.adj.grad)\n",
    "print(GC[idx, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jing_xuzijian/crf/Intrer_VAE/Model.py:20: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n",
      "  nn.init.xavier_normal(m.weight.data, gain=1.414)\n"
     ]
    }
   ],
   "source": [
    "help_encoder_net = encoder(args.dims, args.hidden, args.dims, args.time_step - 1, args.num_nodes, args.dropout, args.alpha)\n",
    "help_decoder_net = decoder(args.dims, args.hidden, args.time_step -1, args.num_nodes, args.dropout, args.alpha)\n",
    "checkpoint = torch.load('/home/jing_xuzijian/crf/Intrer_VAE_result/help_experiment/Inter_help_coef2.pt')\n",
    "help_encoder_net.load_state_dict(checkpoint['encoder_state_dict'])\n",
    "help_decoder_net.load_state_dict(checkpoint['decoder_state_dict'])\n",
    "help_encoder_net.eval()\n",
    "help_decoder_net.eval()\n",
    "mu, log_var = help_encoder_net(X_np[:, :, :-1, :])\n",
    "sigma = torch.exp(log_var / 2)\n",
    "gamma = torch.randn(size = mu.size())\n",
    "gamma = mu + sigma * gamma\n",
    "mask = torch.sigmoid(gamma)\n",
    "mask = mask.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([491, 20, 9])\n"
     ]
    }
   ],
   "source": [
    "print(mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adj\n",
      "mlp1.fc1.weight\n",
      "mlp1.fc1.bias\n",
      "mlp1.fc2.weight\n",
      "mlp1.fc2.bias\n",
      "mlp1.bn.weight\n",
      "mlp1.bn.bias\n",
      "mlp2.fc1.weight\n",
      "mlp2.fc1.bias\n",
      "mlp2.fc2.weight\n",
      "mlp2.fc2.bias\n",
      "mlp2.bn.weight\n",
      "mlp2.bn.bias\n",
      "gat.W\n",
      "gat.a\n",
      "fc.weight\n",
      "fc.bias\n",
      "bn.weight\n",
      "bn.bias\n"
     ]
    }
   ],
   "source": [
    "for para in help_decoder_net.named_parameters():\n",
    "    print(para[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(help_decoder_net.bn.weight.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[8.1529e-01, 7.5017e-01, 8.0229e-01, 8.1132e-01, 8.0489e-01, 8.2746e-01,\n",
      "         8.4336e-01, 7.7390e-01, 8.5451e-01, 8.1560e-01, 7.7644e-01, 8.4206e-01,\n",
      "         8.1700e-01, 8.0074e-01, 8.5518e-01, 7.7297e-01, 8.6488e-01, 7.9932e-01,\n",
      "         8.4872e-01, 7.5807e-01],\n",
      "        [4.9340e-42, 4.9340e-42, 4.9340e-42, 4.9340e-42, 4.9340e-42, 4.9340e-42,\n",
      "         4.9340e-42, 4.9340e-42, 4.9340e-42, 4.9340e-42, 4.9340e-42, 4.9340e-42,\n",
      "         4.9340e-42, 4.9340e-42, 4.9340e-42, 4.9340e-42, 4.9340e-42, 4.9340e-42,\n",
      "         4.9340e-42, 4.9340e-42],\n",
      "        [4.9340e-42, 4.9340e-42, 4.9340e-42, 4.9340e-42, 4.9340e-42, 4.9340e-42,\n",
      "         4.9340e-42, 4.9340e-42, 4.9340e-42, 4.9340e-42, 4.9340e-42, 4.9340e-42,\n",
      "         4.9340e-42, 4.9340e-42, 4.9340e-42, 4.9340e-42, 4.9340e-42, 4.9340e-42,\n",
      "         4.9340e-42, 4.9340e-42],\n",
      "        [4.9340e-42, 4.9340e-42, 4.9340e-42, 4.9340e-42, 4.9340e-42, 4.9340e-42,\n",
      "         4.9340e-42, 4.9340e-42, 4.9340e-42, 4.9340e-42, 4.9340e-42, 4.9340e-42,\n",
      "         4.9340e-42, 4.9340e-42, 4.9340e-42, 4.9340e-42, 4.9340e-42, 4.9340e-42,\n",
      "         4.9340e-42, 4.9340e-42],\n",
      "        [4.9340e-42, 4.9340e-42, 4.9340e-42, 4.9340e-42, 4.9340e-42, 4.9340e-42,\n",
      "         4.9340e-42, 4.9340e-42, 4.9340e-42, 4.9340e-42, 4.9340e-42, 4.9340e-42,\n",
      "         4.9340e-42, 4.9340e-42, 4.9340e-42, 4.9340e-42, 4.9340e-42, 4.9340e-42,\n",
      "         4.9340e-42, 4.9340e-42],\n",
      "        [4.9340e-42, 4.9340e-42, 4.9340e-42, 4.9340e-42, 4.9340e-42, 4.9340e-42,\n",
      "         4.9340e-42, 4.9340e-42, 4.9340e-42, 4.9340e-42, 4.9340e-42, 4.9340e-42,\n",
      "         4.9340e-42, 4.9340e-42, 4.9340e-42, 4.9340e-42, 4.9340e-42, 4.9340e-42,\n",
      "         4.9340e-42, 4.9340e-42],\n",
      "        [4.9340e-42, 4.9340e-42, 4.9340e-42, 4.9340e-42, 4.9340e-42, 4.9340e-42,\n",
      "         4.9340e-42, 4.9340e-42, 4.9340e-42, 4.9340e-42, 4.9340e-42, 4.9340e-42,\n",
      "         4.9340e-42, 4.9340e-42, 4.9340e-42, 4.9340e-42, 4.9340e-42, 4.9340e-42,\n",
      "         4.9340e-42, 4.9340e-42],\n",
      "        [4.9340e-42, 4.9340e-42, 4.9340e-42, 4.9340e-42, 4.9340e-42, 4.9340e-42,\n",
      "         4.9340e-42, 4.9340e-42, 4.9340e-42, 4.9340e-42, 4.9340e-42, 4.9340e-42,\n",
      "         4.9340e-42, 4.9340e-42, 4.9340e-42, 4.9340e-42, 4.9340e-42, 4.9340e-42,\n",
      "         4.9340e-42, 4.9340e-42],\n",
      "        [4.9340e-42, 4.9340e-42, 4.9340e-42, 4.9340e-42, 4.9340e-42, 4.9340e-42,\n",
      "         4.9340e-42, 4.9340e-42, 4.9340e-42, 4.9340e-42, 4.9340e-42, 4.9340e-42,\n",
      "         4.9340e-42, 4.9340e-42, 4.9340e-42, 4.9340e-42, 4.9340e-42, 4.9340e-42,\n",
      "         4.9340e-42, 4.9340e-42],\n",
      "        [4.9340e-42, 4.9340e-42, 4.9340e-42, 4.9340e-42, 4.9340e-42, 4.9340e-42,\n",
      "         4.9340e-42, 4.9340e-42, 4.9340e-42, 4.9340e-42, 4.9340e-42, 4.9340e-42,\n",
      "         4.9340e-42, 4.9340e-42, 4.9340e-42, 4.9340e-42, 4.9340e-42, 4.9340e-42,\n",
      "         4.9340e-42, 4.9340e-42],\n",
      "        [4.9340e-42, 4.9340e-42, 4.9340e-42, 4.9340e-42, 4.9340e-42, 4.9340e-42,\n",
      "         4.9340e-42, 4.9340e-42, 4.9340e-42, 4.9340e-42, 4.9340e-42, 4.9340e-42,\n",
      "         4.9340e-42, 4.9340e-42, 4.9340e-42, 4.9340e-42, 4.9340e-42, 4.9340e-42,\n",
      "         4.9340e-42, 4.9340e-42],\n",
      "        [4.9340e-42, 4.9340e-42, 4.9340e-42, 4.9340e-42, 4.9340e-42, 4.9340e-42,\n",
      "         4.9340e-42, 4.9340e-42, 4.9340e-42, 4.9340e-42, 4.9340e-42, 4.9340e-42,\n",
      "         4.9340e-42, 4.9340e-42, 4.9340e-42, 4.9340e-42, 4.9340e-42, 4.9340e-42,\n",
      "         4.9340e-42, 4.9340e-42],\n",
      "        [4.9340e-42, 4.9340e-42, 4.9340e-42, 4.9340e-42, 4.9340e-42, 4.9340e-42,\n",
      "         4.9340e-42, 4.9340e-42, 4.9340e-42, 4.9340e-42, 4.9340e-42, 4.9340e-42,\n",
      "         4.9340e-42, 4.9340e-42, 4.9340e-42, 4.9340e-42, 4.9340e-42, 4.9340e-42,\n",
      "         4.9340e-42, 4.9340e-42],\n",
      "        [4.9340e-42, 4.9340e-42, 4.9340e-42, 4.9340e-42, 4.9340e-42, 4.9340e-42,\n",
      "         4.9340e-42, 4.9340e-42, 4.9340e-42, 4.9340e-42, 4.9340e-42, 4.9340e-42,\n",
      "         4.9340e-42, 4.9340e-42, 4.9340e-42, 4.9340e-42, 4.9340e-42, 4.9340e-42,\n",
      "         4.9340e-42, 4.9340e-42],\n",
      "        [4.9340e-42, 4.9340e-42, 4.9340e-42, 4.9340e-42, 4.9340e-42, 4.9340e-42,\n",
      "         4.9340e-42, 4.9340e-42, 4.9340e-42, 4.9340e-42, 4.9340e-42, 4.9340e-42,\n",
      "         4.9340e-42, 4.9340e-42, 4.9340e-42, 4.9340e-42, 4.9340e-42, 4.9340e-42,\n",
      "         4.9340e-42, 4.9340e-42],\n",
      "        [4.9340e-42, 4.9340e-42, 4.9340e-42, 4.9340e-42, 4.9340e-42, 4.9340e-42,\n",
      "         4.9340e-42, 4.9340e-42, 4.9340e-42, 4.9340e-42, 4.9340e-42, 4.9340e-42,\n",
      "         4.9340e-42, 4.9340e-42, 4.9340e-42, 4.9340e-42, 4.9340e-42, 4.9340e-42,\n",
      "         4.9340e-42, 4.9340e-42],\n",
      "        [4.9340e-42, 4.9340e-42, 4.9340e-42, 4.9340e-42, 4.9340e-42, 4.9340e-42,\n",
      "         4.9340e-42, 4.9340e-42, 4.9340e-42, 4.9340e-42, 4.9340e-42, 4.9340e-42,\n",
      "         4.9340e-42, 4.9340e-42, 4.9340e-42, 4.9340e-42, 4.9340e-42, 4.9340e-42,\n",
      "         4.9340e-42, 4.9340e-42],\n",
      "        [4.9340e-42, 4.9340e-42, 4.9340e-42, 4.9340e-42, 4.9340e-42, 4.9340e-42,\n",
      "         4.9340e-42, 4.9340e-42, 4.9340e-42, 4.9340e-42, 4.9340e-42, 4.9340e-42,\n",
      "         4.9340e-42, 4.9340e-42, 4.9340e-42, 4.9340e-42, 4.9340e-42, 4.9340e-42,\n",
      "         4.9340e-42, 4.9340e-42],\n",
      "        [4.9340e-42, 4.9340e-42, 4.9340e-42, 4.9340e-42, 4.9340e-42, 4.9340e-42,\n",
      "         4.9340e-42, 4.9340e-42, 4.9340e-42, 4.9340e-42, 4.9340e-42, 4.9340e-42,\n",
      "         4.9340e-42, 4.9340e-42, 4.9340e-42, 4.9340e-42, 4.9340e-42, 4.9340e-42,\n",
      "         4.9340e-42, 4.9340e-42],\n",
      "        [4.9340e-42, 4.9340e-42, 4.9340e-42, 4.9340e-42, 4.9340e-42, 4.9340e-42,\n",
      "         4.9340e-42, 4.9340e-42, 4.9340e-42, 4.9340e-42, 4.9340e-42, 4.9340e-42,\n",
      "         4.9340e-42, 4.9340e-42, 4.9340e-42, 4.9340e-42, 4.9340e-42, 4.9340e-42,\n",
      "         4.9340e-42, 4.9340e-42]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(help_decoder_net.adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0420, 0.0410, 0.0423, 0.0397, 0.0426, 0.0422, 0.0425, 0.0454, 0.0425,\n",
      "        0.0435, 0.0448, 0.0433, 0.0417, 0.0458, 0.0447, 0.0428, 0.0420, 0.0408,\n",
      "        0.0453, 0.0447], grad_fn=<MeanBackward1>)\n",
      "[1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "eva = mask.mean(dim=0).mean(dim=1)\n",
    "print(eva)\n",
    "print(GC[0, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin training feature: 0002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jing_xuzijian/crf/Intrer_VAE/Model.py:20: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n",
      "  nn.init.xavier_normal(m.weight.data, gain=1.414)\n",
      "/home/jing_xuzijian/anaconda3/envs/crf_base/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature: 0002 Epoch: 0000 Loss: 38.1945438385 MSE_Loss: 38.1945438385 time: 0.0859s\n",
      "Feature: 0002 Epoch: 0100 Loss: 22.2226710320 MSE_Loss: 22.2226710320 time: 0.0085s\n",
      "Feature: 0002 Epoch: 0200 Loss: 16.2656733990 MSE_Loss: 16.2656733990 time: 0.0090s\n",
      "Feature: 0002 Epoch: 0300 Loss: 12.2572782040 MSE_Loss: 12.2572782040 time: 0.0090s\n",
      "Feature: 0002 Epoch: 0400 Loss: 9.9791700840 MSE_Loss: 9.9791700840 time: 0.0090s\n",
      "Feature: 0002 Epoch: 0500 Loss: 8.6275345087 MSE_Loss: 8.6275345087 time: 0.0090s\n",
      "Feature: 0002 Epoch: 0600 Loss: 8.8666521311 MSE_Loss: 8.8666521311 time: 0.0090s\n",
      "Feature: 0002 Epoch: 0700 Loss: 7.7709715366 MSE_Loss: 7.7709715366 time: 0.0090s\n",
      "Feature: 0002 Epoch: 0800 Loss: 7.8987344503 MSE_Loss: 7.8987344503 time: 0.0090s\n",
      "Feature: 0002 Epoch: 0900 Loss: 8.0406353474 MSE_Loss: 8.0406353474 time: 0.0090s\n",
      "Feature: 0002 Epoch: 1000 Loss: 7.7239035368 MSE_Loss: 7.7239035368 time: 0.0090s\n",
      "Feature: 0002 Epoch: 1100 Loss: 7.4051609039 MSE_Loss: 7.4051609039 time: 0.0084s\n",
      "Feature: 0002 Epoch: 1200 Loss: 8.2516541481 MSE_Loss: 8.2516541481 time: 0.0090s\n",
      "Feature: 0002 Epoch: 1300 Loss: 8.3117307425 MSE_Loss: 8.3117307425 time: 0.0090s\n",
      "Feature: 0002 Epoch: 1400 Loss: 7.6646273136 MSE_Loss: 7.6646273136 time: 0.0085s\n",
      "Feature: 0002 Epoch: 1500 Loss: 7.6517332792 MSE_Loss: 7.6517332792 time: 0.0090s\n",
      "Feature: 0002 Epoch: 1600 Loss: 6.8902142048 MSE_Loss: 6.8902142048 time: 0.0090s\n",
      "Feature: 0002 Epoch: 1700 Loss: 7.0292762518 MSE_Loss: 7.0292762518 time: 0.0090s\n",
      "Feature: 0002 Epoch: 1800 Loss: 7.6111145020 MSE_Loss: 7.6111145020 time: 0.0090s\n",
      "Feature: 0002 Epoch: 1900 Loss: 7.7844858170 MSE_Loss: 7.7844858170 time: 0.0055s\n",
      "Feature: 0002 Epoch: 2000 Loss: 7.1731927395 MSE_Loss: 7.1731927395 time: 0.0053s\n",
      "Feature: 0002 Epoch: 2100 Loss: 7.8469612598 MSE_Loss: 7.8469612598 time: 0.0053s\n",
      "Feature: 0002 Epoch: 2200 Loss: 6.9507002831 MSE_Loss: 6.9507002831 time: 0.0054s\n",
      "Feature: 0002 Epoch: 2300 Loss: 7.6768299341 MSE_Loss: 7.6768299341 time: 0.0054s\n",
      "Feature: 0002 Epoch: 2400 Loss: 6.9988696575 MSE_Loss: 6.9988696575 time: 0.0054s\n",
      "Feature: 0002 Epoch: 2500 Loss: 7.5074361563 MSE_Loss: 7.5074361563 time: 0.0054s\n",
      "Feature: 0002 Epoch: 2600 Loss: 6.6184591055 MSE_Loss: 6.6184591055 time: 0.0054s\n",
      "Feature: 0002 Epoch: 2700 Loss: 7.3131858110 MSE_Loss: 7.3131858110 time: 0.0054s\n",
      "Feature: 0002 Epoch: 2800 Loss: 6.6502262354 MSE_Loss: 6.6502262354 time: 0.0054s\n",
      "Feature: 0002 Epoch: 2900 Loss: 6.5029345751 MSE_Loss: 6.5029345751 time: 0.0054s\n",
      "Feature: 0002 Epoch: 3000 Loss: 7.3243094683 MSE_Loss: 7.3243094683 time: 0.0054s\n",
      "Feature: 0002 Epoch: 3100 Loss: 7.0639951229 MSE_Loss: 7.0639951229 time: 0.0054s\n",
      "Feature: 0002 Epoch: 3200 Loss: 6.6799418926 MSE_Loss: 6.6799418926 time: 0.0054s\n",
      "Feature: 0002 Epoch: 3300 Loss: 7.0962735415 MSE_Loss: 7.0962735415 time: 0.0054s\n",
      "Feature: 0002 Epoch: 3400 Loss: 7.9474046230 MSE_Loss: 7.9474046230 time: 0.0054s\n",
      "Feature: 0002 Epoch: 3500 Loss: 6.6090297699 MSE_Loss: 6.6090297699 time: 0.0054s\n",
      "Feature: 0002 Epoch: 3600 Loss: 6.1848757267 MSE_Loss: 6.1848757267 time: 0.0054s\n",
      "Feature: 0002 Epoch: 3700 Loss: 7.0756926537 MSE_Loss: 7.0756926537 time: 0.0053s\n",
      "Feature: 0002 Epoch: 3800 Loss: 7.5078508854 MSE_Loss: 7.5078508854 time: 0.0054s\n",
      "Feature: 0002 Epoch: 3900 Loss: 7.0730698109 MSE_Loss: 7.0730698109 time: 0.0054s\n",
      "Feature: 0002 Epoch: 4000 Loss: 8.0881038904 MSE_Loss: 8.0881038904 time: 0.0054s\n",
      "Feature: 0002 Epoch: 4100 Loss: 7.2056992054 MSE_Loss: 7.2056992054 time: 0.0054s\n",
      "Feature: 0002 Epoch: 4200 Loss: 7.4092810154 MSE_Loss: 7.4092810154 time: 0.0054s\n",
      "Feature: 0002 Epoch: 4300 Loss: 6.8564128876 MSE_Loss: 6.8564128876 time: 0.0054s\n",
      "Feature: 0002 Epoch: 4400 Loss: 6.6747804880 MSE_Loss: 6.6747804880 time: 0.0054s\n",
      "Feature: 0002 Epoch: 4500 Loss: 6.2914280891 MSE_Loss: 6.2914280891 time: 0.0054s\n",
      "Feature: 0002 Epoch: 4600 Loss: 7.6403651237 MSE_Loss: 7.6403651237 time: 0.0054s\n",
      "Feature: 0002 Epoch: 4700 Loss: 6.7639937401 MSE_Loss: 6.7639937401 time: 0.0054s\n",
      "Feature: 0002 Epoch: 4800 Loss: 6.9550646544 MSE_Loss: 6.9550646544 time: 0.0054s\n",
      "Feature: 0002 Epoch: 4900 Loss: 6.5276389122 MSE_Loss: 6.5276389122 time: 0.0054s\n"
     ]
    }
   ],
   "source": [
    "idx = 1\n",
    "print('Begin training feature: {:04d}'.format(idx + 1))\n",
    "# decoder_file = os.path.join(save_folder, 'decoder_' + str(idx ) + '.pt')\n",
    "Inter_decoder = decoder(args.dims, args.hidden, args.time_step - 1, args.num_nodes, args.dropout, args.alpha)\n",
    "Inter_decoder = Inter_decoder.cuda()\n",
    "optimizer = optim.Adam(params = Inter_decoder.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=1000, gamma=0.5)\n",
    "loss_val = nn.MSELoss()\n",
    "best_loss = np.Inf\n",
    "for epoch in range(5000):\n",
    "    scheduler.step()\n",
    "    t = time.time()\n",
    "    Loss = []\n",
    "    mse_loss = []\n",
    "    for batch_idx, data in enumerate(data_loader):\n",
    "        data = data.cuda()\n",
    "        target = data[:, idx, 1:, :]\n",
    "        optimizer.zero_grad()\n",
    "        inputs = data[:, :, :-1, :]\n",
    "        pred = Inter_decoder(inputs, idx)\n",
    "        mse = loss_val(pred, target)\n",
    "        loss = mse\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "        Loss.append(loss.item())\n",
    "        mse_loss.append(mse.item())\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print('Feature: {:04d}'.format(idx + 1),\n",
    "              'Epoch: {:04d}'.format(epoch),\n",
    "              'Loss: {:.10f}'.format(np.mean(Loss)),\n",
    "              'MSE_Loss: {:.10f}'.format(np.mean(mse_loss)),\n",
    "              'time: {:.4f}s'.format(time.time() - t))\n",
    "\n",
    "        \n",
    "    # if np.mean(mse_loss) < best_loss:\n",
    "    #     best_loss = np.mean(mse_loss)\n",
    "    #     torch.save(Inter_decoder.state_dict(), decoder_file)\n",
    "    #     print('Feature: {:04d}'.format(idx + 1),\n",
    "    #           'Epoch: {:04d}'.format(epoch),\n",
    "    #           'Loss: {:.10f}'.format(np.mean(Loss)),\n",
    "    #           'mse_loss: {:.10f}'.format(np.mean(mse_loss)),\n",
    "    #           'mmd_loss: {:.10f}'.format(np.mean(mmd_loss)),\n",
    "    #           'time: {:.4f}s'.format(time.time() - t), file=log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.1446e-01,  3.5145e+00,  4.9145e-02, -2.6867e-02, -2.2781e-02,\n",
      "        -1.9490e-02, -4.3720e-03, -3.7241e-03, -2.5528e-03, -5.8850e-03,\n",
      "        -2.0816e-03, -1.3717e-02,  1.0860e-02, -5.9835e-03,  7.0344e-03,\n",
      "         2.2493e-02, -5.7498e-03,  3.7945e-02,  2.3060e-02, -7.1532e-01],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "[1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "print(Inter_decoder.adj[idx, :])\n",
    "print(GC[idx, :])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crf_base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
