{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "from Model import *\n",
    "from itertools import chain\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from synthetic import simulate_lorenz_96, simulate_var\n",
    "from utils import build_flags, time_split, save_result, evaluate_result, count_accuracy, loss_sparsity, loss_divergence, loss_mmd, save_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = build_flags()\n",
    "args = parser.parse_args(args=[])\n",
    "args.seed = 2\n",
    "args.num_nodes = 10\n",
    "args.dims = 1\n",
    "args.threshold = 0.5\n",
    "args.time_length = 500\n",
    "args.time_step = 10\n",
    "args.epochs = 3000\n",
    "args.batch_size = 128\n",
    "args.lr = 1e-3\n",
    "args.weight_decay = 1e-3\n",
    "args.encoder_alpha = 0.02\n",
    "args.decoder_alpha = 0.04\n",
    "args.beta_sparsity = 0.30 #0.25   #log_sum\n",
    "args.beta_kl = 0.1        #JS散度\n",
    "args.beta_mmd = 0.5  #1      #MMD\n",
    "args.encoder_hidden = 20\n",
    "args.decoder_hidden = 15 #20\n",
    "args.encoder_dropout = 0.1\n",
    "args.decoder_dropout = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_np, GC = simulate_lorenz_96(p=10, F=10, T=500, seed=2)\n",
    "X_np_ori = X_np\n",
    "X_np = X_np.transpose(1, 0)\n",
    "X_np = X_np[:, :, np.newaxis]\n",
    "X_np = np.array(time_split(X_np, step=10))\n",
    "X_np = torch.FloatTensor(X_np)\n",
    "data = X_np\n",
    "data_loader = DataLoader(data, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin training feature: 0001\n",
      "Feature: 0001 Epoch: 0000 Loss: 22.3030369282 MSE_Loss: 22.3030369282 time: 0.0261s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/omnisky/Public/ChenRongfa/Intrer_VAE/Model.py:20: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n",
      "  nn.init.xavier_normal(m.weight.data, gain=1.414)\n",
      "/home/omnisky/anaconda3/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature: 0001 Epoch: 0100 Loss: 13.9059495926 MSE_Loss: 13.9059495926 time: 0.0233s\n",
      "Feature: 0001 Epoch: 0200 Loss: 9.4894406796 MSE_Loss: 9.4894406796 time: 0.0202s\n",
      "Feature: 0001 Epoch: 0300 Loss: 7.0740630627 MSE_Loss: 7.0740630627 time: 0.0164s\n",
      "Feature: 0001 Epoch: 0400 Loss: 6.1979525089 MSE_Loss: 6.1979525089 time: 0.0163s\n",
      "Feature: 0001 Epoch: 0500 Loss: 4.7015128732 MSE_Loss: 4.7015128732 time: 0.0230s\n",
      "Feature: 0001 Epoch: 0600 Loss: 4.8551487327 MSE_Loss: 4.8551487327 time: 0.0232s\n",
      "Feature: 0001 Epoch: 0700 Loss: 3.7697503567 MSE_Loss: 3.7697503567 time: 0.0232s\n",
      "Feature: 0001 Epoch: 0800 Loss: 4.3773422837 MSE_Loss: 4.3773422837 time: 0.0224s\n",
      "Feature: 0001 Epoch: 0900 Loss: 4.0970312953 MSE_Loss: 4.0970312953 time: 0.0227s\n",
      "Feature: 0001 Epoch: 1000 Loss: 3.9361172915 MSE_Loss: 3.9361172915 time: 0.0228s\n",
      "Feature: 0001 Epoch: 1100 Loss: 4.5730556250 MSE_Loss: 4.5730556250 time: 0.0233s\n",
      "Feature: 0001 Epoch: 1200 Loss: 4.4327391386 MSE_Loss: 4.4327391386 time: 0.0230s\n",
      "Feature: 0001 Epoch: 1300 Loss: 4.0434225798 MSE_Loss: 4.0434225798 time: 0.0230s\n",
      "Feature: 0001 Epoch: 1400 Loss: 4.0898090005 MSE_Loss: 4.0898090005 time: 0.0232s\n",
      "Feature: 0001 Epoch: 1500 Loss: 4.6277936697 MSE_Loss: 4.6277936697 time: 0.0233s\n",
      "Feature: 0001 Epoch: 1600 Loss: 3.8204818964 MSE_Loss: 3.8204818964 time: 0.0173s\n",
      "Feature: 0001 Epoch: 1700 Loss: 3.9470351338 MSE_Loss: 3.9470351338 time: 0.0231s\n",
      "Feature: 0001 Epoch: 1800 Loss: 3.7172019482 MSE_Loss: 3.7172019482 time: 0.0230s\n",
      "Feature: 0001 Epoch: 1900 Loss: 4.0219470859 MSE_Loss: 4.0219470859 time: 0.0228s\n",
      "Feature: 0001 Epoch: 2000 Loss: 3.6256679296 MSE_Loss: 3.6256679296 time: 0.0228s\n",
      "Feature: 0001 Epoch: 2100 Loss: 3.6403985620 MSE_Loss: 3.6403985620 time: 0.0228s\n",
      "Feature: 0001 Epoch: 2200 Loss: 3.5048491955 MSE_Loss: 3.5048491955 time: 0.0232s\n",
      "Feature: 0001 Epoch: 2300 Loss: 3.8109709620 MSE_Loss: 3.8109709620 time: 0.0232s\n",
      "Feature: 0001 Epoch: 2400 Loss: 3.7743187547 MSE_Loss: 3.7743187547 time: 0.0234s\n",
      "Feature: 0001 Epoch: 2500 Loss: 3.8620324135 MSE_Loss: 3.8620324135 time: 0.0233s\n",
      "Feature: 0001 Epoch: 2600 Loss: 3.8155181408 MSE_Loss: 3.8155181408 time: 0.0232s\n",
      "Feature: 0001 Epoch: 2700 Loss: 3.9753991365 MSE_Loss: 3.9753991365 time: 0.0166s\n",
      "Feature: 0001 Epoch: 2800 Loss: 3.5527696013 MSE_Loss: 3.5527696013 time: 0.0231s\n",
      "Feature: 0001 Epoch: 2900 Loss: 3.6342359185 MSE_Loss: 3.6342359185 time: 0.0194s\n",
      "Begin training feature: 0002\n",
      "Feature: 0002 Epoch: 0000 Loss: 25.2154664993 MSE_Loss: 25.2154664993 time: 0.0237s\n",
      "Feature: 0002 Epoch: 0100 Loss: 16.1716759205 MSE_Loss: 16.1716759205 time: 0.0241s\n",
      "Feature: 0002 Epoch: 0200 Loss: 13.7462277412 MSE_Loss: 13.7462277412 time: 0.0235s\n",
      "Feature: 0002 Epoch: 0300 Loss: 10.6686823368 MSE_Loss: 10.6686823368 time: 0.0231s\n",
      "Feature: 0002 Epoch: 0400 Loss: 7.1652706861 MSE_Loss: 7.1652706861 time: 0.0239s\n",
      "Feature: 0002 Epoch: 0500 Loss: 6.0212723017 MSE_Loss: 6.0212723017 time: 0.0239s\n",
      "Feature: 0002 Epoch: 0600 Loss: 5.3910484314 MSE_Loss: 5.3910484314 time: 0.0234s\n",
      "Feature: 0002 Epoch: 0700 Loss: 5.3193666935 MSE_Loss: 5.3193666935 time: 0.0232s\n",
      "Feature: 0002 Epoch: 0800 Loss: 4.3841871023 MSE_Loss: 4.3841871023 time: 0.0234s\n",
      "Feature: 0002 Epoch: 0900 Loss: 4.8907938004 MSE_Loss: 4.8907938004 time: 0.0230s\n",
      "Feature: 0002 Epoch: 1000 Loss: 4.6076150537 MSE_Loss: 4.6076150537 time: 0.0229s\n",
      "Feature: 0002 Epoch: 1100 Loss: 4.8538115025 MSE_Loss: 4.8538115025 time: 0.0232s\n",
      "Feature: 0002 Epoch: 1200 Loss: 4.7227010131 MSE_Loss: 4.7227010131 time: 0.0223s\n",
      "Feature: 0002 Epoch: 1300 Loss: 4.5906784534 MSE_Loss: 4.5906784534 time: 0.0244s\n",
      "Feature: 0002 Epoch: 1400 Loss: 4.7085366249 MSE_Loss: 4.7085366249 time: 0.0230s\n",
      "Feature: 0002 Epoch: 1500 Loss: 3.9832017422 MSE_Loss: 3.9832017422 time: 0.0233s\n",
      "Feature: 0002 Epoch: 1600 Loss: 4.7784206271 MSE_Loss: 4.7784206271 time: 0.0199s\n",
      "Feature: 0002 Epoch: 1700 Loss: 4.3025122285 MSE_Loss: 4.3025122285 time: 0.0230s\n",
      "Feature: 0002 Epoch: 1800 Loss: 4.3909524679 MSE_Loss: 4.3909524679 time: 0.0232s\n",
      "Feature: 0002 Epoch: 1900 Loss: 3.8664258718 MSE_Loss: 3.8664258718 time: 0.0230s\n",
      "Feature: 0002 Epoch: 2000 Loss: 4.2521250248 MSE_Loss: 4.2521250248 time: 0.0231s\n",
      "Feature: 0002 Epoch: 2100 Loss: 3.9299339652 MSE_Loss: 3.9299339652 time: 0.0229s\n",
      "Feature: 0002 Epoch: 2200 Loss: 4.1133927703 MSE_Loss: 4.1133927703 time: 0.0202s\n",
      "Feature: 0002 Epoch: 2300 Loss: 4.1979166865 MSE_Loss: 4.1979166865 time: 0.0207s\n",
      "Feature: 0002 Epoch: 2400 Loss: 3.8821234703 MSE_Loss: 3.8821234703 time: 0.0230s\n",
      "Feature: 0002 Epoch: 2500 Loss: 3.4820168018 MSE_Loss: 3.4820168018 time: 0.0227s\n",
      "Feature: 0002 Epoch: 2600 Loss: 3.7611016035 MSE_Loss: 3.7611016035 time: 0.0228s\n",
      "Feature: 0002 Epoch: 2700 Loss: 4.3606907129 MSE_Loss: 4.3606907129 time: 0.0229s\n",
      "Feature: 0002 Epoch: 2800 Loss: 4.2229201794 MSE_Loss: 4.2229201794 time: 0.0229s\n",
      "Feature: 0002 Epoch: 2900 Loss: 4.4079982638 MSE_Loss: 4.4079982638 time: 0.0232s\n",
      "Begin training feature: 0003\n",
      "Feature: 0003 Epoch: 0000 Loss: 26.6707720757 MSE_Loss: 26.6707720757 time: 0.0238s\n",
      "Feature: 0003 Epoch: 0100 Loss: 15.1044356823 MSE_Loss: 15.1044356823 time: 0.0230s\n",
      "Feature: 0003 Epoch: 0200 Loss: 9.8665950298 MSE_Loss: 9.8665950298 time: 0.0231s\n",
      "Feature: 0003 Epoch: 0300 Loss: 6.7482089996 MSE_Loss: 6.7482089996 time: 0.0230s\n",
      "Feature: 0003 Epoch: 0400 Loss: 5.7582496405 MSE_Loss: 5.7582496405 time: 0.0171s\n",
      "Feature: 0003 Epoch: 0500 Loss: 5.2956801653 MSE_Loss: 5.2956801653 time: 0.0228s\n",
      "Feature: 0003 Epoch: 0600 Loss: 4.5435414314 MSE_Loss: 4.5435414314 time: 0.0233s\n",
      "Feature: 0003 Epoch: 0700 Loss: 4.6510620117 MSE_Loss: 4.6510620117 time: 0.0233s\n",
      "Feature: 0003 Epoch: 0800 Loss: 4.4385405779 MSE_Loss: 4.4385405779 time: 0.0237s\n",
      "Feature: 0003 Epoch: 0900 Loss: 4.3754079342 MSE_Loss: 4.3754079342 time: 0.0219s\n",
      "Feature: 0003 Epoch: 1000 Loss: 3.9346587062 MSE_Loss: 3.9346587062 time: 0.0235s\n",
      "Feature: 0003 Epoch: 1100 Loss: 3.6619255543 MSE_Loss: 3.6619255543 time: 0.0229s\n",
      "Feature: 0003 Epoch: 1200 Loss: 4.3870135546 MSE_Loss: 4.3870135546 time: 0.0194s\n",
      "Feature: 0003 Epoch: 1300 Loss: 3.8808854818 MSE_Loss: 3.8808854818 time: 0.0232s\n",
      "Feature: 0003 Epoch: 1400 Loss: 4.4939330220 MSE_Loss: 4.4939330220 time: 0.0232s\n",
      "Feature: 0003 Epoch: 1500 Loss: 3.5868690014 MSE_Loss: 3.5868690014 time: 0.0188s\n",
      "Feature: 0003 Epoch: 1600 Loss: 3.4344286919 MSE_Loss: 3.4344286919 time: 0.0231s\n",
      "Feature: 0003 Epoch: 1700 Loss: 3.5626760721 MSE_Loss: 3.5626760721 time: 0.0192s\n",
      "Feature: 0003 Epoch: 1800 Loss: 3.4004269242 MSE_Loss: 3.4004269242 time: 0.0233s\n",
      "Feature: 0003 Epoch: 1900 Loss: 3.8724012375 MSE_Loss: 3.8724012375 time: 0.0231s\n",
      "Feature: 0003 Epoch: 2000 Loss: 3.7011665702 MSE_Loss: 3.7011665702 time: 0.0235s\n",
      "Feature: 0003 Epoch: 2100 Loss: 4.2664127350 MSE_Loss: 4.2664127350 time: 0.0234s\n",
      "Feature: 0003 Epoch: 2200 Loss: 3.8425659537 MSE_Loss: 3.8425659537 time: 0.0209s\n",
      "Feature: 0003 Epoch: 2300 Loss: 3.7548133135 MSE_Loss: 3.7548133135 time: 0.0235s\n",
      "Feature: 0003 Epoch: 2400 Loss: 3.7506683469 MSE_Loss: 3.7506683469 time: 0.0231s\n",
      "Feature: 0003 Epoch: 2500 Loss: 3.9007177353 MSE_Loss: 3.9007177353 time: 0.0234s\n",
      "Feature: 0003 Epoch: 2600 Loss: 3.7639539838 MSE_Loss: 3.7639539838 time: 0.0228s\n",
      "Feature: 0003 Epoch: 2700 Loss: 3.6099511385 MSE_Loss: 3.6099511385 time: 0.0234s\n",
      "Feature: 0003 Epoch: 2800 Loss: 3.4930509925 MSE_Loss: 3.4930509925 time: 0.0231s\n",
      "Feature: 0003 Epoch: 2900 Loss: 3.8519265056 MSE_Loss: 3.8519265056 time: 0.0232s\n",
      "Begin training feature: 0004\n",
      "Feature: 0004 Epoch: 0000 Loss: 29.5761561394 MSE_Loss: 29.5761561394 time: 0.0243s\n",
      "Feature: 0004 Epoch: 0100 Loss: 15.0201117992 MSE_Loss: 15.0201117992 time: 0.0158s\n",
      "Feature: 0004 Epoch: 0200 Loss: 10.9408229589 MSE_Loss: 10.9408229589 time: 0.0232s\n",
      "Feature: 0004 Epoch: 0300 Loss: 7.7560857534 MSE_Loss: 7.7560857534 time: 0.0233s\n",
      "Feature: 0004 Epoch: 0400 Loss: 6.3805599809 MSE_Loss: 6.3805599809 time: 0.0230s\n",
      "Feature: 0004 Epoch: 0500 Loss: 5.9105026722 MSE_Loss: 5.9105026722 time: 0.0238s\n",
      "Feature: 0004 Epoch: 0600 Loss: 5.2552078962 MSE_Loss: 5.2552078962 time: 0.0230s\n",
      "Feature: 0004 Epoch: 0700 Loss: 5.8218467236 MSE_Loss: 5.8218467236 time: 0.0229s\n",
      "Feature: 0004 Epoch: 0800 Loss: 5.2328628302 MSE_Loss: 5.2328628302 time: 0.0231s\n",
      "Feature: 0004 Epoch: 0900 Loss: 4.7156895399 MSE_Loss: 4.7156895399 time: 0.0228s\n",
      "Feature: 0004 Epoch: 1000 Loss: 4.6594172716 MSE_Loss: 4.6594172716 time: 0.0194s\n",
      "Feature: 0004 Epoch: 1100 Loss: 4.6952189207 MSE_Loss: 4.6952189207 time: 0.0191s\n",
      "Feature: 0004 Epoch: 1200 Loss: 4.9522827864 MSE_Loss: 4.9522827864 time: 0.0234s\n",
      "Feature: 0004 Epoch: 1300 Loss: 5.1059217453 MSE_Loss: 5.1059217453 time: 0.0234s\n",
      "Feature: 0004 Epoch: 1400 Loss: 4.7644289136 MSE_Loss: 4.7644289136 time: 0.0236s\n",
      "Feature: 0004 Epoch: 1500 Loss: 5.0763483644 MSE_Loss: 5.0763483644 time: 0.0230s\n",
      "Feature: 0004 Epoch: 1600 Loss: 4.9002075195 MSE_Loss: 4.9002075195 time: 0.0235s\n",
      "Feature: 0004 Epoch: 1700 Loss: 4.7490332723 MSE_Loss: 4.7490332723 time: 0.0235s\n",
      "Feature: 0004 Epoch: 1800 Loss: 4.9595008492 MSE_Loss: 4.9595008492 time: 0.0231s\n",
      "Feature: 0004 Epoch: 1900 Loss: 5.3606070280 MSE_Loss: 5.3606070280 time: 0.0235s\n",
      "Feature: 0004 Epoch: 2000 Loss: 4.4786522388 MSE_Loss: 4.4786522388 time: 0.0233s\n",
      "Feature: 0004 Epoch: 2100 Loss: 4.0594210029 MSE_Loss: 4.0594210029 time: 0.0201s\n",
      "Feature: 0004 Epoch: 2200 Loss: 4.8861287832 MSE_Loss: 4.8861287832 time: 0.0231s\n",
      "Feature: 0004 Epoch: 2300 Loss: 4.6616027951 MSE_Loss: 4.6616027951 time: 0.0232s\n",
      "Feature: 0004 Epoch: 2400 Loss: 4.7520452142 MSE_Loss: 4.7520452142 time: 0.0234s\n",
      "Feature: 0004 Epoch: 2500 Loss: 4.2459438443 MSE_Loss: 4.2459438443 time: 0.0232s\n",
      "Feature: 0004 Epoch: 2600 Loss: 4.3068644404 MSE_Loss: 4.3068644404 time: 0.0230s\n",
      "Feature: 0004 Epoch: 2700 Loss: 4.4706977010 MSE_Loss: 4.4706977010 time: 0.0228s\n",
      "Feature: 0004 Epoch: 2800 Loss: 4.2655575871 MSE_Loss: 4.2655575871 time: 0.0235s\n",
      "Feature: 0004 Epoch: 2900 Loss: 4.4956278205 MSE_Loss: 4.4956278205 time: 0.0232s\n",
      "Begin training feature: 0005\n",
      "Feature: 0005 Epoch: 0000 Loss: 27.4945144653 MSE_Loss: 27.4945144653 time: 0.0175s\n",
      "Feature: 0005 Epoch: 0100 Loss: 12.1289045811 MSE_Loss: 12.1289045811 time: 0.0162s\n",
      "Feature: 0005 Epoch: 0200 Loss: 8.0372207165 MSE_Loss: 8.0372207165 time: 0.0233s\n",
      "Feature: 0005 Epoch: 0300 Loss: 6.6680458784 MSE_Loss: 6.6680458784 time: 0.0231s\n",
      "Feature: 0005 Epoch: 0400 Loss: 6.4139708281 MSE_Loss: 6.4139708281 time: 0.0233s\n",
      "Feature: 0005 Epoch: 0500 Loss: 5.4335080385 MSE_Loss: 5.4335080385 time: 0.0232s\n",
      "Feature: 0005 Epoch: 0600 Loss: 4.9374114275 MSE_Loss: 4.9374114275 time: 0.0233s\n",
      "Feature: 0005 Epoch: 0700 Loss: 5.5399504900 MSE_Loss: 5.5399504900 time: 0.0227s\n",
      "Feature: 0005 Epoch: 0800 Loss: 5.5116550922 MSE_Loss: 5.5116550922 time: 0.0229s\n",
      "Feature: 0005 Epoch: 0900 Loss: 4.8969545364 MSE_Loss: 4.8969545364 time: 0.0235s\n",
      "Feature: 0005 Epoch: 1000 Loss: 4.7392029762 MSE_Loss: 4.7392029762 time: 0.0230s\n",
      "Feature: 0005 Epoch: 1100 Loss: 5.6003940105 MSE_Loss: 5.6003940105 time: 0.0243s\n",
      "Feature: 0005 Epoch: 1200 Loss: 5.2507131100 MSE_Loss: 5.2507131100 time: 0.0235s\n",
      "Feature: 0005 Epoch: 1300 Loss: 4.6312505007 MSE_Loss: 4.6312505007 time: 0.0235s\n",
      "Feature: 0005 Epoch: 1400 Loss: 4.5541639924 MSE_Loss: 4.5541639924 time: 0.0232s\n",
      "Feature: 0005 Epoch: 1500 Loss: 4.9179188013 MSE_Loss: 4.9179188013 time: 0.0231s\n",
      "Feature: 0005 Epoch: 1600 Loss: 4.6508522034 MSE_Loss: 4.6508522034 time: 0.0233s\n",
      "Feature: 0005 Epoch: 1700 Loss: 4.6425933838 MSE_Loss: 4.6425933838 time: 0.0238s\n",
      "Feature: 0005 Epoch: 1800 Loss: 4.7688530087 MSE_Loss: 4.7688530087 time: 0.0253s\n",
      "Feature: 0005 Epoch: 1900 Loss: 4.1468616724 MSE_Loss: 4.1468616724 time: 0.0231s\n",
      "Feature: 0005 Epoch: 2000 Loss: 4.6999367476 MSE_Loss: 4.6999367476 time: 0.0227s\n",
      "Feature: 0005 Epoch: 2100 Loss: 4.8774654865 MSE_Loss: 4.8774654865 time: 0.0230s\n",
      "Feature: 0005 Epoch: 2200 Loss: 4.6428588629 MSE_Loss: 4.6428588629 time: 0.0235s\n",
      "Feature: 0005 Epoch: 2300 Loss: 4.2033717632 MSE_Loss: 4.2033717632 time: 0.0231s\n",
      "Feature: 0005 Epoch: 2400 Loss: 4.2371989489 MSE_Loss: 4.2371989489 time: 0.0230s\n",
      "Feature: 0005 Epoch: 2500 Loss: 4.6705105305 MSE_Loss: 4.6705105305 time: 0.0230s\n",
      "Feature: 0005 Epoch: 2600 Loss: 4.6199333668 MSE_Loss: 4.6199333668 time: 0.0228s\n",
      "Feature: 0005 Epoch: 2700 Loss: 5.0447385311 MSE_Loss: 5.0447385311 time: 0.0225s\n",
      "Feature: 0005 Epoch: 2800 Loss: 4.4532998800 MSE_Loss: 4.4532998800 time: 0.0230s\n",
      "Feature: 0005 Epoch: 2900 Loss: 4.8564581871 MSE_Loss: 4.8564581871 time: 0.0228s\n",
      "Begin training feature: 0006\n",
      "Feature: 0006 Epoch: 0000 Loss: 29.3483381271 MSE_Loss: 29.3483381271 time: 0.0234s\n",
      "Feature: 0006 Epoch: 0100 Loss: 13.7700493336 MSE_Loss: 13.7700493336 time: 0.0169s\n",
      "Feature: 0006 Epoch: 0200 Loss: 8.6189361811 MSE_Loss: 8.6189361811 time: 0.0231s\n",
      "Feature: 0006 Epoch: 0300 Loss: 6.8422046900 MSE_Loss: 6.8422046900 time: 0.0230s\n",
      "Feature: 0006 Epoch: 0400 Loss: 6.3053830862 MSE_Loss: 6.3053830862 time: 0.0173s\n",
      "Feature: 0006 Epoch: 0500 Loss: 6.4749852419 MSE_Loss: 6.4749852419 time: 0.0226s\n",
      "Feature: 0006 Epoch: 0600 Loss: 4.7624963522 MSE_Loss: 4.7624963522 time: 0.0231s\n",
      "Feature: 0006 Epoch: 0700 Loss: 5.1909232140 MSE_Loss: 5.1909232140 time: 0.0180s\n",
      "Feature: 0006 Epoch: 0800 Loss: 5.3985190392 MSE_Loss: 5.3985190392 time: 0.0237s\n",
      "Feature: 0006 Epoch: 0900 Loss: 5.1656525731 MSE_Loss: 5.1656525731 time: 0.0229s\n",
      "Feature: 0006 Epoch: 1000 Loss: 4.5395088196 MSE_Loss: 4.5395088196 time: 0.0231s\n",
      "Feature: 0006 Epoch: 1100 Loss: 3.9301673770 MSE_Loss: 3.9301673770 time: 0.0243s\n",
      "Feature: 0006 Epoch: 1200 Loss: 4.6724072099 MSE_Loss: 4.6724072099 time: 0.0228s\n",
      "Feature: 0006 Epoch: 1300 Loss: 4.5606669784 MSE_Loss: 4.5606669784 time: 0.0231s\n",
      "Feature: 0006 Epoch: 1400 Loss: 4.6933735013 MSE_Loss: 4.6933735013 time: 0.0228s\n",
      "Feature: 0006 Epoch: 1500 Loss: 4.4896225333 MSE_Loss: 4.4896225333 time: 0.0232s\n",
      "Feature: 0006 Epoch: 1600 Loss: 4.3196546435 MSE_Loss: 4.3196546435 time: 0.0229s\n",
      "Feature: 0006 Epoch: 1700 Loss: 4.2197265029 MSE_Loss: 4.2197265029 time: 0.0236s\n",
      "Feature: 0006 Epoch: 1800 Loss: 3.8674219251 MSE_Loss: 3.8674219251 time: 0.0233s\n",
      "Feature: 0006 Epoch: 1900 Loss: 3.9461402297 MSE_Loss: 3.9461402297 time: 0.0233s\n",
      "Feature: 0006 Epoch: 2000 Loss: 4.6555675268 MSE_Loss: 4.6555675268 time: 0.0235s\n",
      "Feature: 0006 Epoch: 2100 Loss: 4.6927007437 MSE_Loss: 4.6927007437 time: 0.0231s\n",
      "Feature: 0006 Epoch: 2200 Loss: 4.2999041677 MSE_Loss: 4.2999041677 time: 0.0235s\n",
      "Feature: 0006 Epoch: 2300 Loss: 4.3910025954 MSE_Loss: 4.3910025954 time: 0.0233s\n",
      "Feature: 0006 Epoch: 2400 Loss: 4.6213929057 MSE_Loss: 4.6213929057 time: 0.0192s\n",
      "Feature: 0006 Epoch: 2500 Loss: 4.5708959103 MSE_Loss: 4.5708959103 time: 0.0219s\n",
      "Feature: 0006 Epoch: 2600 Loss: 4.0704033971 MSE_Loss: 4.0704033971 time: 0.0223s\n",
      "Feature: 0006 Epoch: 2700 Loss: 4.3652809262 MSE_Loss: 4.3652809262 time: 0.0219s\n",
      "Feature: 0006 Epoch: 2800 Loss: 4.1752639413 MSE_Loss: 4.1752639413 time: 0.0233s\n",
      "Feature: 0006 Epoch: 2900 Loss: 3.8741564155 MSE_Loss: 3.8741564155 time: 0.0198s\n",
      "Begin training feature: 0007\n",
      "Feature: 0007 Epoch: 0000 Loss: 30.5506830215 MSE_Loss: 30.5506830215 time: 0.0236s\n",
      "Feature: 0007 Epoch: 0100 Loss: 17.8744509220 MSE_Loss: 17.8744509220 time: 0.0230s\n",
      "Feature: 0007 Epoch: 0200 Loss: 16.0138757229 MSE_Loss: 16.0138757229 time: 0.0234s\n",
      "Feature: 0007 Epoch: 0300 Loss: 9.9831736088 MSE_Loss: 9.9831736088 time: 0.0236s\n",
      "Feature: 0007 Epoch: 0400 Loss: 7.7528967857 MSE_Loss: 7.7528967857 time: 0.0244s\n",
      "Feature: 0007 Epoch: 0500 Loss: 6.4717688560 MSE_Loss: 6.4717688560 time: 0.0233s\n",
      "Feature: 0007 Epoch: 0600 Loss: 6.2772811651 MSE_Loss: 6.2772811651 time: 0.0181s\n",
      "Feature: 0007 Epoch: 0700 Loss: 5.3049174547 MSE_Loss: 5.3049174547 time: 0.0237s\n",
      "Feature: 0007 Epoch: 0800 Loss: 5.5123260021 MSE_Loss: 5.5123260021 time: 0.0230s\n",
      "Feature: 0007 Epoch: 0900 Loss: 4.7298990488 MSE_Loss: 4.7298990488 time: 0.0195s\n",
      "Feature: 0007 Epoch: 1000 Loss: 4.1825837493 MSE_Loss: 4.1825837493 time: 0.0235s\n",
      "Feature: 0007 Epoch: 1100 Loss: 4.4305632114 MSE_Loss: 4.4305632114 time: 0.0240s\n",
      "Feature: 0007 Epoch: 1200 Loss: 4.6989436150 MSE_Loss: 4.6989436150 time: 0.0236s\n",
      "Feature: 0007 Epoch: 1300 Loss: 4.6417983770 MSE_Loss: 4.6417983770 time: 0.0166s\n",
      "Feature: 0007 Epoch: 1400 Loss: 4.8616307378 MSE_Loss: 4.8616307378 time: 0.0229s\n",
      "Feature: 0007 Epoch: 1500 Loss: 4.4997964501 MSE_Loss: 4.4997964501 time: 0.0227s\n",
      "Feature: 0007 Epoch: 1600 Loss: 4.2929309607 MSE_Loss: 4.2929309607 time: 0.0230s\n",
      "Feature: 0007 Epoch: 1700 Loss: 4.4382975698 MSE_Loss: 4.4382975698 time: 0.0229s\n",
      "Feature: 0007 Epoch: 1800 Loss: 4.2705288529 MSE_Loss: 4.2705288529 time: 0.0228s\n",
      "Feature: 0007 Epoch: 1900 Loss: 4.6527792811 MSE_Loss: 4.6527792811 time: 0.0235s\n",
      "Feature: 0007 Epoch: 2000 Loss: 3.9200153947 MSE_Loss: 3.9200153947 time: 0.0240s\n",
      "Feature: 0007 Epoch: 2100 Loss: 4.7388497591 MSE_Loss: 4.7388497591 time: 0.0235s\n",
      "Feature: 0007 Epoch: 2200 Loss: 5.1171604395 MSE_Loss: 5.1171604395 time: 0.0232s\n",
      "Feature: 0007 Epoch: 2300 Loss: 4.2586433887 MSE_Loss: 4.2586433887 time: 0.0235s\n",
      "Feature: 0007 Epoch: 2400 Loss: 4.5517401695 MSE_Loss: 4.5517401695 time: 0.0235s\n",
      "Feature: 0007 Epoch: 2500 Loss: 3.7370894551 MSE_Loss: 3.7370894551 time: 0.0165s\n",
      "Feature: 0007 Epoch: 2600 Loss: 3.7384012938 MSE_Loss: 3.7384012938 time: 0.0235s\n",
      "Feature: 0007 Epoch: 2700 Loss: 4.0167322755 MSE_Loss: 4.0167322755 time: 0.0233s\n",
      "Feature: 0007 Epoch: 2800 Loss: 4.2076767683 MSE_Loss: 4.2076767683 time: 0.0233s\n",
      "Feature: 0007 Epoch: 2900 Loss: 4.3440008163 MSE_Loss: 4.3440008163 time: 0.0232s\n",
      "Begin training feature: 0008\n",
      "Feature: 0008 Epoch: 0000 Loss: 22.2838501930 MSE_Loss: 22.2838501930 time: 0.0236s\n",
      "Feature: 0008 Epoch: 0100 Loss: 12.1903336048 MSE_Loss: 12.1903336048 time: 0.0238s\n",
      "Feature: 0008 Epoch: 0200 Loss: 7.5084296465 MSE_Loss: 7.5084296465 time: 0.0239s\n",
      "Feature: 0008 Epoch: 0300 Loss: 5.5754998922 MSE_Loss: 5.5754998922 time: 0.0235s\n",
      "Feature: 0008 Epoch: 0400 Loss: 4.5852428675 MSE_Loss: 4.5852428675 time: 0.0238s\n",
      "Feature: 0008 Epoch: 0500 Loss: 4.6017705798 MSE_Loss: 4.6017705798 time: 0.0234s\n",
      "Feature: 0008 Epoch: 0600 Loss: 4.2261673808 MSE_Loss: 4.2261673808 time: 0.0231s\n",
      "Feature: 0008 Epoch: 0700 Loss: 4.4064994454 MSE_Loss: 4.4064994454 time: 0.0228s\n",
      "Feature: 0008 Epoch: 0800 Loss: 4.0073856711 MSE_Loss: 4.0073856711 time: 0.0230s\n",
      "Feature: 0008 Epoch: 0900 Loss: 4.3699737787 MSE_Loss: 4.3699737787 time: 0.0232s\n",
      "Feature: 0008 Epoch: 1000 Loss: 4.3747894764 MSE_Loss: 4.3747894764 time: 0.0231s\n",
      "Feature: 0008 Epoch: 1100 Loss: 4.2252876163 MSE_Loss: 4.2252876163 time: 0.0230s\n",
      "Feature: 0008 Epoch: 1200 Loss: 4.2168519497 MSE_Loss: 4.2168519497 time: 0.0159s\n",
      "Feature: 0008 Epoch: 1300 Loss: 4.1596652865 MSE_Loss: 4.1596652865 time: 0.0229s\n",
      "Feature: 0008 Epoch: 1400 Loss: 4.2373963594 MSE_Loss: 4.2373963594 time: 0.0187s\n",
      "Feature: 0008 Epoch: 1500 Loss: 3.6092478633 MSE_Loss: 3.6092478633 time: 0.0172s\n",
      "Feature: 0008 Epoch: 1600 Loss: 3.9470477700 MSE_Loss: 3.9470477700 time: 0.0229s\n",
      "Feature: 0008 Epoch: 1700 Loss: 3.6758800149 MSE_Loss: 3.6758800149 time: 0.0230s\n",
      "Feature: 0008 Epoch: 1800 Loss: 3.4399274588 MSE_Loss: 3.4399274588 time: 0.0232s\n",
      "Feature: 0008 Epoch: 1900 Loss: 3.5000323653 MSE_Loss: 3.5000323653 time: 0.0231s\n",
      "Feature: 0008 Epoch: 2000 Loss: 3.6071857214 MSE_Loss: 3.6071857214 time: 0.0230s\n",
      "Feature: 0008 Epoch: 2100 Loss: 4.0023036003 MSE_Loss: 4.0023036003 time: 0.0231s\n",
      "Feature: 0008 Epoch: 2200 Loss: 4.2113475204 MSE_Loss: 4.2113475204 time: 0.0207s\n",
      "Feature: 0008 Epoch: 2300 Loss: 3.4291284680 MSE_Loss: 3.4291284680 time: 0.0204s\n",
      "Feature: 0008 Epoch: 2400 Loss: 3.5583704710 MSE_Loss: 3.5583704710 time: 0.0231s\n",
      "Feature: 0008 Epoch: 2500 Loss: 4.0945842266 MSE_Loss: 4.0945842266 time: 0.0193s\n",
      "Feature: 0008 Epoch: 2600 Loss: 3.8071424961 MSE_Loss: 3.8071424961 time: 0.0231s\n",
      "Feature: 0008 Epoch: 2700 Loss: 3.2844461799 MSE_Loss: 3.2844461799 time: 0.0204s\n",
      "Feature: 0008 Epoch: 2800 Loss: 3.3116959333 MSE_Loss: 3.3116959333 time: 0.0232s\n",
      "Feature: 0008 Epoch: 2900 Loss: 3.8712322116 MSE_Loss: 3.8712322116 time: 0.0227s\n",
      "Begin training feature: 0009\n",
      "Feature: 0009 Epoch: 0000 Loss: 26.2993955612 MSE_Loss: 26.2993955612 time: 0.0232s\n",
      "Feature: 0009 Epoch: 0100 Loss: 13.9777703285 MSE_Loss: 13.9777703285 time: 0.0230s\n",
      "Feature: 0009 Epoch: 0200 Loss: 8.9514981508 MSE_Loss: 8.9514981508 time: 0.0203s\n",
      "Feature: 0009 Epoch: 0300 Loss: 6.1679366827 MSE_Loss: 6.1679366827 time: 0.0157s\n",
      "Feature: 0009 Epoch: 0400 Loss: 5.3865783811 MSE_Loss: 5.3865783811 time: 0.0190s\n",
      "Feature: 0009 Epoch: 0500 Loss: 5.5910531878 MSE_Loss: 5.5910531878 time: 0.0228s\n",
      "Feature: 0009 Epoch: 0600 Loss: 4.9612633586 MSE_Loss: 4.9612633586 time: 0.0228s\n",
      "Feature: 0009 Epoch: 0700 Loss: 4.9842931032 MSE_Loss: 4.9842931032 time: 0.0226s\n",
      "Feature: 0009 Epoch: 0800 Loss: 4.9506974220 MSE_Loss: 4.9506974220 time: 0.0226s\n",
      "Feature: 0009 Epoch: 0900 Loss: 4.2996239662 MSE_Loss: 4.2996239662 time: 0.0230s\n",
      "Feature: 0009 Epoch: 1000 Loss: 4.6292700768 MSE_Loss: 4.6292700768 time: 0.0228s\n",
      "Feature: 0009 Epoch: 1100 Loss: 4.1717150211 MSE_Loss: 4.1717150211 time: 0.0227s\n",
      "Feature: 0009 Epoch: 1200 Loss: 3.9687253833 MSE_Loss: 3.9687253833 time: 0.0227s\n",
      "Feature: 0009 Epoch: 1300 Loss: 4.3066577315 MSE_Loss: 4.3066577315 time: 0.0230s\n",
      "Feature: 0009 Epoch: 1400 Loss: 3.5878725052 MSE_Loss: 3.5878725052 time: 0.0194s\n",
      "Feature: 0009 Epoch: 1500 Loss: 4.2619979978 MSE_Loss: 4.2619979978 time: 0.0166s\n",
      "Feature: 0009 Epoch: 1600 Loss: 4.2229159474 MSE_Loss: 4.2229159474 time: 0.0232s\n",
      "Feature: 0009 Epoch: 1700 Loss: 3.5434399247 MSE_Loss: 3.5434399247 time: 0.0231s\n",
      "Feature: 0009 Epoch: 1800 Loss: 3.9826964140 MSE_Loss: 3.9826964140 time: 0.0231s\n",
      "Feature: 0009 Epoch: 1900 Loss: 4.0915390253 MSE_Loss: 4.0915390253 time: 0.0232s\n",
      "Feature: 0009 Epoch: 2000 Loss: 3.6479131579 MSE_Loss: 3.6479131579 time: 0.0205s\n",
      "Feature: 0009 Epoch: 2100 Loss: 3.9329468608 MSE_Loss: 3.9329468608 time: 0.0230s\n",
      "Feature: 0009 Epoch: 2200 Loss: 3.3106089830 MSE_Loss: 3.3106089830 time: 0.0229s\n",
      "Feature: 0009 Epoch: 2300 Loss: 4.1334609985 MSE_Loss: 4.1334609985 time: 0.0233s\n",
      "Feature: 0009 Epoch: 2400 Loss: 3.7089961767 MSE_Loss: 3.7089961767 time: 0.0232s\n",
      "Feature: 0009 Epoch: 2500 Loss: 3.8628427982 MSE_Loss: 3.8628427982 time: 0.0194s\n",
      "Feature: 0009 Epoch: 2600 Loss: 3.8044552207 MSE_Loss: 3.8044552207 time: 0.0228s\n",
      "Feature: 0009 Epoch: 2700 Loss: 3.9209958911 MSE_Loss: 3.9209958911 time: 0.0189s\n",
      "Feature: 0009 Epoch: 2800 Loss: 3.7739840746 MSE_Loss: 3.7739840746 time: 0.0232s\n",
      "Feature: 0009 Epoch: 2900 Loss: 3.9331375957 MSE_Loss: 3.9331375957 time: 0.0234s\n",
      "Begin training feature: 0010\n",
      "Feature: 0010 Epoch: 0000 Loss: 26.8533968925 MSE_Loss: 26.8533968925 time: 0.0235s\n",
      "Feature: 0010 Epoch: 0100 Loss: 13.6967329979 MSE_Loss: 13.6967329979 time: 0.0170s\n",
      "Feature: 0010 Epoch: 0200 Loss: 7.0993722677 MSE_Loss: 7.0993722677 time: 0.0234s\n",
      "Feature: 0010 Epoch: 0300 Loss: 5.9786931276 MSE_Loss: 5.9786931276 time: 0.0237s\n",
      "Feature: 0010 Epoch: 0400 Loss: 4.6207516193 MSE_Loss: 4.6207516193 time: 0.0180s\n",
      "Feature: 0010 Epoch: 0500 Loss: 4.0961884856 MSE_Loss: 4.0961884856 time: 0.0231s\n",
      "Feature: 0010 Epoch: 0600 Loss: 4.3399232030 MSE_Loss: 4.3399232030 time: 0.0228s\n",
      "Feature: 0010 Epoch: 0700 Loss: 4.3117300868 MSE_Loss: 4.3117300868 time: 0.0228s\n",
      "Feature: 0010 Epoch: 0800 Loss: 3.6203885674 MSE_Loss: 3.6203885674 time: 0.0193s\n",
      "Feature: 0010 Epoch: 0900 Loss: 3.9746226668 MSE_Loss: 3.9746226668 time: 0.0230s\n",
      "Feature: 0010 Epoch: 1000 Loss: 3.9461384416 MSE_Loss: 3.9461384416 time: 0.0230s\n",
      "Feature: 0010 Epoch: 1100 Loss: 3.7643381357 MSE_Loss: 3.7643381357 time: 0.0230s\n",
      "Feature: 0010 Epoch: 1200 Loss: 3.5823560357 MSE_Loss: 3.5823560357 time: 0.0230s\n",
      "Feature: 0010 Epoch: 1300 Loss: 3.5359788537 MSE_Loss: 3.5359788537 time: 0.0233s\n",
      "Feature: 0010 Epoch: 1400 Loss: 3.3365889192 MSE_Loss: 3.3365889192 time: 0.0238s\n",
      "Feature: 0010 Epoch: 1500 Loss: 3.3913550973 MSE_Loss: 3.3913550973 time: 0.0234s\n",
      "Feature: 0010 Epoch: 1600 Loss: 3.1837360263 MSE_Loss: 3.1837360263 time: 0.0231s\n",
      "Feature: 0010 Epoch: 1700 Loss: 3.2767411470 MSE_Loss: 3.2767411470 time: 0.0232s\n",
      "Feature: 0010 Epoch: 1800 Loss: 3.2913996577 MSE_Loss: 3.2913996577 time: 0.0231s\n",
      "Feature: 0010 Epoch: 1900 Loss: 3.1118419170 MSE_Loss: 3.1118419170 time: 0.0160s\n",
      "Feature: 0010 Epoch: 2000 Loss: 3.2807539403 MSE_Loss: 3.2807539403 time: 0.0235s\n",
      "Feature: 0010 Epoch: 2100 Loss: 3.3794108033 MSE_Loss: 3.3794108033 time: 0.0231s\n",
      "Feature: 0010 Epoch: 2200 Loss: 3.1728212237 MSE_Loss: 3.1728212237 time: 0.0233s\n",
      "Feature: 0010 Epoch: 2300 Loss: 3.0926038027 MSE_Loss: 3.0926038027 time: 0.0233s\n",
      "Feature: 0010 Epoch: 2400 Loss: 3.3370022774 MSE_Loss: 3.3370022774 time: 0.0233s\n",
      "Feature: 0010 Epoch: 2500 Loss: 2.7966644764 MSE_Loss: 2.7966644764 time: 0.0205s\n",
      "Feature: 0010 Epoch: 2600 Loss: 2.9194341302 MSE_Loss: 2.9194341302 time: 0.0238s\n",
      "Feature: 0010 Epoch: 2700 Loss: 3.1975428462 MSE_Loss: 3.1975428462 time: 0.0232s\n",
      "Feature: 0010 Epoch: 2800 Loss: 3.1278478503 MSE_Loss: 3.1278478503 time: 0.0229s\n",
      "Feature: 0010 Epoch: 2900 Loss: 3.2444378138 MSE_Loss: 3.2444378138 time: 0.0230s\n"
     ]
    }
   ],
   "source": [
    "for idx in range(10):\n",
    "    print('Begin training feature: {:04d}'.format(idx + 1))\n",
    "    decoder_file = 'decoder' + str(idx) + '.pt'\n",
    "    decoder_file = os.path.join('/home/omnisky/Public/ChenRongfa/Intrer_VAE_result/Lorenz96.10.250/help', decoder_file)\n",
    "    Inter_decoder = decoder(args.dims, args.decoder_hidden, args.time_step - 1, args.num_nodes, args.decoder_dropout, args.decoder_alpha)\n",
    "    Inter_decoder = Inter_decoder.cuda()\n",
    "    optimizer = optim.Adam(params = Inter_decoder.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
    "    scheduler = lr_scheduler.StepLR(optimizer, step_size=1000, gamma=0.5)\n",
    "    loss_val = nn.MSELoss()\n",
    "    best_loss = np.Inf\n",
    "    for epoch in range(3000):\n",
    "        scheduler.step()\n",
    "        t = time.time()\n",
    "        Loss = []\n",
    "        mse_loss = []\n",
    "        for batch_idx, data in enumerate(data_loader):\n",
    "            data = data.cuda()\n",
    "            target = data[:, idx, 1:, :]\n",
    "            optimizer.zero_grad()\n",
    "            inputs = data[:, :, :-1, :]\n",
    "            pred = Inter_decoder(inputs, idx)\n",
    "            mse = loss_val(pred, target)\n",
    "            loss = mse\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "            Loss.append(loss.item())\n",
    "            mse_loss.append(mse.item())\n",
    "\n",
    "        if epoch % 100 == 0:\n",
    "            print('Feature: {:04d}'.format(idx + 1),\n",
    "                'Epoch: {:04d}'.format(epoch),\n",
    "                'Loss: {:.10f}'.format(np.mean(Loss)),\n",
    "                'MSE_Loss: {:.10f}'.format(np.mean(mse_loss)),\n",
    "                'time: {:.4f}s'.format(time.time() - t))\n",
    "\n",
    "            \n",
    "        if np.mean(mse_loss) < best_loss:\n",
    "            best_loss = np.mean(mse_loss)\n",
    "            torch.save(Inter_decoder.state_dict(), decoder_file)\n",
    "            # print('Feature: {:04d}'.format(idx + 1),\n",
    "            #       'Epoch: {:04d}'.format(epoch),\n",
    "            #       'Loss: {:.10f}'.format(np.mean(Loss)),\n",
    "            #       'mse_loss: {:.10f}'.format(np.mean(mse_loss)),\n",
    "            #       'mmd_loss: {:.10f}'.format(np.mean(mmd_loss)),\n",
    "            #       'time: {:.4f}s'.format(time.time() - t), file=log)\n",
    "\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj = []\n",
    "for idx in range(10):\n",
    "    decoder_file = 'decoder' + str(idx) + '.pt'\n",
    "    decoder_file = os.path.join('/home/omnisky/Public/ChenRongfa/Intrer_VAE_result/Lorenz96.10.250/help', decoder_file)\n",
    "    decoder_net = decoder(args.dims, args.decoder_hidden, args.time_step - 1, args.num_nodes, args.decoder_dropout, args.decoder_alpha)\n",
    "    decoder_net.load_state_dict(torch.load(decoder_file))\n",
    "    adj.append(decoder_net.adj[idx, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 10])\n"
     ]
    }
   ],
   "source": [
    "init_adj = torch.cat([temp.unsqueeze(0) for temp in adj], dim=0)\n",
    "init_adj = init_adj.clone().detach()\n",
    "print(init_adj.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.69, 'precision': 0.9090909090909091, 'recall': 0.25, 'F1': 0.392156862745098, 'ROC_AUC': 0.9470833333333333, 'PR_AUC': 0.9215491348619778}\n"
     ]
    }
   ],
   "source": [
    "result, _ = evaluate_result(GC, init_adj.detach().numpy(), 0.5)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin training feature: 0001\n",
      "Feature: 0001 Epoch: 0000 Loss: 17.9841623306 MSE_Loss: 15.8561654091 Sparsity_loss: 8.3888163567 KL_loss: 0.0736840013 MMD_loss: 0.0601114146 time: 0.0517s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/omnisky/Public/ChenRongfa/Intrer_VAE/Model.py:20: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n",
      "  nn.init.xavier_normal(m.weight.data, gain=1.414)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature: 0001 Epoch: 0100 Loss: 10.0013122559 MSE_Loss: 8.3927799463 Sparsity_loss: 6.3209041357 KL_loss: 0.0789477415 MMD_loss: 0.0550340116 time: 0.0464s\n",
      "Feature: 0001 Epoch: 0200 Loss: 9.2066121101 MSE_Loss: 8.4056054354 Sparsity_loss: 3.0913723111 KL_loss: 0.0507447869 MMD_loss: 0.0553122722 time: 0.0476s\n",
      "Feature: 0001 Epoch: 0300 Loss: 8.9428900480 MSE_Loss: 8.5026884079 Sparsity_loss: 1.6474025846 KL_loss: 0.0489704264 MMD_loss: 0.0557225198 time: 0.0451s\n",
      "Feature: 0001 Epoch: 0400 Loss: 8.9056705236 MSE_Loss: 8.5447691679 Sparsity_loss: 1.3295034170 KL_loss: 0.0458138222 MMD_loss: 0.0561346970 time: 0.0450s\n",
      "Feature: 0001 Epoch: 0500 Loss: 8.8814654350 MSE_Loss: 8.5410394669 Sparsity_loss: 1.2473345995 KL_loss: 0.0452468842 MMD_loss: 0.0562800094 time: 0.0447s\n",
      "Feature: 0001 Epoch: 0600 Loss: 8.8964592218 MSE_Loss: 8.5669318438 Sparsity_loss: 1.2042451799 KL_loss: 0.0447562085 MMD_loss: 0.0560366437 time: 0.0446s\n",
      "Feature: 0001 Epoch: 0700 Loss: 8.8769940138 MSE_Loss: 8.5451002121 Sparsity_loss: 1.2133114934 KL_loss: 0.0452836053 MMD_loss: 0.0562268533 time: 0.0455s\n",
      "Feature: 0001 Epoch: 0800 Loss: 8.8694486618 MSE_Loss: 8.5430152416 Sparsity_loss: 1.1914284229 KL_loss: 0.0452001421 MMD_loss: 0.0562487207 time: 0.0456s\n",
      "Feature: 0001 Epoch: 0900 Loss: 8.8685413599 MSE_Loss: 8.5409952402 Sparsity_loss: 1.1962637007 KL_loss: 0.0450754398 MMD_loss: 0.0560590141 time: 0.0472s\n",
      "Feature: 0001 Epoch: 1000 Loss: 8.8634599447 MSE_Loss: 8.5455566645 Sparsity_loss: 1.1573598087 KL_loss: 0.0448771566 MMD_loss: 0.0562298782 time: 0.0453s\n",
      "Feature: 0001 Epoch: 1100 Loss: 8.8628832102 MSE_Loss: 8.5443506241 Sparsity_loss: 1.1601197720 KL_loss: 0.0451772790 MMD_loss: 0.0561018065 time: 0.0454s\n",
      "Feature: 0001 Epoch: 1200 Loss: 8.8630545139 MSE_Loss: 8.5451511145 Sparsity_loss: 1.1572929919 KL_loss: 0.0450152168 MMD_loss: 0.0562598035 time: 0.0474s\n",
      "Feature: 0001 Epoch: 1300 Loss: 8.8680075407 MSE_Loss: 8.5496186018 Sparsity_loss: 1.1591636539 KL_loss: 0.0451076748 MMD_loss: 0.0562934689 time: 0.0459s\n",
      "Feature: 0001 Epoch: 1400 Loss: 8.8520603180 MSE_Loss: 8.5333825350 Sparsity_loss: 1.1603550911 KL_loss: 0.0451687453 MMD_loss: 0.0562743582 time: 0.0442s\n",
      "Feature: 0001 Epoch: 1500 Loss: 8.8595042229 MSE_Loss: 8.5336416960 Sparsity_loss: 1.1893481016 KL_loss: 0.0457835933 MMD_loss: 0.0561343804 time: 0.0453s\n",
      "Feature: 0001 Epoch: 1600 Loss: 8.8559840918 MSE_Loss: 8.5351012945 Sparsity_loss: 1.1691970527 KL_loss: 0.0453494880 MMD_loss: 0.0562605150 time: 0.0466s\n",
      "Feature: 0001 Epoch: 1700 Loss: 8.8646303415 MSE_Loss: 8.5436410904 Sparsity_loss: 1.1695728004 KL_loss: 0.0449760752 MMD_loss: 0.0562921986 time: 0.0465s\n",
      "Feature: 0001 Epoch: 1800 Loss: 8.8565382957 MSE_Loss: 8.5371527672 Sparsity_loss: 1.1631776392 KL_loss: 0.0447873492 MMD_loss: 0.0562855788 time: 0.0468s\n",
      "Feature: 0001 Epoch: 1900 Loss: 8.8451958895 MSE_Loss: 8.5259351730 Sparsity_loss: 1.1624578834 KL_loss: 0.0449176049 MMD_loss: 0.0563944988 time: 0.0453s\n",
      "Begin training feature: 0002\n",
      "Feature: 0002 Epoch: 0000 Loss: 12.2605946064 MSE_Loss: 10.1631265879 Sparsity_loss: 8.3025140762 KL_loss: 0.0688963253 MMD_loss: 0.0423014499 time: 0.0520s\n",
      "Feature: 0002 Epoch: 0100 Loss: 5.1689054966 MSE_Loss: 3.6195151210 Sparsity_loss: 6.1239479780 KL_loss: 0.0661032051 MMD_loss: 0.0354845263 time: 0.0506s\n",
      "Feature: 0002 Epoch: 0200 Loss: 4.4581071734 MSE_Loss: 3.6080101132 Sparsity_loss: 3.3257885575 KL_loss: 0.0873463377 MMD_loss: 0.0355529115 time: 0.0454s\n",
      "Feature: 0002 Epoch: 0300 Loss: 4.4049466252 MSE_Loss: 3.6039719582 Sparsity_loss: 3.1299986243 KL_loss: 0.0866003186 MMD_loss: 0.0352181010 time: 0.0544s\n",
      "Feature: 0002 Epoch: 0400 Loss: 4.1817212105 MSE_Loss: 3.6244117022 Sparsity_loss: 2.1546981931 KL_loss: 0.0677091293 MMD_loss: 0.0359157324 time: 0.0453s\n",
      "Feature: 0002 Epoch: 0500 Loss: 4.1607562304 MSE_Loss: 3.6166700721 Sparsity_loss: 2.1013758183 KL_loss: 0.0676256418 MMD_loss: 0.0361320861 time: 0.0458s\n",
      "Feature: 0002 Epoch: 0600 Loss: 4.1383453608 MSE_Loss: 3.5929211378 Sparsity_loss: 2.1076171994 KL_loss: 0.0679970793 MMD_loss: 0.0356795602 time: 0.0450s\n",
      "Feature: 0002 Epoch: 0700 Loss: 4.1414228082 MSE_Loss: 3.5969387889 Sparsity_loss: 2.1036353111 KL_loss: 0.0680378899 MMD_loss: 0.0357893184 time: 0.0470s\n",
      "Feature: 0002 Epoch: 0800 Loss: 4.1329299212 MSE_Loss: 3.5922828317 Sparsity_loss: 2.0892094374 KL_loss: 0.0673679616 MMD_loss: 0.0353422277 time: 0.0457s\n",
      "Feature: 0002 Epoch: 0900 Loss: 3.9380989671 MSE_Loss: 3.6299675107 Sparsity_loss: 1.1590423286 KL_loss: 0.0497046132 MMD_loss: 0.0357475393 time: 0.0474s\n",
      "Feature: 0002 Epoch: 1000 Loss: 3.9361327887 MSE_Loss: 3.6229814887 Sparsity_loss: 1.1798468530 KL_loss: 0.0500491913 MMD_loss: 0.0353782028 time: 0.0464s\n",
      "Feature: 0002 Epoch: 1100 Loss: 3.9313272834 MSE_Loss: 3.6212946177 Sparsity_loss: 1.1667796373 KL_loss: 0.0500269877 MMD_loss: 0.0356751122 time: 0.0440s\n",
      "Feature: 0002 Epoch: 1200 Loss: 3.9373680353 MSE_Loss: 3.6320452690 Sparsity_loss: 1.1482475698 KL_loss: 0.0497608595 MMD_loss: 0.0355266780 time: 0.0462s\n",
      "Feature: 0002 Epoch: 1300 Loss: 3.9204291701 MSE_Loss: 3.6141474843 Sparsity_loss: 1.1522076428 KL_loss: 0.0498674847 MMD_loss: 0.0354620330 time: 0.0456s\n",
      "Feature: 0002 Epoch: 1400 Loss: 3.9273386598 MSE_Loss: 3.6210402846 Sparsity_loss: 1.1524913013 KL_loss: 0.0498503689 MMD_loss: 0.0353541039 time: 0.0470s\n",
      "Feature: 0002 Epoch: 1500 Loss: 3.9190717936 MSE_Loss: 3.6109352112 Sparsity_loss: 1.1596530974 KL_loss: 0.0499150651 MMD_loss: 0.0354485586 time: 0.0451s\n",
      "Feature: 0002 Epoch: 1600 Loss: 3.9181446433 MSE_Loss: 3.6089399457 Sparsity_loss: 1.1641796231 KL_loss: 0.0499605676 MMD_loss: 0.0353203639 time: 0.0438s\n",
      "Feature: 0002 Epoch: 1700 Loss: 3.9136564136 MSE_Loss: 3.6066055894 Sparsity_loss: 1.1554766893 KL_loss: 0.0499442015 MMD_loss: 0.0353644341 time: 0.0466s\n",
      "Feature: 0002 Epoch: 1800 Loss: 3.9098310471 MSE_Loss: 3.6025480032 Sparsity_loss: 1.1569822431 KL_loss: 0.0496290373 MMD_loss: 0.0350823365 time: 0.0453s\n",
      "Feature: 0002 Epoch: 1900 Loss: 3.8989321589 MSE_Loss: 3.5933368802 Sparsity_loss: 1.1497532725 KL_loss: 0.0497858925 MMD_loss: 0.0353182182 time: 0.0459s\n",
      "Begin training feature: 0003\n",
      "Feature: 0003 Epoch: 0000 Loss: 14.0447099209 MSE_Loss: 11.9049832821 Sparsity_loss: 8.4517810345 KL_loss: 0.0819867160 MMD_loss: 0.0519227907 time: 0.0565s\n",
      "Feature: 0003 Epoch: 0100 Loss: 7.0790245533 MSE_Loss: 5.5414612293 Sparsity_loss: 6.0553470850 KL_loss: 0.0675528310 MMD_loss: 0.0461020060 time: 0.0477s\n",
      "Feature: 0003 Epoch: 0200 Loss: 6.4662046432 MSE_Loss: 5.5456167459 Sparsity_loss: 3.5875558853 KL_loss: 0.0641868860 MMD_loss: 0.0461143181 time: 0.0669s\n",
      "Feature: 0003 Epoch: 0300 Loss: 6.4436532259 MSE_Loss: 5.5450838804 Sparsity_loss: 3.4994726181 KL_loss: 0.0637925025 MMD_loss: 0.0461264700 time: 0.0459s\n",
      "Feature: 0003 Epoch: 0400 Loss: 6.4392114878 MSE_Loss: 5.5447323322 Sparsity_loss: 3.4831277132 KL_loss: 0.0637332853 MMD_loss: 0.0461196005 time: 0.0464s\n",
      "Feature: 0003 Epoch: 0500 Loss: 6.4430532455 MSE_Loss: 5.5443940163 Sparsity_loss: 3.4998500943 KL_loss: 0.0635517165 MMD_loss: 0.0461223572 time: 0.0463s\n",
      "Feature: 0003 Epoch: 0600 Loss: 6.4210832119 MSE_Loss: 5.5498288870 Sparsity_loss: 3.3902843595 KL_loss: 0.0609453097 MMD_loss: 0.0461471602 time: 0.0457s\n",
      "Feature: 0003 Epoch: 0700 Loss: 6.4197797775 MSE_Loss: 5.5496772528 Sparsity_loss: 3.3856854439 KL_loss: 0.0609190725 MMD_loss: 0.0461439528 time: 0.0464s\n",
      "Feature: 0003 Epoch: 0800 Loss: 6.4197756052 MSE_Loss: 5.5497423410 Sparsity_loss: 3.3854051828 KL_loss: 0.0609155297 MMD_loss: 0.0461455137 time: 0.0463s\n",
      "Feature: 0003 Epoch: 0900 Loss: 6.4193722010 MSE_Loss: 5.5496346951 Sparsity_loss: 3.3842258453 KL_loss: 0.0609152662 MMD_loss: 0.0461441316 time: 0.0474s\n",
      "Feature: 0003 Epoch: 1000 Loss: 6.4198637009 MSE_Loss: 5.5496947765 Sparsity_loss: 3.3859506845 KL_loss: 0.0609161286 MMD_loss: 0.0461439863 time: 0.0459s\n",
      "Feature: 0003 Epoch: 1100 Loss: 6.4190115929 MSE_Loss: 5.5496534109 Sparsity_loss: 3.3827087879 KL_loss: 0.0609126613 MMD_loss: 0.0461439118 time: 0.0455s\n",
      "Feature: 0003 Epoch: 1200 Loss: 6.4192956686 MSE_Loss: 5.5497473478 Sparsity_loss: 3.3834676147 KL_loss: 0.0609120280 MMD_loss: 0.0461449400 time: 0.0449s\n",
      "Feature: 0003 Epoch: 1300 Loss: 6.4191900492 MSE_Loss: 5.5497027636 Sparsity_loss: 3.3832212687 KL_loss: 0.0609058468 MMD_loss: 0.0461459868 time: 0.0496s\n",
      "Feature: 0003 Epoch: 1400 Loss: 6.4190688133 MSE_Loss: 5.5496473312 Sparsity_loss: 3.3829583526 KL_loss: 0.0608993005 MMD_loss: 0.0461462028 time: 0.0446s\n",
      "Feature: 0003 Epoch: 1500 Loss: 6.4188554287 MSE_Loss: 5.5497384071 Sparsity_loss: 3.3817436099 KL_loss: 0.0608893614 MMD_loss: 0.0461443290 time: 0.0451s\n",
      "Feature: 0003 Epoch: 1600 Loss: 6.0660287142 MSE_Loss: 5.5803524256 Sparsity_loss: 1.8480792344 KL_loss: 0.0704531278 MMD_loss: 0.0459040478 time: 0.0444s\n",
      "Feature: 0003 Epoch: 1700 Loss: 6.0655217171 MSE_Loss: 5.5803163052 Sparsity_loss: 1.8461984694 KL_loss: 0.0704496000 MMD_loss: 0.0459031127 time: 0.0450s\n",
      "Feature: 0003 Epoch: 1800 Loss: 6.0658221245 MSE_Loss: 5.5804158449 Sparsity_loss: 1.8469983935 KL_loss: 0.0704454407 MMD_loss: 0.0459047072 time: 0.0460s\n",
      "Feature: 0003 Epoch: 1900 Loss: 6.0658402443 MSE_Loss: 5.5804249048 Sparsity_loss: 1.8470335901 KL_loss: 0.0704463609 MMD_loss: 0.0459051505 time: 0.0462s\n",
      "Begin training feature: 0004\n",
      "Feature: 0004 Epoch: 0000 Loss: 12.9769234657 MSE_Loss: 10.8572883606 Sparsity_loss: 8.3817691803 KL_loss: 0.0751245767 MMD_loss: 0.0468838029 time: 0.0755s\n",
      "Feature: 0004 Epoch: 0100 Loss: 5.4570537806 MSE_Loss: 3.8112242222 Sparsity_loss: 6.4894057512 KL_loss: 0.0747749638 MMD_loss: 0.0454611592 time: 0.0459s\n",
      "Feature: 0004 Epoch: 0200 Loss: 4.5313748717 MSE_Loss: 3.8634713292 Sparsity_loss: 2.5780526400 KL_loss: 0.0616067573 MMD_loss: 0.0455488376 time: 0.0535s\n",
      "Feature: 0004 Epoch: 0300 Loss: 4.4299834371 MSE_Loss: 3.8932110667 Sparsity_loss: 2.0528033078 KL_loss: 0.0668341666 MMD_loss: 0.0458060876 time: 0.0459s\n",
      "Feature: 0004 Epoch: 0400 Loss: 4.4180482626 MSE_Loss: 3.8898218274 Sparsity_loss: 2.0188924372 KL_loss: 0.0668556560 MMD_loss: 0.0456695892 time: 0.0447s\n",
      "Feature: 0004 Epoch: 0500 Loss: 4.4420624971 MSE_Loss: 3.9166972637 Sparsity_loss: 2.0072301924 KL_loss: 0.0662130099 MMD_loss: 0.0457912982 time: 0.0449s\n",
      "Feature: 0004 Epoch: 0600 Loss: 4.4238298535 MSE_Loss: 3.9100480080 Sparsity_loss: 1.9606307447 KL_loss: 0.0668022595 MMD_loss: 0.0459123850 time: 0.0460s\n",
      "Feature: 0004 Epoch: 0700 Loss: 4.4281744361 MSE_Loss: 3.9072436094 Sparsity_loss: 1.9890080094 KL_loss: 0.0671531055 MMD_loss: 0.0460145883 time: 0.0447s\n",
      "Feature: 0004 Epoch: 0800 Loss: 4.4134032726 MSE_Loss: 3.9259953499 Sparsity_loss: 1.8550851047 KL_loss: 0.0700209681 MMD_loss: 0.0458727442 time: 0.0451s\n",
      "Feature: 0004 Epoch: 0900 Loss: 4.4134135842 MSE_Loss: 3.9272547960 Sparsity_loss: 1.8501182199 KL_loss: 0.0704103559 MMD_loss: 0.0458500870 time: 0.0457s\n",
      "Feature: 0004 Epoch: 1000 Loss: 4.4126551151 MSE_Loss: 3.9266688824 Sparsity_loss: 1.8494153321 KL_loss: 0.0703893360 MMD_loss: 0.0458567999 time: 0.0455s\n",
      "Feature: 0004 Epoch: 1100 Loss: 4.4127750397 MSE_Loss: 3.9273039103 Sparsity_loss: 1.8473693132 KL_loss: 0.0704592876 MMD_loss: 0.0458485335 time: 0.0471s\n",
      "Feature: 0004 Epoch: 1200 Loss: 4.4149819016 MSE_Loss: 3.9273403883 Sparsity_loss: 1.8560503125 KL_loss: 0.0704438593 MMD_loss: 0.0458492488 time: 0.0456s\n",
      "Feature: 0004 Epoch: 1300 Loss: 4.4128323793 MSE_Loss: 3.9273125529 Sparsity_loss: 1.8475631475 KL_loss: 0.0704584643 MMD_loss: 0.0458487011 time: 0.0464s\n",
      "Feature: 0004 Epoch: 1400 Loss: 4.4128733277 MSE_Loss: 3.9273976684 Sparsity_loss: 1.8473899364 KL_loss: 0.0704545304 MMD_loss: 0.0458475277 time: 0.0452s\n",
      "Feature: 0004 Epoch: 1500 Loss: 4.4128190875 MSE_Loss: 3.9272459149 Sparsity_loss: 1.8477730751 KL_loss: 0.0704429541 MMD_loss: 0.0458508544 time: 0.0457s\n",
      "Feature: 0004 Epoch: 1600 Loss: 4.4130614996 MSE_Loss: 3.9276181459 Sparsity_loss: 1.8472610116 KL_loss: 0.0704517514 MMD_loss: 0.0458472259 time: 0.0445s\n",
      "Feature: 0004 Epoch: 1700 Loss: 4.4126244783 MSE_Loss: 3.9274367094 Sparsity_loss: 1.8462373316 KL_loss: 0.0704522654 MMD_loss: 0.0458479226 time: 0.0459s\n",
      "Feature: 0004 Epoch: 1800 Loss: 4.4126583338 MSE_Loss: 3.9274896979 Sparsity_loss: 1.8461636901 KL_loss: 0.0704462826 MMD_loss: 0.0458468348 time: 0.0455s\n",
      "Feature: 0004 Epoch: 1900 Loss: 4.4125315547 MSE_Loss: 3.9275088906 Sparsity_loss: 1.8455896080 KL_loss: 0.0704445411 MMD_loss: 0.0458419397 time: 0.0459s\n",
      "Begin training feature: 0005\n",
      "Feature: 0005 Epoch: 0000 Loss: 11.1009891033 MSE_Loss: 8.9584206343 Sparsity_loss: 8.4562785625 KL_loss: 0.0816184469 MMD_loss: 0.0553645045 time: 0.0484s\n",
      "Feature: 0005 Epoch: 0100 Loss: 5.5106166601 MSE_Loss: 3.7678011060 Sparsity_loss: 6.8819342852 KL_loss: 0.0821885839 MMD_loss: 0.0430199765 time: 0.0454s\n",
      "Feature: 0005 Epoch: 0200 Loss: 4.5685098171 MSE_Loss: 3.7843156457 Sparsity_loss: 3.0494175553 KL_loss: 0.0445716046 MMD_loss: 0.0427884422 time: 0.0475s\n",
      "Feature: 0005 Epoch: 0300 Loss: 4.2147570848 MSE_Loss: 3.8170592189 Sparsity_loss: 1.5036038160 KL_loss: 0.0361037795 MMD_loss: 0.0428716801 time: 0.0573s\n",
      "Feature: 0005 Epoch: 0400 Loss: 4.1707831025 MSE_Loss: 3.7981431484 Sparsity_loss: 1.4038006663 KL_loss: 0.0364780976 MMD_loss: 0.0426497497 time: 0.0462s\n",
      "Feature: 0005 Epoch: 0500 Loss: 4.1630140543 MSE_Loss: 3.7888989449 Sparsity_loss: 1.4095306396 KL_loss: 0.0365026556 MMD_loss: 0.0427348763 time: 0.0460s\n",
      "Feature: 0005 Epoch: 0600 Loss: 4.1723493338 MSE_Loss: 3.7986664772 Sparsity_loss: 1.4079101980 KL_loss: 0.0363111310 MMD_loss: 0.0426843688 time: 0.0455s\n",
      "Feature: 0005 Epoch: 0700 Loss: 4.1492283940 MSE_Loss: 3.7802481651 Sparsity_loss: 1.3890714049 KL_loss: 0.0372550469 MMD_loss: 0.0426795222 time: 0.0457s\n",
      "Feature: 0005 Epoch: 0800 Loss: 4.1467339993 MSE_Loss: 3.7776466608 Sparsity_loss: 1.3893337250 KL_loss: 0.0369447907 MMD_loss: 0.0427688137 time: 0.0453s\n",
      "Feature: 0005 Epoch: 0900 Loss: 4.1578927040 MSE_Loss: 3.8013878465 Sparsity_loss: 1.3389665484 KL_loss: 0.0373288766 MMD_loss: 0.0427796207 time: 0.0462s\n",
      "Feature: 0005 Epoch: 1000 Loss: 4.1393842697 MSE_Loss: 3.7744747996 Sparsity_loss: 1.3727905750 KL_loss: 0.0373297716 MMD_loss: 0.0426771641 time: 0.0471s\n",
      "Feature: 0005 Epoch: 1100 Loss: 4.1357282996 MSE_Loss: 3.7630589008 Sparsity_loss: 1.4035821855 KL_loss: 0.0367891407 MMD_loss: 0.0428116806 time: 0.0452s\n",
      "Feature: 0005 Epoch: 1200 Loss: 4.1461339593 MSE_Loss: 3.7789133191 Sparsity_loss: 1.3818047047 KL_loss: 0.0372444298 MMD_loss: 0.0427940749 time: 0.0448s\n",
      "Feature: 0005 Epoch: 1300 Loss: 4.1415426135 MSE_Loss: 3.7687523365 Sparsity_loss: 1.4041041434 KL_loss: 0.0367940078 MMD_loss: 0.0427927114 time: 0.0450s\n",
      "Feature: 0005 Epoch: 1400 Loss: 4.1284361482 MSE_Loss: 3.7610612512 Sparsity_loss: 1.3819745481 KL_loss: 0.0371168591 MMD_loss: 0.0430197567 time: 0.0474s\n",
      "Feature: 0005 Epoch: 1500 Loss: 4.1569663286 MSE_Loss: 3.7853069901 Sparsity_loss: 1.3994484842 KL_loss: 0.0375904692 MMD_loss: 0.0428427011 time: 0.0479s\n",
      "Feature: 0005 Epoch: 1600 Loss: 4.1283496022 MSE_Loss: 3.7674846649 Sparsity_loss: 1.3562397957 KL_loss: 0.0370575339 MMD_loss: 0.0428689122 time: 0.0464s\n",
      "Feature: 0005 Epoch: 1700 Loss: 4.1334216595 MSE_Loss: 3.7589961886 Sparsity_loss: 1.4104535878 KL_loss: 0.0366231762 MMD_loss: 0.0428917706 time: 0.0458s\n",
      "Feature: 0005 Epoch: 1800 Loss: 4.1730742455 MSE_Loss: 3.8260124922 Sparsity_loss: 1.3000127971 KL_loss: 0.0380009823 MMD_loss: 0.0433571897 time: 0.0453s\n",
      "Feature: 0005 Epoch: 1900 Loss: 4.1275478005 MSE_Loss: 3.7647464275 Sparsity_loss: 1.3639765978 KL_loss: 0.0371407578 MMD_loss: 0.0428718515 time: 0.0457s\n",
      "Begin training feature: 0006\n",
      "Feature: 0006 Epoch: 0000 Loss: 17.0207912922 MSE_Loss: 14.8917706013 Sparsity_loss: 8.4040405750 KL_loss: 0.0804496072 MMD_loss: 0.0544124953 time: 0.0451s\n",
      "Feature: 0006 Epoch: 0100 Loss: 10.1239464283 MSE_Loss: 8.5746166706 Sparsity_loss: 6.1029224396 KL_loss: 0.0858288743 MMD_loss: 0.0454823636 time: 0.0578s\n",
      "Feature: 0006 Epoch: 0200 Loss: 9.5755326748 MSE_Loss: 8.5819176435 Sparsity_loss: 3.8795518875 KL_loss: 0.0978136398 MMD_loss: 0.0454983152 time: 0.0462s\n",
      "Feature: 0006 Epoch: 0300 Loss: 9.5383595228 MSE_Loss: 8.5769500732 Sparsity_loss: 3.7506423593 KL_loss: 0.1010635085 MMD_loss: 0.0454761051 time: 0.0459s\n",
      "Feature: 0006 Epoch: 0400 Loss: 9.5346229076 MSE_Loss: 8.5754829645 Sparsity_loss: 3.7415879965 KL_loss: 0.1009758264 MMD_loss: 0.0454662554 time: 0.0456s\n",
      "Feature: 0006 Epoch: 0500 Loss: 9.5334131718 MSE_Loss: 8.5748075247 Sparsity_loss: 3.7394850254 KL_loss: 0.1009987071 MMD_loss: 0.0454488955 time: 0.0470s\n",
      "Feature: 0006 Epoch: 0600 Loss: 9.5318441391 MSE_Loss: 8.5737713575 Sparsity_loss: 3.7373291850 KL_loss: 0.1009462886 MMD_loss: 0.0454613417 time: 0.0457s\n",
      "Feature: 0006 Epoch: 0700 Loss: 9.5343011618 MSE_Loss: 8.5763520002 Sparsity_loss: 3.7368090749 KL_loss: 0.1009067278 MMD_loss: 0.0454758070 time: 0.0451s\n",
      "Feature: 0006 Epoch: 0800 Loss: 9.3330864906 MSE_Loss: 8.6024693251 Sparsity_loss: 2.8282774091 KL_loss: 0.0827546157 MMD_loss: 0.0454410166 time: 0.0458s\n",
      "Feature: 0006 Epoch: 0900 Loss: 9.3310732841 MSE_Loss: 8.6034333706 Sparsity_loss: 2.8163730502 KL_loss: 0.0834024400 MMD_loss: 0.0454243645 time: 0.0465s\n",
      "Feature: 0006 Epoch: 1000 Loss: 9.3285851479 MSE_Loss: 8.6008663177 Sparsity_loss: 2.8167105317 KL_loss: 0.0834165905 MMD_loss: 0.0454137288 time: 0.0460s\n",
      "Feature: 0006 Epoch: 1100 Loss: 9.3294234276 MSE_Loss: 8.6024597883 Sparsity_loss: 2.8136886358 KL_loss: 0.0834370162 MMD_loss: 0.0454143323 time: 0.0450s\n",
      "Feature: 0006 Epoch: 1200 Loss: 9.3304542303 MSE_Loss: 8.6037629843 Sparsity_loss: 2.8125985265 KL_loss: 0.0835567210 MMD_loss: 0.0454116836 time: 0.0449s\n",
      "Feature: 0006 Epoch: 1300 Loss: 9.3284208775 MSE_Loss: 8.6008869410 Sparsity_loss: 2.8158715963 KL_loss: 0.0833878610 MMD_loss: 0.0454641618 time: 0.0447s\n",
      "Feature: 0006 Epoch: 1400 Loss: 9.3308876753 MSE_Loss: 8.6040036678 Sparsity_loss: 2.8133358955 KL_loss: 0.0835199729 MMD_loss: 0.0454299822 time: 0.0461s\n",
      "Feature: 0006 Epoch: 1500 Loss: 9.3306713104 MSE_Loss: 8.6028167009 Sparsity_loss: 2.8171362281 KL_loss: 0.0833393037 MMD_loss: 0.0454751290 time: 0.0463s\n",
      "Feature: 0006 Epoch: 1600 Loss: 9.3281610012 MSE_Loss: 8.6010955572 Sparsity_loss: 2.8141178489 KL_loss: 0.0832596272 MMD_loss: 0.0454070754 time: 0.0463s\n",
      "Feature: 0006 Epoch: 1700 Loss: 9.3300583363 MSE_Loss: 8.6033157110 Sparsity_loss: 2.8127683401 KL_loss: 0.0833579954 MMD_loss: 0.0454334915 time: 0.0464s\n",
      "Feature: 0006 Epoch: 1800 Loss: 9.3282506466 MSE_Loss: 8.6007001400 Sparsity_loss: 2.8160327673 KL_loss: 0.0831477493 MMD_loss: 0.0454215556 time: 0.0454s\n",
      "Feature: 0006 Epoch: 1900 Loss: 9.3282620907 MSE_Loss: 8.6013735533 Sparsity_loss: 2.8133602142 KL_loss: 0.0834230818 MMD_loss: 0.0454293080 time: 0.0457s\n",
      "Begin training feature: 0007\n",
      "Feature: 0007 Epoch: 0000 Loss: 13.1649529934 MSE_Loss: 11.0275738239 Sparsity_loss: 8.4346740246 KL_loss: 0.0833104029 MMD_loss: 0.0557550415 time: 0.0515s\n",
      "Feature: 0007 Epoch: 0100 Loss: 6.9173847437 MSE_Loss: 5.4334959984 Sparsity_loss: 5.8197939396 KL_loss: 0.0906412080 MMD_loss: 0.0560675189 time: 0.0650s\n",
      "Feature: 0007 Epoch: 0200 Loss: 6.2763307095 MSE_Loss: 5.4445710182 Sparsity_loss: 3.2121212482 KL_loss: 0.0527935624 MMD_loss: 0.0564025119 time: 0.0457s\n",
      "Feature: 0007 Epoch: 0300 Loss: 5.8003228903 MSE_Loss: 5.4812016487 Sparsity_loss: 1.1613839865 KL_loss: 0.0477931257 MMD_loss: 0.0565942600 time: 0.0460s\n",
      "Feature: 0007 Epoch: 0400 Loss: 5.7829834223 MSE_Loss: 5.5064246655 Sparsity_loss: 0.9908486009 KL_loss: 0.0480947578 MMD_loss: 0.0567314178 time: 0.0451s\n",
      "Feature: 0007 Epoch: 0500 Loss: 5.7707577944 MSE_Loss: 5.4921118021 Sparsity_loss: 0.9996350259 KL_loss: 0.0478889355 MMD_loss: 0.0565167479 time: 0.0503s\n",
      "Feature: 0007 Epoch: 0600 Loss: 5.7810161114 MSE_Loss: 5.4968389273 Sparsity_loss: 1.0217641890 KL_loss: 0.0478883879 MMD_loss: 0.0565148890 time: 0.0470s\n",
      "Feature: 0007 Epoch: 0700 Loss: 5.7623279095 MSE_Loss: 5.4923301935 Sparsity_loss: 0.9649760574 KL_loss: 0.0479253521 MMD_loss: 0.0565487705 time: 0.0458s\n",
      "Feature: 0007 Epoch: 0800 Loss: 5.7607462406 MSE_Loss: 5.4950466156 Sparsity_loss: 0.9475499988 KL_loss: 0.0483325301 MMD_loss: 0.0566575639 time: 0.0461s\n",
      "Feature: 0007 Epoch: 0900 Loss: 5.7625174522 MSE_Loss: 5.4966427088 Sparsity_loss: 0.9486159384 KL_loss: 0.0480016572 MMD_loss: 0.0564811043 time: 0.0461s\n",
      "Feature: 0007 Epoch: 1000 Loss: 5.7559134960 MSE_Loss: 5.4880753756 Sparsity_loss: 0.9561694264 KL_loss: 0.0481874673 MMD_loss: 0.0566279255 time: 0.0460s\n",
      "Feature: 0007 Epoch: 1100 Loss: 5.7573107481 MSE_Loss: 5.4961098433 Sparsity_loss: 0.9296982437 KL_loss: 0.0483323867 MMD_loss: 0.0565859452 time: 0.0454s\n",
      "Feature: 0007 Epoch: 1200 Loss: 5.7580232620 MSE_Loss: 5.4913274050 Sparsity_loss: 0.9516176134 KL_loss: 0.0482130339 MMD_loss: 0.0566185042 time: 0.0454s\n",
      "Feature: 0007 Epoch: 1300 Loss: 5.7542606592 MSE_Loss: 5.4886441231 Sparsity_loss: 0.9476116747 KL_loss: 0.0484567396 MMD_loss: 0.0564583279 time: 0.0464s\n",
      "Feature: 0007 Epoch: 1400 Loss: 5.7612942457 MSE_Loss: 5.4964035749 Sparsity_loss: 0.9445247501 KL_loss: 0.0486537702 MMD_loss: 0.0565458946 time: 0.0440s\n",
      "Feature: 0007 Epoch: 1500 Loss: 5.7564408779 MSE_Loss: 5.4923706055 Sparsity_loss: 0.9412825406 KL_loss: 0.0483030779 MMD_loss: 0.0565329194 time: 0.0446s\n",
      "Feature: 0007 Epoch: 1600 Loss: 5.7557779551 MSE_Loss: 5.4913433790 Sparsity_loss: 0.9428841770 KL_loss: 0.0480240881 MMD_loss: 0.0564665087 time: 0.0448s\n",
      "Feature: 0007 Epoch: 1700 Loss: 5.7542511225 MSE_Loss: 5.4866608381 Sparsity_loss: 0.9553399086 KL_loss: 0.0483310837 MMD_loss: 0.0565443635 time: 0.0467s\n",
      "Feature: 0007 Epoch: 1800 Loss: 5.7537385225 MSE_Loss: 5.4854084253 Sparsity_loss: 0.9581788927 KL_loss: 0.0482554389 MMD_loss: 0.0566059649 time: 0.0481s\n",
      "Feature: 0007 Epoch: 1900 Loss: 5.7595113516 MSE_Loss: 5.4894514084 Sparsity_loss: 0.9651147723 KL_loss: 0.0485721733 MMD_loss: 0.0565907732 time: 0.0451s\n",
      "Begin training feature: 0008\n",
      "Feature: 0008 Epoch: 0000 Loss: 10.0846104622 MSE_Loss: 8.0125676394 Sparsity_loss: 8.1954703331 KL_loss: 0.0627018232 MMD_loss: 0.0450960249 time: 0.0730s\n",
      "Feature: 0008 Epoch: 0100 Loss: 4.0340806246 MSE_Loss: 2.3572160602 Sparsity_loss: 6.6125931740 KL_loss: 0.0673104264 MMD_loss: 0.0460861139 time: 0.0451s\n",
      "Feature: 0008 Epoch: 0200 Loss: 3.1566799283 MSE_Loss: 2.4023418427 Sparsity_loss: 2.9230200648 KL_loss: 0.0305035254 MMD_loss: 0.0465561152 time: 0.0452s\n",
      "Feature: 0008 Epoch: 0300 Loss: 2.9735479355 MSE_Loss: 2.3982939124 Sparsity_loss: 2.2071821690 KL_loss: 0.0256880233 MMD_loss: 0.0464031063 time: 0.0446s\n",
      "Feature: 0008 Epoch: 0400 Loss: 2.9636128545 MSE_Loss: 2.3918545842 Sparsity_loss: 2.1932649016 KL_loss: 0.0257273605 MMD_loss: 0.0463694744 time: 0.0449s\n",
      "Feature: 0008 Epoch: 0500 Loss: 2.7519720197 MSE_Loss: 2.3924360275 Sparsity_loss: 1.3436104357 KL_loss: 0.0458033504 MMD_loss: 0.0463505574 time: 0.0469s\n",
      "Feature: 0008 Epoch: 0600 Loss: 2.7482352853 MSE_Loss: 2.3890196681 Sparsity_loss: 1.3425033391 KL_loss: 0.0447917394 MMD_loss: 0.0462837480 time: 0.0456s\n",
      "Feature: 0008 Epoch: 0700 Loss: 2.7373560071 MSE_Loss: 2.3934559822 Sparsity_loss: 1.2814437151 KL_loss: 0.0440174853 MMD_loss: 0.0461980850 time: 0.0439s\n",
      "Feature: 0008 Epoch: 0800 Loss: 2.7193888426 MSE_Loss: 2.4107825160 Sparsity_loss: 1.1406011581 KL_loss: 0.0436576530 MMD_loss: 0.0460388288 time: 0.0456s\n",
      "Feature: 0008 Epoch: 0900 Loss: 2.7147459388 MSE_Loss: 2.4051330090 Sparsity_loss: 1.1444613934 KL_loss: 0.0434993701 MMD_loss: 0.0461251996 time: 0.0444s\n",
      "Feature: 0008 Epoch: 1000 Loss: 2.7121447325 MSE_Loss: 2.3986268044 Sparsity_loss: 1.1603459716 KL_loss: 0.0433389973 MMD_loss: 0.0459962375 time: 0.0456s\n",
      "Feature: 0008 Epoch: 1100 Loss: 2.7002215981 MSE_Loss: 2.3860185146 Sparsity_loss: 1.1631609797 KL_loss: 0.0446791584 MMD_loss: 0.0459321141 time: 0.0464s\n",
      "Feature: 0008 Epoch: 1200 Loss: 2.6989405751 MSE_Loss: 2.3873892426 Sparsity_loss: 1.1521511376 KL_loss: 0.0443639718 MMD_loss: 0.0461396836 time: 0.0453s\n",
      "Feature: 0008 Epoch: 1300 Loss: 2.6981034279 MSE_Loss: 2.3898592591 Sparsity_loss: 1.1390388608 KL_loss: 0.0446940102 MMD_loss: 0.0460750684 time: 0.0461s\n",
      "Feature: 0008 Epoch: 1400 Loss: 2.6970955729 MSE_Loss: 2.3926594853 Sparsity_loss: 1.1239992976 KL_loss: 0.0449468270 MMD_loss: 0.0459736809 time: 0.0428s\n",
      "Feature: 0008 Epoch: 1500 Loss: 2.6911342740 MSE_Loss: 2.3802808523 Sparsity_loss: 1.1502096355 KL_loss: 0.0440975158 MMD_loss: 0.0457200333 time: 0.0460s\n",
      "Feature: 0008 Epoch: 1600 Loss: 2.6851873398 MSE_Loss: 2.3743301630 Sparsity_loss: 1.1497762203 KL_loss: 0.0443516709 MMD_loss: 0.0459392220 time: 0.0453s\n",
      "Feature: 0008 Epoch: 1700 Loss: 2.6783241034 MSE_Loss: 2.3718753457 Sparsity_loss: 1.1320389211 KL_loss: 0.0446375180 MMD_loss: 0.0459853858 time: 0.0453s\n",
      "Feature: 0008 Epoch: 1800 Loss: 2.6896139383 MSE_Loss: 2.3802673221 Sparsity_loss: 1.1437821388 KL_loss: 0.0445090374 MMD_loss: 0.0459120385 time: 0.0460s\n",
      "Feature: 0008 Epoch: 1900 Loss: 2.6718635559 MSE_Loss: 2.3649216890 Sparsity_loss: 1.1343995035 KL_loss: 0.0441216696 MMD_loss: 0.0458012410 time: 0.0410s\n",
      "Begin training feature: 0009\n",
      "Feature: 0009 Epoch: 0000 Loss: 11.5634546280 MSE_Loss: 9.4378886223 Sparsity_loss: 8.4102210999 KL_loss: 0.0775160734 MMD_loss: 0.0444718376 time: 0.0510s\n",
      "Feature: 0009 Epoch: 0100 Loss: 4.2512969971 MSE_Loss: 2.5536935627 Sparsity_loss: 6.7140763998 KL_loss: 0.0828865170 MMD_loss: 0.0365111157 time: 0.0530s\n",
      "Feature: 0009 Epoch: 0200 Loss: 3.3656334877 MSE_Loss: 2.5451371074 Sparsity_loss: 3.2071843147 KL_loss: 0.0491755204 MMD_loss: 0.0364168622 time: 0.0452s\n",
      "Feature: 0009 Epoch: 0300 Loss: 3.0096932650 MSE_Loss: 2.6095205843 Sparsity_loss: 1.5257056952 KL_loss: 0.0409321683 MMD_loss: 0.0366740562 time: 0.0453s\n",
      "Feature: 0009 Epoch: 0400 Loss: 2.9600206614 MSE_Loss: 2.6426665485 Sparsity_loss: 1.1941390932 KL_loss: 0.0424233964 MMD_loss: 0.0367899276 time: 0.0445s\n",
      "Feature: 0009 Epoch: 0500 Loss: 2.9459801912 MSE_Loss: 2.6470774412 Sparsity_loss: 1.1205493212 KL_loss: 0.0431757402 MMD_loss: 0.0366673097 time: 0.0465s\n",
      "Feature: 0009 Epoch: 0600 Loss: 2.9503249526 MSE_Loss: 2.6475993991 Sparsity_loss: 1.1355884671 KL_loss: 0.0429442590 MMD_loss: 0.0367983580 time: 0.0459s\n",
      "Feature: 0009 Epoch: 0700 Loss: 2.9392920136 MSE_Loss: 2.6554594040 Sparsity_loss: 1.0601968765 KL_loss: 0.0438116537 MMD_loss: 0.0366902128 time: 0.0450s\n",
      "Feature: 0009 Epoch: 0800 Loss: 2.9312899113 MSE_Loss: 2.6374517381 Sparsity_loss: 1.1002546251 KL_loss: 0.0423126454 MMD_loss: 0.0367027149 time: 0.0453s\n",
      "Feature: 0009 Epoch: 0900 Loss: 2.9224659801 MSE_Loss: 2.6236837804 Sparsity_loss: 1.1200518012 KL_loss: 0.0424337704 MMD_loss: 0.0366896465 time: 0.0455s\n",
      "Feature: 0009 Epoch: 1000 Loss: 2.9184905291 MSE_Loss: 2.6268678010 Sparsity_loss: 1.0916503668 KL_loss: 0.0432237638 MMD_loss: 0.0365558229 time: 0.0456s\n",
      "Feature: 0009 Epoch: 1100 Loss: 2.9191794991 MSE_Loss: 2.6247974932 Sparsity_loss: 1.1022118330 KL_loss: 0.0432041483 MMD_loss: 0.0367939435 time: 0.0494s\n",
      "Feature: 0009 Epoch: 1200 Loss: 2.9083594084 MSE_Loss: 2.6085297465 Sparsity_loss: 1.1242447495 KL_loss: 0.0426259022 MMD_loss: 0.0366845168 time: 0.0474s\n",
      "Feature: 0009 Epoch: 1300 Loss: 2.9111674428 MSE_Loss: 2.6018046737 Sparsity_loss: 1.1626888514 KL_loss: 0.0424010158 MMD_loss: 0.0365329683 time: 0.0451s\n",
      "Feature: 0009 Epoch: 1400 Loss: 2.9140630960 MSE_Loss: 2.6189338863 Sparsity_loss: 1.1057103574 KL_loss: 0.0434051575 MMD_loss: 0.0365348719 time: 0.0455s\n",
      "Feature: 0009 Epoch: 1500 Loss: 2.9074699879 MSE_Loss: 2.6120096743 Sparsity_loss: 1.1070014536 KL_loss: 0.0433389628 MMD_loss: 0.0365532599 time: 0.0454s\n",
      "Feature: 0009 Epoch: 1600 Loss: 2.9090833664 MSE_Loss: 2.6056337357 Sparsity_loss: 1.1389498711 KL_loss: 0.0430156300 MMD_loss: 0.0365641192 time: 0.0476s\n",
      "Feature: 0009 Epoch: 1700 Loss: 2.9036918879 MSE_Loss: 2.6058349311 Sparsity_loss: 1.1168179214 KL_loss: 0.0432216646 MMD_loss: 0.0364404730 time: 0.0460s\n",
      "Feature: 0009 Epoch: 1800 Loss: 2.9092840552 MSE_Loss: 2.6082890928 Sparsity_loss: 1.1292249262 KL_loss: 0.0430939710 MMD_loss: 0.0365153588 time: 0.0449s\n",
      "Feature: 0009 Epoch: 1900 Loss: 2.9112285972 MSE_Loss: 2.6137784719 Sparsity_loss: 1.1148521602 KL_loss: 0.0434646364 MMD_loss: 0.0366051421 time: 0.0466s\n",
      "Begin training feature: 0010\n",
      "Feature: 0010 Epoch: 0000 Loss: 14.2375016212 MSE_Loss: 12.1453950405 Sparsity_loss: 8.2669334412 KL_loss: 0.0662087724 MMD_loss: 0.0494223274 time: 0.0517s\n",
      "Feature: 0010 Epoch: 0100 Loss: 7.1858006716 MSE_Loss: 5.6201502085 Sparsity_loss: 6.1712850332 KL_loss: 0.0826441031 MMD_loss: 0.0440050848 time: 0.0468s\n",
      "Feature: 0010 Epoch: 0200 Loss: 6.5701613426 MSE_Loss: 5.6053480506 Sparsity_loss: 3.7681851387 KL_loss: 0.0864125937 MMD_loss: 0.0438059084 time: 0.0470s\n",
      "Feature: 0010 Epoch: 0300 Loss: 6.2436649799 MSE_Loss: 5.6531610489 Sparsity_loss: 2.2717013359 KL_loss: 0.0573960217 MMD_loss: 0.0440090336 time: 0.0501s\n",
      "Feature: 0010 Epoch: 0400 Loss: 6.0514560938 MSE_Loss: 5.6437609196 Sparsity_loss: 1.5400176346 KL_loss: 0.0368749388 MMD_loss: 0.0446439609 time: 0.0452s\n",
      "Feature: 0010 Epoch: 0500 Loss: 6.0202234983 MSE_Loss: 5.6284196377 Sparsity_loss: 1.4757670760 KL_loss: 0.0377607131 MMD_loss: 0.0449693017 time: 0.0473s\n",
      "Feature: 0010 Epoch: 0600 Loss: 6.0185976028 MSE_Loss: 5.6457350254 Sparsity_loss: 1.3997778594 KL_loss: 0.0380799789 MMD_loss: 0.0450746715 time: 0.0457s\n",
      "Feature: 0010 Epoch: 0700 Loss: 6.0096855164 MSE_Loss: 5.6306293011 Sparsity_loss: 1.4245033562 KL_loss: 0.0378711643 MMD_loss: 0.0451037697 time: 0.0447s\n",
      "Feature: 0010 Epoch: 0800 Loss: 5.9826786518 MSE_Loss: 5.6111359596 Sparsity_loss: 1.3946050107 KL_loss: 0.0377247892 MMD_loss: 0.0450286046 time: 0.0467s\n",
      "Feature: 0010 Epoch: 0900 Loss: 5.9815772772 MSE_Loss: 5.6146649122 Sparsity_loss: 1.3762673140 KL_loss: 0.0380036552 MMD_loss: 0.0449309088 time: 0.0462s\n",
      "Feature: 0010 Epoch: 1000 Loss: 5.9878314734 MSE_Loss: 5.6191669703 Sparsity_loss: 1.3837231994 KL_loss: 0.0370484414 MMD_loss: 0.0447265394 time: 0.0464s\n",
      "Feature: 0010 Epoch: 1100 Loss: 5.9737269878 MSE_Loss: 5.6075657606 Sparsity_loss: 1.3725358546 KL_loss: 0.0366412979 MMD_loss: 0.0453218818 time: 0.0476s\n",
      "Feature: 0010 Epoch: 1200 Loss: 5.9672560692 MSE_Loss: 5.6007872820 Sparsity_loss: 1.3740260899 KL_loss: 0.0359355183 MMD_loss: 0.0452057496 time: 0.0463s\n",
      "Feature: 0010 Epoch: 1300 Loss: 5.9658650160 MSE_Loss: 5.5976798534 Sparsity_loss: 1.3806490600 KL_loss: 0.0361545635 MMD_loss: 0.0453228392 time: 0.0453s\n",
      "Feature: 0010 Epoch: 1400 Loss: 5.9479510784 MSE_Loss: 5.5759435296 Sparsity_loss: 1.3964307308 KL_loss: 0.0351913469 MMD_loss: 0.0450957865 time: 0.0446s\n",
      "Feature: 0010 Epoch: 1500 Loss: 5.9516034126 MSE_Loss: 5.5837818980 Sparsity_loss: 1.3798233867 KL_loss: 0.0346849263 MMD_loss: 0.0450375900 time: 0.0448s\n",
      "Feature: 0010 Epoch: 1600 Loss: 5.9517028332 MSE_Loss: 5.5771527886 Sparsity_loss: 1.4068886936 KL_loss: 0.0352330608 MMD_loss: 0.0449511036 time: 0.0470s\n",
      "Feature: 0010 Epoch: 1700 Loss: 5.9392975569 MSE_Loss: 5.5658476353 Sparsity_loss: 1.4024961293 KL_loss: 0.0347565031 MMD_loss: 0.0449565277 time: 0.0536s\n",
      "Feature: 0010 Epoch: 1800 Loss: 5.9367506504 MSE_Loss: 5.5733733177 Sparsity_loss: 1.3617794514 KL_loss: 0.0348125538 MMD_loss: 0.0451685525 time: 0.0466s\n",
      "Feature: 0010 Epoch: 1900 Loss: 5.9382786751 MSE_Loss: 5.5664566159 Sparsity_loss: 1.3960445821 KL_loss: 0.0341130588 MMD_loss: 0.0449398383 time: 0.0453s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/omnisky/Public/ChenRongfa/Intrer_VAE/utils.py:215: FutureWarning: As the xlwt package is no longer maintained, the xlwt engine will be removed in a future version of pandas. This is the only engine in pandas that supports writing in the xls format. Install openpyxl and write to an xlsx file instead. You can set the option io.excel.xls.writer to 'xlwt' to silence this warning. While this option is deprecated and will also raise a warning, it can be globally set and the warning suppressed.\n",
      "  result.to_excel(filename)\n",
      "/home/omnisky/Public/ChenRongfa/Intrer_VAE/Model.py:20: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n",
      "  nn.init.xavier_normal(m.weight.data, gain=1.414)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin training feature: 0001\n",
      "Feature: 0001 Epoch: 0000 Loss: 18.0316145420 MSE_Loss: 15.9031710625 Sparsity_loss: 8.3801233768 KL_loss: 0.0732744671 MMD_loss: 0.0594986603 time: 0.0406s\n",
      "Feature: 0001 Epoch: 0100 Loss: 10.0753629208 MSE_Loss: 8.3936897516 Sparsity_loss: 6.6009343863 KL_loss: 0.0775318500 MMD_loss: 0.0551260784 time: 0.0491s\n",
      "Feature: 0001 Epoch: 0200 Loss: 9.3602738380 MSE_Loss: 8.3984992504 Sparsity_loss: 3.7265461087 KL_loss: 0.0513949450 MMD_loss: 0.0551370606 time: 0.0492s\n",
      "Feature: 0001 Epoch: 0300 Loss: 9.0289897919 MSE_Loss: 8.4877834320 Sparsity_loss: 2.0450813174 KL_loss: 0.0430744560 MMD_loss: 0.0555649363 time: 0.0460s\n",
      "Feature: 0001 Epoch: 0400 Loss: 8.9482084513 MSE_Loss: 8.5342736244 Sparsity_loss: 1.5337091684 KL_loss: 0.0498676449 MMD_loss: 0.0560285598 time: 0.0469s\n",
      "Feature: 0001 Epoch: 0500 Loss: 8.9118061066 MSE_Loss: 8.5466321707 Sparsity_loss: 1.3388533592 KL_loss: 0.0464617414 MMD_loss: 0.0562752821 time: 0.0452s\n",
      "Feature: 0001 Epoch: 0600 Loss: 8.9040131569 MSE_Loss: 8.5693243742 Sparsity_loss: 1.2169873416 KL_loss: 0.0462010624 MMD_loss: 0.0562634543 time: 0.0487s\n",
      "Feature: 0001 Epoch: 0700 Loss: 8.9094207287 MSE_Loss: 8.5757892132 Sparsity_loss: 1.2124804556 KL_loss: 0.0464418409 MMD_loss: 0.0563789420 time: 0.0463s\n",
      "Feature: 0001 Epoch: 0800 Loss: 8.8901357651 MSE_Loss: 8.5647826195 Sparsity_loss: 1.1800137162 KL_loss: 0.0453521973 MMD_loss: 0.0561641566 time: 0.0465s\n",
      "Feature: 0001 Epoch: 0900 Loss: 8.8784931898 MSE_Loss: 8.5560363531 Sparsity_loss: 1.1681479812 KL_loss: 0.0461142929 MMD_loss: 0.0562281460 time: 0.0456s\n",
      "Feature: 0001 Epoch: 1000 Loss: 8.8760187626 MSE_Loss: 8.5545941591 Sparsity_loss: 1.1638571918 KL_loss: 0.0466479743 MMD_loss: 0.0562565736 time: 0.0460s\n",
      "Feature: 0001 Epoch: 1100 Loss: 8.8713058233 MSE_Loss: 8.5560632944 Sparsity_loss: 1.1391740441 KL_loss: 0.0459984066 MMD_loss: 0.0562985241 time: 0.0472s\n",
      "Feature: 0001 Epoch: 1200 Loss: 8.8798661232 MSE_Loss: 8.5613818169 Sparsity_loss: 1.1520685554 KL_loss: 0.0456172209 MMD_loss: 0.0563721210 time: 0.0455s\n",
      "Feature: 0001 Epoch: 1300 Loss: 8.8576971292 MSE_Loss: 8.5425758362 Sparsity_loss: 1.1385874152 KL_loss: 0.0462848069 MMD_loss: 0.0563205555 time: 0.0467s\n",
      "Feature: 0001 Epoch: 1400 Loss: 8.8668085337 MSE_Loss: 8.5469055176 Sparsity_loss: 1.1579021811 KL_loss: 0.0460949624 MMD_loss: 0.0562454499 time: 0.0455s\n",
      "Feature: 0001 Epoch: 1500 Loss: 8.8673752546 MSE_Loss: 8.5519659519 Sparsity_loss: 1.1395036876 KL_loss: 0.0464773513 MMD_loss: 0.0564193204 time: 0.0526s\n",
      "Feature: 0001 Epoch: 1600 Loss: 8.8615733385 MSE_Loss: 8.5483542681 Sparsity_loss: 1.1308713853 KL_loss: 0.0457902439 MMD_loss: 0.0564239733 time: 0.0473s\n",
      "Feature: 0001 Epoch: 1700 Loss: 8.8605581522 MSE_Loss: 8.5497189760 Sparsity_loss: 1.1215218008 KL_loss: 0.0459856838 MMD_loss: 0.0563193448 time: 0.0532s\n",
      "Feature: 0001 Epoch: 1800 Loss: 8.8565882444 MSE_Loss: 8.5431915522 Sparsity_loss: 1.1318950355 KL_loss: 0.0461525675 MMD_loss: 0.0562307090 time: 0.0462s\n",
      "Feature: 0001 Epoch: 1900 Loss: 8.8552340269 MSE_Loss: 8.5390232801 Sparsity_loss: 1.1430023313 KL_loss: 0.0458927155 MMD_loss: 0.0563316159 time: 0.0470s\n",
      "Begin training feature: 0002\n",
      "Feature: 0002 Epoch: 0000 Loss: 10.1869246960 MSE_Loss: 8.0534261465 Sparsity_loss: 8.4341552258 KL_loss: 0.0810897574 MMD_loss: 0.0418108329 time: 0.0499s\n",
      "Feature: 0002 Epoch: 0100 Loss: 5.1103971004 MSE_Loss: 3.6260648966 Sparsity_loss: 5.8523502350 KL_loss: 0.0719604772 MMD_loss: 0.0352933891 time: 0.0469s\n",
      "Feature: 0002 Epoch: 0200 Loss: 4.4395821095 MSE_Loss: 3.6158493161 Sparsity_loss: 3.2061179280 KL_loss: 0.0872268900 MMD_loss: 0.0356838331 time: 0.0457s\n",
      "Feature: 0002 Epoch: 0300 Loss: 4.2075864077 MSE_Loss: 3.6313162446 Sparsity_loss: 2.2197414637 KL_loss: 0.0694214962 MMD_loss: 0.0357273147 time: 0.0547s\n",
      "Feature: 0002 Epoch: 0400 Loss: 4.1930608153 MSE_Loss: 3.6349618435 Sparsity_loss: 2.1462202668 KL_loss: 0.0685109403 MMD_loss: 0.0362370610 time: 0.0464s\n",
      "Feature: 0002 Epoch: 0500 Loss: 4.1602836847 MSE_Loss: 3.6114853024 Sparsity_loss: 2.1103838682 KL_loss: 0.0674412940 MMD_loss: 0.0356608666 time: 0.0459s\n",
      "Feature: 0002 Epoch: 0600 Loss: 3.9817223549 MSE_Loss: 3.6679906845 Sparsity_loss: 1.1736529469 KL_loss: 0.0489531010 MMD_loss: 0.0357418172 time: 0.0504s\n",
      "Feature: 0002 Epoch: 0700 Loss: 3.9695620537 MSE_Loss: 3.6590186954 Sparsity_loss: 1.1610031724 KL_loss: 0.0490710475 MMD_loss: 0.0356782079 time: 0.0475s\n",
      "Feature: 0002 Epoch: 0800 Loss: 3.9552763700 MSE_Loss: 3.6470826268 Sparsity_loss: 1.1512543559 KL_loss: 0.0488612745 MMD_loss: 0.0358742513 time: 0.0459s\n",
      "Feature: 0002 Epoch: 0900 Loss: 3.9461892843 MSE_Loss: 3.6393171549 Sparsity_loss: 1.1466902494 KL_loss: 0.0490240362 MMD_loss: 0.0354969054 time: 0.0446s\n",
      "Feature: 0002 Epoch: 1000 Loss: 3.9593647718 MSE_Loss: 3.6546881795 Sparsity_loss: 1.1379946172 KL_loss: 0.0489985682 MMD_loss: 0.0354558527 time: 0.0451s\n",
      "Feature: 0002 Epoch: 1100 Loss: 3.9369747639 MSE_Loss: 3.6262744665 Sparsity_loss: 1.1626446843 KL_loss: 0.0485148914 MMD_loss: 0.0352268033 time: 0.0452s\n",
      "Feature: 0002 Epoch: 1200 Loss: 3.9272587299 MSE_Loss: 3.6147124171 Sparsity_loss: 1.1698393822 KL_loss: 0.0484838793 MMD_loss: 0.0353246666 time: 0.0465s\n",
      "Feature: 0002 Epoch: 1300 Loss: 3.9271191359 MSE_Loss: 3.6154315472 Sparsity_loss: 1.1666552424 KL_loss: 0.0480176909 MMD_loss: 0.0352457687 time: 0.0460s\n",
      "Feature: 0002 Epoch: 1400 Loss: 3.9287434220 MSE_Loss: 3.6132135391 Sparsity_loss: 1.1815111041 KL_loss: 0.0482069086 MMD_loss: 0.0354834571 time: 0.0459s\n",
      "Feature: 0002 Epoch: 1500 Loss: 3.9232138991 MSE_Loss: 3.6135853529 Sparsity_loss: 1.1585064828 KL_loss: 0.0480293762 MMD_loss: 0.0352010317 time: 0.0488s\n",
      "Feature: 0002 Epoch: 1600 Loss: 3.9277182221 MSE_Loss: 3.6167135835 Sparsity_loss: 1.1636998653 KL_loss: 0.0478862077 MMD_loss: 0.0353707969 time: 0.0448s\n",
      "Feature: 0002 Epoch: 1700 Loss: 3.9231759310 MSE_Loss: 3.6143481135 Sparsity_loss: 1.1552147865 KL_loss: 0.0482498379 MMD_loss: 0.0352231041 time: 0.0472s\n",
      "Feature: 0002 Epoch: 1800 Loss: 3.9116523266 MSE_Loss: 3.6012637019 Sparsity_loss: 1.1617876589 KL_loss: 0.0478995591 MMD_loss: 0.0350936353 time: 0.0456s\n",
      "Feature: 0002 Epoch: 1900 Loss: 3.9215520620 MSE_Loss: 3.6089430451 Sparsity_loss: 1.1703567803 KL_loss: 0.0479999660 MMD_loss: 0.0352396294 time: 0.0457s\n",
      "Begin training feature: 0003\n",
      "Feature: 0003 Epoch: 0000 Loss: 13.8389000893 MSE_Loss: 11.7073287964 Sparsity_loss: 8.4069201946 KL_loss: 0.0791238789 MMD_loss: 0.0517703444 time: 0.0498s\n",
      "Feature: 0003 Epoch: 0100 Loss: 7.0697243214 MSE_Loss: 5.5451703072 Sparsity_loss: 5.9930129051 KL_loss: 0.0643248633 MMD_loss: 0.0461686812 time: 0.0522s\n",
      "Feature: 0003 Epoch: 0200 Loss: 6.4554809332 MSE_Loss: 5.5589854717 Sparsity_loss: 3.4812957644 KL_loss: 0.0612213481 MMD_loss: 0.0462209135 time: 0.0460s\n",
      "Feature: 0003 Epoch: 0300 Loss: 6.2644509077 MSE_Loss: 5.5646759272 Sparsity_loss: 2.6981974840 KL_loss: 0.0428555487 MMD_loss: 0.0461655594 time: 0.0472s\n",
      "Feature: 0003 Epoch: 0400 Loss: 6.2593182325 MSE_Loss: 5.5647110939 Sparsity_loss: 2.6775252819 KL_loss: 0.0428571599 MMD_loss: 0.0461654216 time: 0.0463s\n",
      "Feature: 0003 Epoch: 0500 Loss: 6.2576750517 MSE_Loss: 5.5646576881 Sparsity_loss: 2.6711669564 KL_loss: 0.0428541498 MMD_loss: 0.0461655408 time: 0.0456s\n",
      "Feature: 0003 Epoch: 0600 Loss: 6.2573596239 MSE_Loss: 5.5646820068 Sparsity_loss: 2.6698072553 KL_loss: 0.0428539449 MMD_loss: 0.0461656563 time: 0.0455s\n",
      "Feature: 0003 Epoch: 0700 Loss: 6.2570103407 MSE_Loss: 5.5646865368 Sparsity_loss: 2.6683930755 KL_loss: 0.0428539226 MMD_loss: 0.0461653806 time: 0.0461s\n",
      "Feature: 0003 Epoch: 0800 Loss: 6.0969746113 MSE_Loss: 5.6038219929 Sparsity_loss: 1.8666629195 KL_loss: 0.0704409685 MMD_loss: 0.0459297150 time: 0.0475s\n",
      "Feature: 0003 Epoch: 0900 Loss: 6.0690019131 MSE_Loss: 5.5799969435 Sparsity_loss: 1.8501205742 KL_loss: 0.0704581179 MMD_loss: 0.0459038392 time: 0.0479s\n",
      "Feature: 0003 Epoch: 1000 Loss: 6.0686676502 MSE_Loss: 5.5799984932 Sparsity_loss: 1.8487780988 KL_loss: 0.0704587940 MMD_loss: 0.0459037535 time: 0.0479s\n",
      "Feature: 0003 Epoch: 1100 Loss: 6.0688524246 MSE_Loss: 5.5799964666 Sparsity_loss: 1.8495253325 KL_loss: 0.0704582408 MMD_loss: 0.0459037945 time: 0.0461s\n",
      "Feature: 0003 Epoch: 1200 Loss: 6.0684087276 MSE_Loss: 5.5800008774 Sparsity_loss: 1.8477324247 KL_loss: 0.0704593770 MMD_loss: 0.0459039025 time: 0.0458s\n",
      "Feature: 0003 Epoch: 1300 Loss: 6.0690143108 MSE_Loss: 5.5799983740 Sparsity_loss: 1.8501644731 KL_loss: 0.0704574548 MMD_loss: 0.0459038019 time: 0.0446s\n",
      "Feature: 0003 Epoch: 1400 Loss: 6.0691959858 MSE_Loss: 5.5804162025 Sparsity_loss: 1.8492240310 KL_loss: 0.0704524796 MMD_loss: 0.0459019281 time: 0.0469s\n",
      "Feature: 0003 Epoch: 1500 Loss: 6.0682215691 MSE_Loss: 5.5800629854 Sparsity_loss: 1.8467352390 KL_loss: 0.0704545267 MMD_loss: 0.0459042862 time: 0.0463s\n",
      "Feature: 0003 Epoch: 1600 Loss: 6.0681438446 MSE_Loss: 5.5800905228 Sparsity_loss: 1.8463144004 KL_loss: 0.0704508144 MMD_loss: 0.0459041521 time: 0.0452s\n",
      "Feature: 0003 Epoch: 1700 Loss: 6.0710791349 MSE_Loss: 5.5830335617 Sparsity_loss: 1.8462605476 KL_loss: 0.0704292655 MMD_loss: 0.0459177308 time: 0.0461s\n",
      "Feature: 0003 Epoch: 1800 Loss: 6.0680203438 MSE_Loss: 5.5801777840 Sparsity_loss: 1.8454630077 KL_loss: 0.0704363063 MMD_loss: 0.0459096804 time: 0.0452s\n",
      "Feature: 0003 Epoch: 1900 Loss: 6.0680841208 MSE_Loss: 5.5801123381 Sparsity_loss: 1.8459898531 KL_loss: 0.0704444423 MMD_loss: 0.0459040292 time: 0.0461s\n",
      "Begin training feature: 0004\n",
      "Feature: 0004 Epoch: 0000 Loss: 14.1191871166 MSE_Loss: 12.0285286903 Sparsity_loss: 8.2555699348 KL_loss: 0.0662656352 MMD_loss: 0.0469059534 time: 0.0493s\n",
      "Feature: 0004 Epoch: 0100 Loss: 5.4537825584 MSE_Loss: 3.8130708337 Sparsity_loss: 6.4564980268 KL_loss: 0.0772266947 MMD_loss: 0.0454513580 time: 0.0456s\n",
      "Feature: 0004 Epoch: 0200 Loss: 4.6509354711 MSE_Loss: 3.8510728478 Sparsity_loss: 3.0965689421 KL_loss: 0.0597439408 MMD_loss: 0.0454664752 time: 0.0503s\n",
      "Feature: 0004 Epoch: 0300 Loss: 4.4348998666 MSE_Loss: 3.8868608475 Sparsity_loss: 2.0875105262 KL_loss: 0.0664587878 MMD_loss: 0.0456770808 time: 0.0472s\n",
      "Feature: 0004 Epoch: 0400 Loss: 4.4314994812 MSE_Loss: 3.9039487839 Sparsity_loss: 2.0052967668 KL_loss: 0.0668953378 MMD_loss: 0.0457633175 time: 0.0457s\n",
      "Feature: 0004 Epoch: 0500 Loss: 4.4347887039 MSE_Loss: 3.9010574222 Sparsity_loss: 2.0296362042 KL_loss: 0.0673894677 MMD_loss: 0.0459053367 time: 0.0458s\n",
      "Feature: 0004 Epoch: 0600 Loss: 4.4230195284 MSE_Loss: 3.8900459409 Sparsity_loss: 2.0270168185 KL_loss: 0.0665956810 MMD_loss: 0.0457791202 time: 0.0450s\n",
      "Feature: 0004 Epoch: 0700 Loss: 4.4143297076 MSE_Loss: 3.8886843324 Sparsity_loss: 1.9977782667 KL_loss: 0.0665032472 MMD_loss: 0.0457513183 time: 0.0459s\n",
      "Feature: 0004 Epoch: 0800 Loss: 4.4123032093 MSE_Loss: 3.8896964788 Sparsity_loss: 1.9857738316 KL_loss: 0.0662832875 MMD_loss: 0.0456980616 time: 0.0448s\n",
      "Feature: 0004 Epoch: 0900 Loss: 4.4131650925 MSE_Loss: 3.9207856655 Sparsity_loss: 1.8640347421 KL_loss: 0.0690544210 MMD_loss: 0.0458362848 time: 0.0457s\n",
      "Feature: 0004 Epoch: 1000 Loss: 4.4151908755 MSE_Loss: 3.9246440530 Sparsity_loss: 1.8565567434 KL_loss: 0.0696980543 MMD_loss: 0.0458454266 time: 0.0481s\n",
      "Feature: 0004 Epoch: 1100 Loss: 4.4151548147 MSE_Loss: 3.9259855747 Sparsity_loss: 1.8509730995 KL_loss: 0.0700803045 MMD_loss: 0.0458441265 time: 0.0457s\n",
      "Feature: 0004 Epoch: 1200 Loss: 4.4152731895 MSE_Loss: 3.9257125854 Sparsity_loss: 1.8525685668 KL_loss: 0.0699314196 MMD_loss: 0.0458440520 time: 0.0457s\n",
      "Feature: 0004 Epoch: 1300 Loss: 4.4155520797 MSE_Loss: 3.9272998571 Sparsity_loss: 1.8472204506 KL_loss: 0.0704594180 MMD_loss: 0.0458485931 time: 0.0447s\n",
      "Feature: 0004 Epoch: 1400 Loss: 4.4153885841 MSE_Loss: 3.9270094633 Sparsity_loss: 1.8477157950 KL_loss: 0.0704566110 MMD_loss: 0.0458546020 time: 0.0462s\n",
      "Feature: 0004 Epoch: 1500 Loss: 4.4159219861 MSE_Loss: 3.9275838733 Sparsity_loss: 1.8475640714 KL_loss: 0.0704537034 MMD_loss: 0.0458490364 time: 0.0453s\n",
      "Feature: 0004 Epoch: 1600 Loss: 4.4157271385 MSE_Loss: 3.9274644852 Sparsity_loss: 1.8472597599 KL_loss: 0.0704448447 MMD_loss: 0.0458507165 time: 0.0452s\n",
      "Feature: 0004 Epoch: 1700 Loss: 4.4155719876 MSE_Loss: 3.9274872541 Sparsity_loss: 1.8465526104 KL_loss: 0.0704454146 MMD_loss: 0.0458489694 time: 0.0445s\n",
      "Feature: 0004 Epoch: 1800 Loss: 4.4154878259 MSE_Loss: 3.9275354743 Sparsity_loss: 1.8460224867 KL_loss: 0.0704493169 MMD_loss: 0.0458487011 time: 0.0452s\n",
      "Feature: 0004 Epoch: 1900 Loss: 4.4153284431 MSE_Loss: 3.9274297953 Sparsity_loss: 1.8458098769 KL_loss: 0.0704451390 MMD_loss: 0.0458479114 time: 0.0467s\n",
      "Begin training feature: 0005\n",
      "Feature: 0005 Epoch: 0000 Loss: 11.5861113071 MSE_Loss: 9.4623044729 Sparsity_loss: 8.3684604168 KL_loss: 0.0728047304 MMD_loss: 0.0561027527 time: 0.0490s\n",
      "Feature: 0005 Epoch: 0100 Loss: 5.5213707685 MSE_Loss: 3.7730030417 Sparsity_loss: 6.8929988146 KL_loss: 0.0779722352 MMD_loss: 0.0424388833 time: 0.0532s\n",
      "Feature: 0005 Epoch: 0200 Loss: 4.5783367157 MSE_Loss: 3.7894675732 Sparsity_loss: 3.0608435273 KL_loss: 0.0418938678 MMD_loss: 0.0431273282 time: 0.0460s\n",
      "Feature: 0005 Epoch: 0300 Loss: 4.2257125378 MSE_Loss: 3.8249799013 Sparsity_loss: 1.5101693273 KL_loss: 0.0350399744 MMD_loss: 0.0428762138 time: 0.0477s\n",
      "Feature: 0005 Epoch: 0400 Loss: 4.1790359020 MSE_Loss: 3.8056948185 Sparsity_loss: 1.4009585679 KL_loss: 0.0358305536 MMD_loss: 0.0426198617 time: 0.0451s\n",
      "Feature: 0005 Epoch: 0500 Loss: 4.1675415635 MSE_Loss: 3.7786186337 Sparsity_loss: 1.4635240436 KL_loss: 0.0330549227 MMD_loss: 0.0427782722 time: 0.0475s\n",
      "Feature: 0005 Epoch: 0600 Loss: 4.1636672020 MSE_Loss: 3.7822117209 Sparsity_loss: 1.4336418509 KL_loss: 0.0343522811 MMD_loss: 0.0426545776 time: 0.0478s\n",
      "Feature: 0005 Epoch: 0700 Loss: 4.1633688211 MSE_Loss: 3.7750983238 Sparsity_loss: 1.4600197971 KL_loss: 0.0339624435 MMD_loss: 0.0431350768 time: 0.0464s\n",
      "Feature: 0005 Epoch: 0800 Loss: 4.1431609392 MSE_Loss: 3.7637211680 Sparsity_loss: 1.4251486957 KL_loss: 0.0335345361 MMD_loss: 0.0429515280 time: 0.0478s\n",
      "Feature: 0005 Epoch: 0900 Loss: 4.1353530288 MSE_Loss: 3.7568801045 Sparsity_loss: 1.4217072129 KL_loss: 0.0341463396 MMD_loss: 0.0426776670 time: 0.0469s\n",
      "Feature: 0005 Epoch: 1000 Loss: 4.1287506819 MSE_Loss: 3.7594770193 Sparsity_loss: 1.3843472302 KL_loss: 0.0346011585 MMD_loss: 0.0429134741 time: 0.0459s\n",
      "Feature: 0005 Epoch: 1100 Loss: 4.1300792694 MSE_Loss: 3.7686874270 Sparsity_loss: 1.3527647257 KL_loss: 0.0351946605 MMD_loss: 0.0428819396 time: 0.0483s\n",
      "Feature: 0005 Epoch: 1200 Loss: 4.1269533038 MSE_Loss: 3.7535513639 Sparsity_loss: 1.4010532796 KL_loss: 0.0348010501 MMD_loss: 0.0427970812 time: 0.0452s\n",
      "Feature: 0005 Epoch: 1300 Loss: 4.1248844862 MSE_Loss: 3.7368804216 Sparsity_loss: 1.4597494304 KL_loss: 0.0336573478 MMD_loss: 0.0427677669 time: 0.0466s\n",
      "Feature: 0005 Epoch: 1400 Loss: 4.1129064560 MSE_Loss: 3.7365708351 Sparsity_loss: 1.4129215479 KL_loss: 0.0342089320 MMD_loss: 0.0427899063 time: 0.0474s\n"
     ]
    }
   ],
   "source": [
    "for mmd in [0.5]:\n",
    "    for sparsity in [0.25]:\n",
    "        for beta_kl in [0.01, 0.05]:\n",
    "\n",
    "            args.beta_mmd = mmd\n",
    "            args.beta_sparsity = sparsity\n",
    "            args.beta_kl = beta_kl\n",
    "            root_fodler = r'/home/omnisky/Public/ChenRongfa/Intrer_VAE_result/11Lorenz96.' + str(args.num_nodes) + '.' + str(args.time_length) + '/mmd' + str(args.beta_mmd) + '/sparsity' + str(args.beta_sparsity) + '/beta_kl' + str(args.beta_kl)\n",
    "            if not os.path.exists(root_fodler):\n",
    "                os.makedirs(root_fodler)\n",
    "            for idx in range(10):\n",
    "                print('Begin training feature: {:04d}'.format(idx + 1))\n",
    "                decoder_file = 'decoder' + str(idx) + '.pt'\n",
    "                decoder_file = os.path.join('/home/omnisky/Public/ChenRongfa/Intrer_VAE_result/Lorenz96.10.250/help', decoder_file)\n",
    "                encoder_file = 'encoder' + str(idx) + '.pt'\n",
    "                encoder_file = os.path.join(root_fodler, encoder_file)\n",
    "\n",
    "                Inter_decoder = decoder(args.dims, args.decoder_hidden, args.time_step - 1, args.num_nodes, args.decoder_dropout, args.decoder_alpha)\n",
    "                Inter_decoder.load_state_dict(torch.load(decoder_file))\n",
    "                Inter_decoder = Inter_decoder.cuda()\n",
    "                Inter_decoder.eval()\n",
    "\n",
    "                Inter_encoder = encoder(init_adj, args.dims, args.encoder_hidden, args.dims, args.time_step - 1, args.encoder_dropout, args.encoder_alpha)\n",
    "                Inter_encoder = Inter_encoder.cuda()\n",
    "\n",
    "                optimizer = optim.Adam(Inter_encoder.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
    "                loss_mse = nn.MSELoss()\n",
    "                best_loss = np.inf\n",
    "                    \n",
    "                for epoch in range(2000):\n",
    "                    t = time.time()\n",
    "                    Loss = []\n",
    "                    MSE_loss = []\n",
    "                    SPA_loss = []\n",
    "                    KL_loss = []\n",
    "                    MMD_loss = []\n",
    "                    for batch_idx, data in enumerate(data_loader):\n",
    "                        optimizer.zero_grad()\n",
    "                        data = data.cuda()\n",
    "                        target = data[:, idx, 1:, :]\n",
    "                        inputs = data[:, :, :-1, :]\n",
    "\n",
    "                        mu, log_var = Inter_encoder(inputs)  #Inter_encoder(inputs, adj)\n",
    "                        sigma = torch.exp(log_var / 2)\n",
    "                        # sigma2 = torch.exp(log_var2 / 2)\n",
    "                        gamma = torch.randn(size = mu.size()).cuda()\n",
    "                        # theta = torch.randn(size = mu2.size()).cuda()\n",
    "                        gamma = mu + sigma * gamma\n",
    "                        # theta = mu2 + sigma2 * theta\n",
    "                        mask = torch.sigmoid(gamma) #* torch.sigmoid(theta) #* torch.sigmoid(theta + gamma)\n",
    "                        # gamma = torch.sigmoid(gamma)\n",
    "                        # theta = torch.sigmoid(theta)\n",
    "\n",
    "                        inputs = mask_inputs(mask, inputs)\n",
    "                        pred = Inter_decoder(inputs, idx)   #Inter_decoder(inputs, adj, idx)\n",
    "\n",
    "\n",
    "\n",
    "                        mse_loss = loss_mse(pred, target)\n",
    "                        spa_loss = loss_sparsity(mask, 'log_sum')\n",
    "                        kl_loss = loss_divergence(mask, 'JS')\n",
    "                        mmd_loss = loss_mmd(data[:, :, 1:, :], pred, idx)\n",
    "\n",
    "                        loss = mse_loss + args.beta_sparsity * spa_loss + args.beta_kl * kl_loss + args.beta_mmd * mmd_loss\n",
    "\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                        Loss.append(loss.item())\n",
    "                        MSE_loss.append(mse_loss.item())\n",
    "                        SPA_loss.append(spa_loss.item())\n",
    "                        KL_loss.append(kl_loss.item())\n",
    "                        MMD_loss.append(mmd_loss.item())\n",
    "                        \n",
    "                    # if epoch == 500:\n",
    "                    #     optimizer.param_groups[0]['lr'] = args.lr/10\n",
    "\n",
    "                    if epoch % 100 == 0:\n",
    "                        print(  'Feature: {:04d}'.format(idx + 1),\n",
    "                                'Epoch: {:04d}'.format(epoch),\n",
    "                                'Loss: {:.10f}'.format(np.mean(Loss)),\n",
    "                                'MSE_Loss: {:.10f}'.format(np.mean(MSE_loss)),\n",
    "                                'Sparsity_loss: {:.10f}'.format(np.mean(SPA_loss)),\n",
    "                                'KL_loss: {:.10f}'.format(np.mean(KL_loss)),\n",
    "                                'MMD_loss: {:.10f}'.format(np.mean(MMD_loss)),\n",
    "                                'time: {:.4f}s'.format(time.time() - t))\n",
    "                                \n",
    "                    if np.mean(Loss) < best_loss:\n",
    "                        best_loss = np.mean(Loss)\n",
    "                        #M[idx, :] = \n",
    "                        # gamma_matrix[idx, :] = gamma.squeeze().mean(dim=2).mean(dim=1)\n",
    "                        # theta_matrix[idx, :] = theta.squeeze().mean(dim=2).mean(dim=1)\n",
    "                        # torch.save({\n",
    "                        #             'encoder_state_dict': Inter_encoder.state_dict(),\n",
    "                        #             'decoder_state_dict': Inter_decoder.state_dict(),\n",
    "                        #                 # 'adj' : adj\n",
    "\n",
    "                        #             }, encoder_file)\n",
    "                        torch.save(Inter_encoder.state_dict(), encoder_file)\n",
    "                            # np.save(save_file + str(idx) + '.npy', mask.cpu().detach().numpy())\n",
    "\n",
    "                        # print('Feature: {:04d}'.format(idx + 1),\n",
    "                        #       'Epoch: {:04d}'.format(epoch),\n",
    "                        #       'Loss: {:.10f}'.format(np.mean(Loss)),\n",
    "                        #       'mse_loss: {:.10f}'.format(np.mean(MSE_loss)),\n",
    "                        #       'Sparsity_loss: {:.10f}'.format(np.mean(SPA_loss)),\n",
    "                        #       'KL_loss: {:.10f}'.format(np.mean(KL_loss)),\n",
    "                        #       #'mmd_loss: {:.10f}'.format(np.mean(mmd_loss)),\n",
    "                        #       # 'time: {:.4f}s'.format(time.time() - t), file=log)\n",
    "            causality_matrix = []\n",
    "            for idx in range(10):\n",
    "                encoder_file = 'encoder' + str(idx) + '.pt'\n",
    "                encoder_file = os.path.join(root_fodler, encoder_file)\n",
    "                est_net = encoder(init_adj, args.dims, args.encoder_hidden, args.dims, args.time_step - 1, args.encoder_dropout, args.encoder_alpha)\n",
    "                est_net.load_state_dict(torch.load(encoder_file))\n",
    "                est_net.eval()\n",
    "                inputs = X_np[:, :, :-1, :]#.cuda()   #:-1和1:有什么区别\n",
    "                mu, log_var = est_net(inputs)\n",
    "                sigma = torch.exp(log_var / 2)\n",
    "                gamma = torch.randn(size = mu.size())\n",
    "                gamma = mu + sigma * gamma\n",
    "                mask_matrix = torch.sigmoid(gamma) #* torch.sigmoid(theta)\n",
    "                mask_matrix = mask_matrix.squeeze()\n",
    "                causality_matrix.append(mask_matrix)\n",
    "            causality_matrix = torch.stack(causality_matrix, dim=1)\n",
    "            adj_gca = causality_matrix.mean(dim=3).mean(dim=0)\n",
    "            result, _ = evaluate_result(GC, adj_gca.detach().numpy(), 0.5)\n",
    "            np.save(root_fodler, 'adj.npy', adj_gca.detach().numpy())\n",
    "            save_result(result, 'encoder', root_fodler)\n",
    "            \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.855, 'precision': 1.0, 'recall': 0.275, 'F1': 0.4313725490196079, 'ROC_AUC': 0.9642578125000001, 'PR_AUC': 0.9109595841158191}\n",
      "{'accuracy': 0.855, 'precision': 1.0, 'recall': 0.275, 'F1': 0.4313725490196079, 'ROC_AUC': 0.9642578125000001, 'PR_AUC': 0.9109595841158191}\n",
      "{'accuracy': 0.855, 'precision': 1.0, 'recall': 0.275, 'F1': 0.4313725490196079, 'ROC_AUC': 0.9642578125000001, 'PR_AUC': 0.9109595841158191}\n",
      "{'accuracy': 0.855, 'precision': 1.0, 'recall': 0.275, 'F1': 0.4313725490196079, 'ROC_AUC': 0.9642578125000001, 'PR_AUC': 0.9109595841158191}\n",
      "{'accuracy': 0.855, 'precision': 1.0, 'recall': 0.275, 'F1': 0.4313725490196079, 'ROC_AUC': 0.9642578125000001, 'PR_AUC': 0.9109595841158191}\n",
      "{'accuracy': 0.855, 'precision': 1.0, 'recall': 0.275, 'F1': 0.4313725490196079, 'ROC_AUC': 0.9642578125000001, 'PR_AUC': 0.9109595841158191}\n",
      "{'accuracy': 0.855, 'precision': 1.0, 'recall': 0.275, 'F1': 0.4313725490196079, 'ROC_AUC': 0.9642578125000001, 'PR_AUC': 0.9109595841158191}\n",
      "{'accuracy': 0.855, 'precision': 1.0, 'recall': 0.275, 'F1': 0.4313725490196079, 'ROC_AUC': 0.9642578125000001, 'PR_AUC': 0.9109595841158191}\n",
      "{'accuracy': 0.855, 'precision': 1.0, 'recall': 0.275, 'F1': 0.4313725490196079, 'ROC_AUC': 0.9642578125000001, 'PR_AUC': 0.9109595841158191}\n",
      "{'accuracy': 0.855, 'precision': 1.0, 'recall': 0.275, 'F1': 0.4313725490196079, 'ROC_AUC': 0.9642578125000001, 'PR_AUC': 0.9109595841158191}\n",
      "{'accuracy': 0.855, 'precision': 1.0, 'recall': 0.275, 'F1': 0.4313725490196079, 'ROC_AUC': 0.9642578125000001, 'PR_AUC': 0.9109595841158191}\n",
      "{'accuracy': 0.855, 'precision': 1.0, 'recall': 0.275, 'F1': 0.4313725490196079, 'ROC_AUC': 0.9642578125000001, 'PR_AUC': 0.9109595841158191}\n",
      "{'accuracy': 0.855, 'precision': 1.0, 'recall': 0.275, 'F1': 0.4313725490196079, 'ROC_AUC': 0.9642578125000001, 'PR_AUC': 0.9109595841158191}\n",
      "{'accuracy': 0.855, 'precision': 1.0, 'recall': 0.275, 'F1': 0.4313725490196079, 'ROC_AUC': 0.9642578125000001, 'PR_AUC': 0.9109595841158191}\n",
      "{'accuracy': 0.855, 'precision': 1.0, 'recall': 0.275, 'F1': 0.4313725490196079, 'ROC_AUC': 0.9642578125000001, 'PR_AUC': 0.9109595841158191}\n",
      "{'accuracy': 0.855, 'precision': 1.0, 'recall': 0.275, 'F1': 0.4313725490196079, 'ROC_AUC': 0.9642578125000001, 'PR_AUC': 0.9109595841158191}\n",
      "{'accuracy': 0.855, 'precision': 1.0, 'recall': 0.275, 'F1': 0.4313725490196079, 'ROC_AUC': 0.9642578125000001, 'PR_AUC': 0.9109595841158191}\n",
      "{'accuracy': 0.855, 'precision': 1.0, 'recall': 0.275, 'F1': 0.4313725490196079, 'ROC_AUC': 0.9642578125000001, 'PR_AUC': 0.9109595841158191}\n",
      "{'accuracy': 0.855, 'precision': 1.0, 'recall': 0.275, 'F1': 0.4313725490196079, 'ROC_AUC': 0.9642578125000001, 'PR_AUC': 0.9109595841158191}\n",
      "{'accuracy': 0.855, 'precision': 1.0, 'recall': 0.275, 'F1': 0.4313725490196079, 'ROC_AUC': 0.9642578125000001, 'PR_AUC': 0.9109595841158191}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jing_xuzijian/crf/Intrer_VAE/Model.py:20: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n",
      "  nn.init.xavier_normal(m.weight.data, gain=1.414)\n",
      "/home/jing_xuzijian/crf/Intrer_VAE/Model.py:20: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n",
      "  nn.init.xavier_normal(m.weight.data, gain=1.414)\n",
      "/home/jing_xuzijian/crf/Intrer_VAE/Model.py:20: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n",
      "  nn.init.xavier_normal(m.weight.data, gain=1.414)\n",
      "/home/jing_xuzijian/crf/Intrer_VAE/Model.py:20: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n",
      "  nn.init.xavier_normal(m.weight.data, gain=1.414)\n",
      "/home/jing_xuzijian/crf/Intrer_VAE/Model.py:20: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n",
      "  nn.init.xavier_normal(m.weight.data, gain=1.414)\n",
      "/home/jing_xuzijian/crf/Intrer_VAE/Model.py:20: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n",
      "  nn.init.xavier_normal(m.weight.data, gain=1.414)\n",
      "/home/jing_xuzijian/crf/Intrer_VAE/Model.py:20: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n",
      "  nn.init.xavier_normal(m.weight.data, gain=1.414)\n",
      "/home/jing_xuzijian/crf/Intrer_VAE/Model.py:20: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n",
      "  nn.init.xavier_normal(m.weight.data, gain=1.414)\n",
      "/home/jing_xuzijian/crf/Intrer_VAE/Model.py:20: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n",
      "  nn.init.xavier_normal(m.weight.data, gain=1.414)\n",
      "/home/jing_xuzijian/crf/Intrer_VAE/Model.py:20: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n",
      "  nn.init.xavier_normal(m.weight.data, gain=1.414)\n",
      "/home/jing_xuzijian/crf/Intrer_VAE/Model.py:20: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n",
      "  nn.init.xavier_normal(m.weight.data, gain=1.414)\n",
      "/home/jing_xuzijian/crf/Intrer_VAE/Model.py:20: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n",
      "  nn.init.xavier_normal(m.weight.data, gain=1.414)\n",
      "/home/jing_xuzijian/crf/Intrer_VAE/Model.py:20: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n",
      "  nn.init.xavier_normal(m.weight.data, gain=1.414)\n",
      "/home/jing_xuzijian/crf/Intrer_VAE/Model.py:20: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n",
      "  nn.init.xavier_normal(m.weight.data, gain=1.414)\n",
      "/home/jing_xuzijian/crf/Intrer_VAE/Model.py:20: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n",
      "  nn.init.xavier_normal(m.weight.data, gain=1.414)\n",
      "/home/jing_xuzijian/crf/Intrer_VAE/Model.py:20: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n",
      "  nn.init.xavier_normal(m.weight.data, gain=1.414)\n",
      "/home/jing_xuzijian/crf/Intrer_VAE/Model.py:20: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n",
      "  nn.init.xavier_normal(m.weight.data, gain=1.414)\n",
      "/home/jing_xuzijian/crf/Intrer_VAE/Model.py:20: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n",
      "  nn.init.xavier_normal(m.weight.data, gain=1.414)\n",
      "/home/jing_xuzijian/crf/Intrer_VAE/Model.py:20: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n",
      "  nn.init.xavier_normal(m.weight.data, gain=1.414)\n"
     ]
    }
   ],
   "source": [
    "for idx in range(20):\n",
    "    encoder_file = 'encoder' + str(idx) + '.pt'\n",
    "    encoder_file = os.path.join('/home/jing_xuzijian/crf/Intrer_VAE_result/Lorenz96.20.500/help', encoder_file)\n",
    "    est_net = encoder(init_adj, args.dims, args.encoder_hidden, args.dims, args.time_step - 1, args.encoder_dropout, args.encoder_alpha)\n",
    "    est_net.load_state_dict(torch.load(encoder_file))\n",
    "    est_adj = est_net.adj.cpu().detach().numpy()\n",
    "    result, _ = evaluate_result(GC, est_adj, 0.5)\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.8025, 'precision': 1.0, 'recall': 0.0125, 'F1': 0.02469135802469136, 'ROC_AUC': 0.467734375, 'PR_AUC': 0.2090953304192029}\n"
     ]
    }
   ],
   "source": [
    "est_adj = est_net.adj.clone().detach()\n",
    "result, _ = evaluate_result(GC, est_adj, 0.5)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "causality_matrix = []\n",
    "total_gamma_matrix = []\n",
    "total_theta_matrix = []\n",
    "#init_adj = torch.eye(20)\n",
    "for idx in range(10):\n",
    "    encoder_file = 'encoder' + str(idx) + '.pt'\n",
    "    encoder_file = os.path.join('/home/omnisky/Public/ChenRongfa/Intrer_VAE_result/Lorenz96.10.250/help', encoder_file)\n",
    "    est_net = encoder(init_adj, args.dims, args.encoder_hidden, args.dims, args.time_step - 1, args.encoder_dropout, args.encoder_alpha)\n",
    "    est_net.load_state_dict(torch.load(encoder_file))\n",
    "    # est_net = est_net.cuda()\n",
    "    est_net.eval()\n",
    "    inputs = X_np[:, :, :-1, :]#.cuda()   #:-1和1:有什么区别\n",
    "    mu, log_var = est_net(inputs)\n",
    "    # mu = mu.cpu().detach()\n",
    "    # log_var = log_var.cpu().detach()\n",
    "    sigma = torch.exp(log_var / 2)\n",
    "    # sigma2 = torch.exp(log_var2 / 2)\n",
    "    gamma = torch.randn(size = mu.size())\n",
    "    # theta = torch.randn(size = mu1.size())\n",
    "    gamma = mu + sigma * gamma\n",
    "    # theta = mu2 + sigma2* theta\n",
    "    mask_matrix = torch.sigmoid(gamma) #* torch.sigmoid(theta)\n",
    "    mask_matrix = mask_matrix.squeeze()\n",
    "    causality_matrix.append(mask_matrix)\n",
    "    # gamma_matrix = torch.sigmoid(gamma)\n",
    "    # gamma_matrix = gamma_matrix.squeeze()\n",
    "    # total_gamma_matrix.append(gamma_matrix)\n",
    "    # theta_matrix = torch.sigmoid(theta)\n",
    "    # theta_matrix = theta_matrix.squeeze()\n",
    "    # total_theta_matrix.append(theta_matrix)\n",
    "\n",
    "causality_matrix = torch.stack(causality_matrix, dim=1)\n",
    "# total_gamma_matrix = torch.stack(total_gamma_matrix, dim=1)\n",
    "# total_theta_matrix = torch.stack(total_theta_matrix, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 10])\n"
     ]
    }
   ],
   "source": [
    "adj_gca = causality_matrix.mean(dim=3).mean(dim=0)\n",
    "# gamma_adj = total_gamma_matrix.mean(dim=3).mean(dim=0)\n",
    "# theta_adj = total_theta_matrix.mean(dim=3).mean(dim=0)\n",
    "print(adj_gca.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[7.6896e-01, 7.8296e-02, 5.1112e-05, 5.2073e-05, 5.6759e-05, 4.9417e-02,\n",
      "         5.4774e-05, 5.5814e-05, 5.4371e-05, 5.4734e-05, 5.6489e-05, 5.5717e-05,\n",
      "         5.4487e-05, 5.5353e-05, 7.1983e-02, 5.1452e-05, 2.6985e-02, 1.0124e-01,\n",
      "         3.2742e-01, 4.2244e-01],\n",
      "        [1.2391e-01, 8.4173e-01, 5.9056e-03, 2.3101e-03, 2.3056e-03, 2.3278e-03,\n",
      "         2.3600e-03, 2.2972e-03, 2.3250e-03, 2.2922e-03, 2.2934e-03, 2.3774e-03,\n",
      "         2.3698e-03, 2.2588e-03, 2.3671e-03, 2.3379e-03, 2.3117e-03, 2.3549e-03,\n",
      "         2.3715e-03, 3.0842e-01],\n",
      "        [2.4178e-01, 3.8542e-01, 9.8017e-01, 1.8064e-01, 3.7534e-05, 3.9863e-05,\n",
      "         3.8117e-05, 2.2170e-03, 7.2371e-02, 3.7368e-05, 3.9130e-05, 1.9649e-03,\n",
      "         3.9108e-05, 3.8433e-05, 3.7392e-05, 4.0173e-05, 3.8965e-05, 3.8006e-05,\n",
      "         3.9025e-05, 2.4334e-03],\n",
      "        [7.9220e-05, 3.2196e-01, 3.1193e-01, 8.7320e-01, 2.5348e-03, 1.0796e-01,\n",
      "         8.2627e-05, 7.7709e-04, 8.1066e-05, 8.2243e-05, 8.0188e-05, 8.1518e-05,\n",
      "         8.0625e-05, 8.1983e-05, 8.0631e-05, 7.5274e-03, 8.1032e-04, 7.9210e-05,\n",
      "         7.7937e-05, 8.1177e-05],\n",
      "        [1.5007e-04, 1.4963e-04, 3.2582e-01, 3.8654e-04, 8.8049e-01, 1.2348e-01,\n",
      "         1.4481e-04, 1.4327e-04, 1.5115e-04, 1.5534e-04, 1.4971e-04, 1.5144e-04,\n",
      "         1.4928e-04, 1.5505e-04, 1.5045e-04, 1.4965e-04, 1.5325e-04, 1.5064e-04,\n",
      "         1.4955e-04, 1.4502e-04],\n",
      "        [1.3119e-02, 4.1996e-04, 4.0297e-04, 2.8507e-01, 7.3035e-02, 7.9742e-01,\n",
      "         3.3213e-01, 2.7190e-01, 4.1225e-04, 6.4289e-02, 9.9735e-02, 1.4047e-01,\n",
      "         3.4082e-03, 4.0725e-04, 9.7725e-02, 4.0345e-04, 4.1666e-04, 4.2774e-04,\n",
      "         6.6273e-02, 4.1623e-04],\n",
      "        [1.8864e-04, 1.8908e-04, 1.8342e-04, 5.2417e-02, 1.1534e-01, 3.5714e-01,\n",
      "         1.0000e+00, 4.9248e-02, 8.2457e-02, 1.9038e-04, 1.8992e-04, 1.9344e-04,\n",
      "         4.0329e-04, 1.8572e-04, 1.8391e-04, 1.8922e-04, 1.0410e-03, 1.8659e-04,\n",
      "         1.8651e-04, 1.6928e-03],\n",
      "        [1.5575e-04, 1.5898e-04, 5.7761e-03, 4.0490e-02, 1.3609e-01, 3.1231e-01,\n",
      "         7.4765e-02, 7.7753e-01, 8.4973e-02, 1.2332e-01, 1.5229e-04, 1.5335e-04,\n",
      "         1.5052e-04, 1.5344e-04, 1.5261e-04, 1.5183e-04, 1.5333e-04, 1.4558e-04,\n",
      "         9.5944e-03, 1.3575e-04],\n",
      "        [4.0674e-04, 3.8810e-04, 4.0062e-04, 6.4706e-04, 1.0254e-02, 3.8899e-04,\n",
      "         2.0964e-01, 3.9492e-04, 9.3349e-01, 1.8889e-01, 3.9376e-04, 3.8803e-04,\n",
      "         1.5752e-03, 3.8788e-04, 8.7768e-03, 4.0726e-04, 1.0311e-02, 4.0704e-04,\n",
      "         3.9989e-04, 3.9274e-04],\n",
      "        [1.7502e-04, 1.7336e-04, 1.7004e-04, 1.7103e-04, 1.7726e-04, 1.7333e-04,\n",
      "         1.7131e-04, 2.7583e-01, 1.7009e-04, 8.5050e-01, 3.9634e-01, 1.6804e-04,\n",
      "         1.7511e-04, 1.7095e-04, 1.7092e-04, 1.7450e-04, 1.7174e-04, 1.7179e-04,\n",
      "         1.6664e-04, 1.7470e-04],\n",
      "        [5.4675e-02, 1.3467e-05, 1.4257e-05, 1.3213e-05, 1.4304e-05, 1.8951e-01,\n",
      "         2.1024e-01, 4.7360e-03, 4.5826e-01, 2.7049e-01, 8.7010e-01, 2.8634e-01,\n",
      "         1.0479e-01, 1.7725e-02, 1.3850e-05, 7.8259e-02, 1.4484e-05, 1.4390e-05,\n",
      "         1.4345e-05, 1.4620e-05],\n",
      "        [4.0529e-04, 3.9780e-04, 4.0885e-04, 4.0925e-04, 3.9103e-04, 4.0054e-04,\n",
      "         3.9255e-04, 3.9469e-04, 3.9570e-04, 2.0252e-01, 2.5406e-02, 8.4589e-01,\n",
      "         7.8133e-03, 1.9446e-01, 4.1676e-04, 4.1988e-04, 4.0387e-04, 3.9620e-04,\n",
      "         3.9320e-04, 3.9636e-04],\n",
      "        [1.6714e-03, 1.6281e-03, 1.6547e-03, 1.5886e-03, 4.7224e-03, 1.6270e-03,\n",
      "         1.4652e-02, 2.9836e-03, 3.4181e-03, 2.3317e-03, 1.6576e-01, 4.6415e-01,\n",
      "         8.4105e-01, 8.3567e-02, 2.3180e-01, 7.5941e-02, 1.4224e-02, 1.4089e-03,\n",
      "         1.5848e-03, 1.6494e-03],\n",
      "        [6.1862e-05, 5.5270e-05, 5.8864e-05, 6.0474e-05, 5.9153e-05, 6.3249e-05,\n",
      "         6.2178e-05, 6.9171e-05, 5.9593e-05, 2.1822e-01, 6.0456e-05, 3.0679e-01,\n",
      "         4.9366e-05, 1.0000e+00, 3.6326e-01, 6.4284e-05, 5.6158e-05, 6.2179e-05,\n",
      "         5.7500e-05, 6.9351e-05],\n",
      "        [2.8151e-02, 6.5898e-05, 3.7214e-02, 1.3122e-02, 5.8280e-03, 6.4221e-05,\n",
      "         6.5938e-05, 6.5102e-05, 6.5078e-05, 6.2764e-05, 9.2947e-02, 6.5184e-05,\n",
      "         2.3323e-01, 4.2274e-03, 9.3251e-01, 1.7959e-01, 6.3394e-05, 6.4966e-05,\n",
      "         6.3038e-05, 4.7535e-04],\n",
      "        [8.1260e-05, 8.3476e-02, 6.0747e-05, 1.4277e-01, 1.2880e-01, 6.9079e-05,\n",
      "         6.6686e-05, 6.1346e-05, 7.3560e-05, 6.7806e-05, 2.4876e-01, 1.2279e-01,\n",
      "         6.3957e-02, 3.2559e-01, 2.9083e-01, 8.3862e-01, 3.1010e-01, 2.3252e-01,\n",
      "         2.7135e-02, 3.8117e-05],\n",
      "        [6.1331e-04, 6.1405e-04, 5.7830e-04, 6.0646e-04, 6.0490e-04, 6.1723e-04,\n",
      "         5.9745e-04, 6.0794e-04, 6.0228e-04, 5.9782e-04, 6.1475e-04, 6.1065e-04,\n",
      "         6.0412e-04, 6.0543e-04, 6.9643e-04, 1.1312e-01, 8.1333e-01, 6.1280e-04,\n",
      "         2.4916e-02, 6.2038e-04],\n",
      "        [5.1480e-05, 5.0550e-05, 5.2142e-05, 8.3294e-03, 3.7538e-02, 5.3563e-05,\n",
      "         1.4724e-02, 5.2948e-05, 5.3306e-05, 5.2872e-05, 5.1052e-05, 2.7882e-03,\n",
      "         5.3615e-05, 5.2945e-05, 3.0678e-04, 4.2257e-01, 6.8644e-02, 9.9253e-01,\n",
      "         2.2528e-02, 1.5792e-01],\n",
      "        [2.9176e-04, 4.4140e-03, 3.1705e-04, 3.2599e-04, 3.0961e-04, 3.0816e-04,\n",
      "         2.5196e-04, 1.5977e-03, 3.0550e-04, 2.9777e-04, 3.0501e-04, 2.9916e-04,\n",
      "         3.1230e-04, 2.9867e-04, 3.0490e-04, 2.9082e-04, 3.3120e-01, 4.9732e-02,\n",
      "         7.4836e-01, 2.7439e-01],\n",
      "        [3.8942e-01, 2.6410e-04, 2.6272e-04, 2.6461e-04, 2.6836e-04, 2.5956e-04,\n",
      "         2.7098e-04, 2.7435e-04, 2.5305e-04, 2.7143e-04, 5.1710e-02, 2.6497e-04,\n",
      "         2.6729e-04, 2.7074e-04, 2.7149e-04, 2.0203e-01, 8.9010e-02, 1.9864e-01,\n",
      "         1.3068e-02, 9.4109e-01]], grad_fn=<MeanBackward1>)\n"
     ]
    }
   ],
   "source": [
    "print(theta_adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[7.4909e-01, 7.4239e-02, 1.0852e-07, 1.0922e-07, 1.2867e-07, 4.6770e-02,\n",
      "         1.2158e-07, 1.2443e-07, 1.2163e-07, 1.1989e-07, 1.2625e-07, 1.2637e-07,\n",
      "         1.2013e-07, 1.2420e-07, 6.7696e-02, 1.1373e-07, 2.5288e-02, 9.6654e-02,\n",
      "         3.1617e-01, 4.0582e-01],\n",
      "        [1.1625e-01, 8.3866e-01, 4.9221e-03, 2.5222e-07, 2.6218e-07, 2.7182e-07,\n",
      "         2.5677e-07, 2.6314e-07, 2.6949e-07, 2.5992e-07, 2.6494e-07, 2.7781e-07,\n",
      "         2.6403e-07, 2.6299e-07, 2.7177e-07, 2.6159e-07, 2.5859e-07, 2.7005e-07,\n",
      "         2.7816e-07, 2.9851e-01],\n",
      "        [2.3491e-01, 3.7936e-01, 9.7944e-01, 1.7477e-01, 1.1422e-08, 1.2310e-08,\n",
      "         1.1873e-08, 2.0021e-03, 6.9232e-02, 1.0887e-08, 1.1675e-08, 1.7427e-03,\n",
      "         1.2874e-08, 1.2228e-08, 1.1453e-08, 1.2806e-08, 1.1806e-08, 1.1823e-08,\n",
      "         1.1806e-08, 2.1349e-03],\n",
      "        [1.5731e-08, 3.1262e-01, 2.9733e-01, 8.4128e-01, 2.0902e-03, 1.0322e-01,\n",
      "         1.7735e-08, 6.6305e-04, 1.6666e-08, 1.7288e-08, 1.6466e-08, 1.7279e-08,\n",
      "         1.6449e-08, 1.6904e-08, 1.6802e-08, 6.7756e-03, 6.5513e-04, 1.6263e-08,\n",
      "         1.5907e-08, 1.6551e-08],\n",
      "        [2.7512e-08, 2.7127e-08, 3.1728e-01, 1.2979e-04, 8.7814e-01, 1.2092e-01,\n",
      "         2.6328e-08, 2.4924e-08, 2.7103e-08, 2.9114e-08, 2.7515e-08, 2.8189e-08,\n",
      "         2.7357e-08, 2.9205e-08, 2.9467e-08, 2.7391e-08, 2.8978e-08, 2.7829e-08,\n",
      "         2.6958e-08, 2.5529e-08],\n",
      "        [1.1988e-02, 4.1426e-08, 3.7252e-08, 2.7801e-01, 6.7910e-02, 7.9332e-01,\n",
      "         3.2218e-01, 2.6514e-01, 3.9082e-08, 5.9902e-02, 9.4718e-02, 1.3477e-01,\n",
      "         2.9931e-03, 4.0437e-08, 9.2758e-02, 3.7680e-08, 4.0724e-08, 4.2662e-08,\n",
      "         6.1596e-02, 4.0938e-08],\n",
      "        [7.8505e-08, 7.9725e-08, 7.6723e-08, 5.1480e-02, 1.1335e-01, 3.4960e-01,\n",
      "         9.9999e-01, 4.8316e-02, 8.0941e-02, 7.9510e-08, 8.0638e-08, 7.8738e-08,\n",
      "         2.2029e-04, 7.7358e-08, 7.7320e-08, 7.8747e-08, 8.0120e-04, 7.6168e-08,\n",
      "         7.8387e-08, 1.4637e-03],\n",
      "        [7.0951e-08, 7.1737e-08, 4.8338e-03, 3.7977e-02, 1.2747e-01, 3.0191e-01,\n",
      "         6.9913e-02, 7.4789e-01, 8.0160e-02, 1.1549e-01, 7.0802e-08, 7.0809e-08,\n",
      "         6.4534e-08, 7.1023e-08, 6.9247e-08, 6.9162e-08, 6.9013e-08, 6.5885e-08,\n",
      "         8.6801e-03, 5.7272e-08],\n",
      "        [1.3026e-07, 1.2165e-07, 1.2713e-07, 3.8746e-04, 8.9819e-03, 1.1737e-07,\n",
      "         2.0320e-01, 1.2721e-07, 9.3074e-01, 1.8263e-01, 1.2665e-07, 1.1875e-07,\n",
      "         1.0336e-03, 1.2131e-07, 7.5611e-03, 1.2500e-07, 8.8406e-03, 1.2596e-07,\n",
      "         1.2765e-07, 1.2125e-07],\n",
      "        [9.3261e-08, 9.1305e-08, 8.9536e-08, 8.5614e-08, 9.3321e-08, 9.3026e-08,\n",
      "         9.1010e-08, 2.7127e-01, 8.5075e-08, 8.4507e-01, 3.8973e-01, 8.8750e-08,\n",
      "         8.9464e-08, 8.9825e-08, 9.0946e-08, 9.3300e-08, 9.2947e-08, 8.8167e-08,\n",
      "         8.4729e-08, 9.0672e-08],\n",
      "        [5.2821e-02, 3.1233e-08, 3.2451e-08, 3.0078e-08, 3.4247e-08, 1.8347e-01,\n",
      "         2.0140e-01, 4.3586e-03, 4.4346e-01, 2.6201e-01, 8.3276e-01, 2.7753e-01,\n",
      "         9.7998e-02, 1.6489e-02, 3.1441e-08, 7.3414e-02, 3.3728e-08, 3.1781e-08,\n",
      "         3.3444e-08, 3.3608e-08],\n",
      "        [1.4552e-07, 1.3953e-07, 1.4778e-07, 1.4979e-07, 1.3694e-07, 1.4134e-07,\n",
      "         1.3868e-07, 1.4191e-07, 1.4230e-07, 1.9599e-01, 2.4591e-02, 8.3903e-01,\n",
      "         6.9871e-03, 1.8795e-01, 1.5132e-07, 1.5394e-07, 1.4335e-07, 1.3784e-07,\n",
      "         1.3655e-07, 1.3820e-07],\n",
      "        [2.9498e-08, 3.0219e-08, 3.1004e-08, 2.7157e-08, 3.7498e-03, 3.1157e-08,\n",
      "         1.2701e-02, 2.2880e-03, 2.7450e-03, 1.9523e-03, 1.5269e-01, 4.4683e-01,\n",
      "         8.3570e-01, 7.6130e-02, 2.1751e-01, 6.9911e-02, 1.1971e-02, 1.9162e-08,\n",
      "         2.9082e-08, 3.0775e-08],\n",
      "        [2.0128e-08, 1.4893e-08, 2.0108e-08, 1.9357e-08, 1.9663e-08, 2.3210e-08,\n",
      "         2.3215e-08, 2.7717e-08, 1.9952e-08, 2.0413e-01, 2.1207e-08, 2.9753e-01,\n",
      "         1.3206e-08, 1.0000e+00, 3.4940e-01, 2.2357e-08, 1.7827e-08, 1.8414e-08,\n",
      "         1.5561e-08, 2.5765e-08],\n",
      "        [2.6861e-02, 5.3465e-08, 3.6132e-02, 1.2581e-02, 5.4429e-03, 5.0291e-08,\n",
      "         5.1990e-08, 4.9026e-08, 5.1158e-08, 4.7404e-08, 8.9414e-02, 5.0725e-08,\n",
      "         2.2722e-01, 4.0721e-03, 9.1549e-01, 1.7530e-01, 4.8809e-08, 5.0189e-08,\n",
      "         4.8653e-08, 4.3124e-04],\n",
      "        [7.2528e-09, 8.0006e-02, 4.9136e-09, 1.3708e-01, 1.2512e-01, 5.5743e-09,\n",
      "         5.0223e-09, 5.1620e-09, 6.5459e-09, 5.5314e-09, 2.4397e-01, 1.1880e-01,\n",
      "         6.0395e-02, 3.2008e-01, 2.8512e-01, 8.3545e-01, 3.0304e-01, 2.2363e-01,\n",
      "         2.5691e-02, 1.7567e-09],\n",
      "        [1.2422e-07, 1.2081e-07, 1.0634e-07, 1.1734e-07, 1.1813e-07, 1.2563e-07,\n",
      "         1.1473e-07, 1.1929e-07, 1.1679e-07, 1.1906e-07, 1.2606e-07, 1.1673e-07,\n",
      "         1.1907e-07, 1.1700e-07, 5.1842e-04, 1.0830e-01, 8.0346e-01, 1.2181e-07,\n",
      "         2.2772e-02, 1.2563e-07],\n",
      "        [2.5647e-10, 2.4490e-10, 2.7468e-10, 7.2539e-03, 3.3614e-02, 2.6192e-10,\n",
      "         1.3550e-02, 2.7567e-10, 2.7103e-10, 2.8364e-10, 2.6328e-10, 2.6206e-03,\n",
      "         2.8302e-10, 2.8009e-10, 2.0367e-04, 4.1241e-01, 6.4551e-02, 9.9244e-01,\n",
      "         2.0796e-02, 1.5094e-01],\n",
      "        [8.4173e-08, 3.6930e-03, 1.0492e-07, 1.0766e-07, 9.8179e-08, 9.4973e-08,\n",
      "         6.4177e-08, 1.2546e-03, 9.6075e-08, 8.8651e-08, 9.0596e-08, 8.8154e-08,\n",
      "         9.5023e-08, 8.8324e-08, 9.7036e-08, 8.1825e-08, 3.1920e-01, 4.4263e-02,\n",
      "         7.4653e-01, 2.6591e-01],\n",
      "        [3.8108e-01, 5.7225e-08, 5.7606e-08, 5.7365e-08, 5.9800e-08, 5.5266e-08,\n",
      "         5.9636e-08, 6.3689e-08, 5.4057e-08, 5.9648e-08, 4.7891e-02, 5.8282e-08,\n",
      "         5.9047e-08, 6.1504e-08, 5.9107e-08, 1.9250e-01, 8.3072e-02, 1.9277e-01,\n",
      "         1.1117e-02, 9.2969e-01]], grad_fn=<MeanBackward1>)\n",
      "[1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "print(adj_gca)\n",
    "print(GC[1, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.76, 'precision': 1.0, 'recall': 0.4, 'F1': 0.5714285714285715, 'ROC_AUC': 0.8783333333333333, 'PR_AUC': 0.8890493942929301}\n"
     ]
    }
   ],
   "source": [
    "result, _ = evaluate_result(GC, adj_gca.detach().numpy(), 0.5)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 0 0 0 0 0 0 1 1]\n",
      " [1 1 1 0 0 0 0 0 0 1]\n",
      " [1 1 1 1 0 0 0 0 0 0]\n",
      " [0 1 1 1 1 0 0 0 0 0]\n",
      " [0 0 1 1 1 1 0 0 0 0]\n",
      " [0 0 0 1 1 1 1 0 0 0]\n",
      " [0 0 0 0 1 1 1 1 0 0]\n",
      " [0 0 0 0 0 1 1 1 1 0]\n",
      " [0 0 0 0 0 0 1 1 1 1]\n",
      " [1 0 0 0 0 0 0 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "print(GC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[9.8090e-01, 2.4526e-01, 3.0937e-08, 2.7574e-08, 3.4536e-08, 8.7615e-02,\n",
      "         3.8875e-08, 4.2935e-02, 1.9347e-02, 4.1903e-08],\n",
      "        [2.2222e-01, 9.8181e-01, 2.2222e-01, 2.2222e-01, 2.2222e-01, 2.2222e-01,\n",
      "         2.2222e-01, 2.2222e-01, 2.2222e-01, 2.2222e-01],\n",
      "        [1.0000e+00, 1.6369e-01, 1.0000e+00, 8.2401e-02, 1.7285e-07, 1.9243e-07,\n",
      "         9.6406e-02, 1.8654e-07, 1.3874e-07, 2.0195e-07],\n",
      "        [4.3069e-07, 7.3208e-02, 4.6705e-01, 9.9998e-01, 3.7583e-07, 3.4952e-02,\n",
      "         3.5127e-07, 5.4465e-02, 3.3362e-07, 3.4672e-07],\n",
      "        [9.0340e-07, 9.2341e-07, 2.4287e-02, 9.2389e-07, 9.9945e-01, 9.5101e-07,\n",
      "         9.4541e-07, 9.4353e-07, 9.4964e-07, 9.3344e-07],\n",
      "        [9.6907e-04, 5.4347e-08, 6.6765e-08, 4.4504e-02, 1.0755e-01, 9.9631e-01,\n",
      "         1.0056e-01, 1.0439e-02, 4.9131e-08, 6.4329e-08],\n",
      "        [2.5810e-07, 2.6611e-07, 3.2856e-07, 1.9583e-07, 1.8355e-01, 9.9956e-01,\n",
      "         1.0000e+00, 4.2288e-07, 2.5699e-07, 3.1616e-07],\n",
      "        [2.2524e-07, 4.7017e-08, 5.1814e-08, 3.4718e-08, 5.7345e-08, 1.8081e-01,\n",
      "         1.0990e-02, 9.7017e-01, 1.1855e-01, 1.2505e-01],\n",
      "        [8.2279e-03, 3.2436e-07, 3.2158e-07, 3.3411e-07, 3.4807e-07, 3.7928e-07,\n",
      "         1.0168e-02, 1.0000e+00, 1.0000e+00, 1.0000e+00],\n",
      "        [8.6507e-08, 1.0370e-07, 9.3110e-08, 6.7624e-08, 7.9784e-08, 1.1662e-07,\n",
      "         8.5138e-08, 4.2047e-01, 2.3970e-01, 9.9113e-01]],\n",
      "       grad_fn=<MeanBackward1>)\n"
     ]
    }
   ],
   "source": [
    "print(adj_gca)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "ae5e1b78a42d3a00ffbad3028f2882e576ba2589a97bb9b799d13c9e22855bf5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
