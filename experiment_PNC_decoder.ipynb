{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "from Model import *\n",
    "from itertools import chain\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from synthetic import simulate_lorenz_96, simulate_var\n",
    "from utils import build_flags, time_split, save_result, evaluate_result, count_accuracy, loss_sparsity, loss_divergence, loss_mmd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> 开始查询GPU使用情况：\n",
      "第 0 块显卡：已使用 10808  Mib, 全部 24564  Mib，使用率: 44.00% \n",
      "第 1 块显卡：已使用   13   Mib, 全部 24564  Mib，使用率: 0.05%  \n",
      "--> 选择第 1 块GPU\n"
     ]
    }
   ],
   "source": [
    "def choseGPU():\n",
    "    '''在程序运行最开始, 根据GPU使用情况，自动选择GPU'''\n",
    "    from re import findall\n",
    "    from subprocess import check_output\n",
    "    from torch.cuda import set_device, is_available\n",
    "    \n",
    "    if(is_available()):\n",
    "        output = check_output(\"nvidia-smi\", shell=True)\n",
    "        info = findall(\"(\\d+)MiB\\s/\\s(\\d+)MiB\", output.decode(\"utf-8\"))\n",
    "        print(\"--> 开始查询GPU使用情况：\")\n",
    "        # 正确匹配\n",
    "        best_index = 0\n",
    "        current_um = -1\n",
    "        for index, (um, am) in enumerate(info):\n",
    "            print(\"第{:^3}块显卡：已使用{:^8}Mib, 全部{:^8}Mib，使用率:{:^8.2%}\".format(index, um, am, float(um)/float(am)))\n",
    "            best_index = index if(current_um>float(um) and current_um>=0) else best_index\n",
    "            current_um = float(um)\n",
    "        # 选择\n",
    "        best_index=best_index\n",
    "        print(\"--> 选择第{:^3}块GPU\".format(best_index))\n",
    "        set_device(best_index)\n",
    "        \n",
    "    else:\n",
    "        print(\"显卡不可用\")\n",
    "        \n",
    "# 设置随机数种子\n",
    "# set_random_seed()\n",
    "# 选择GPU\n",
    "choseGPU()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = build_flags()\n",
    "args = parser.parse_args(args=[])\n",
    "args.seed = 2\n",
    "args.num_nodes = 264\n",
    "args.dims = 1\n",
    "args.threshold = 0.5\n",
    "args.time_length = 500\n",
    "args.time_step = 10\n",
    "args.epochs = 3000\n",
    "args.batch_size = 64\n",
    "args.lr = 1e-3\n",
    "args.weight_decay = 1e-3\n",
    "args.encoder_alpha = 0.02\n",
    "args.decoder_alpha = 0.02\n",
    "args.beta_sparsity = 3.00 #0.25   #log_sum  #1.25是F=40,N=30的最优值\n",
    "args.beta_kl = 0.01        #JS散度\n",
    "args.beta_mmd = 2      #MMD\n",
    "args.encoder_hidden = 20\n",
    "args.decoder_hidden = 20\n",
    "args.encoder_dropout = 0.1\n",
    "args.decoder_dropout = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "torch.cuda.manual_seed(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = np.load('/home/omnisky/Public/ChenRongfa/Intrer_VAE/PNC_SPM_emoid4.npy')\n",
    "X_np = []\n",
    "for sample in samples:\n",
    "    X_np.append(np.array(time_split(sample[:,:,np.newaxis], step=10)))\n",
    "X_np = np.vstack(X_np)\n",
    "X_np = torch.FloatTensor(X_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([41004, 264, 10, 1])\n"
     ]
    }
   ],
   "source": [
    "print(X_np.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = X_np\n",
    "data_loader = DataLoader(data, batch_size=args.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/omnisky/Public/ChenRongfa/Intrer_VAE/Model.py:20: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n",
      "  nn.init.xavier_normal(m.weight.data, gain=1.414)\n"
     ]
    }
   ],
   "source": [
    "adj = []\n",
    "for idx in range(264):\n",
    "    decoder_file = 'decoder' + str(idx) + '.pt'\n",
    "    decoder_file = os.path.join('/home/omnisky/Public/ChenRongfa/Intrer_VAE_result/DLCR_PNC_emoid4', decoder_file)\n",
    "    decoder_net = decoder(args.dims, args.decoder_hidden, args.time_step - 1, args.num_nodes, args.decoder_dropout, args.decoder_alpha)\n",
    "    decoder_net.load_state_dict(torch.load(decoder_file, map_location='cuda:1'))\n",
    "    adj.append(decoder_net.adj[idx, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([264, 264])\n"
     ]
    }
   ],
   "source": [
    "init_adj = torch.cat([temp.unsqueeze(0) for temp in adj], dim=0)\n",
    "init_adj = init_adj.clone().detach()\n",
    "print(init_adj.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin training feature: 0231\n",
      "Feature: 0231 Epoch: 0000 Loss: 13.1691446877 MSE_Loss: 1.3565797349 Sparsity_loss: 3.9305438343 KL_loss: 0.0672202492 MMD_loss: 0.0101306271 time: 76.1780s\n",
      "Feature: 0231 Epoch: 0005 Loss: 1.6860594985 MSE_Loss: 1.3084381302 Sparsity_loss: 0.1188597710 KL_loss: 0.0366788571 MMD_loss: 0.0103376261 time: 76.6707s\n",
      "Feature: 0231 Epoch: 0010 Loss: 1.6869469199 MSE_Loss: 1.2928538629 Sparsity_loss: 0.1243957515 KL_loss: 0.0368710994 MMD_loss: 0.0102685409 time: 76.8215s\n",
      "Feature: 0231 Epoch: 0015 Loss: 1.6869706588 MSE_Loss: 1.2929092171 Sparsity_loss: 0.1243849711 KL_loss: 0.0368710370 MMD_loss: 0.0102689088 time: 76.7585s\n",
      "Begin training feature: 0232\n",
      "Feature: 0232 Epoch: 0000 Loss: 12.0242579502 MSE_Loss: 1.4726261216 Sparsity_loss: 3.5075928746 KL_loss: 0.0465027150 MMD_loss: 0.0141940815 time: 76.8286s\n",
      "Feature: 0232 Epoch: 0005 Loss: 1.6646925485 MSE_Loss: 1.4287137751 Sparsity_loss: 0.0689063295 KL_loss: 0.0351960881 MMD_loss: 0.0144539132 time: 76.8321s\n",
      "Feature: 0232 Epoch: 0010 Loss: 1.5796049237 MSE_Loss: 1.3776172195 Sparsity_loss: 0.0575054916 KL_loss: 0.0345873166 MMD_loss: 0.0145626787 time: 76.8277s\n",
      "Feature: 0232 Epoch: 0015 Loss: 1.5496177072 MSE_Loss: 1.3524980303 Sparsity_loss: 0.0558936401 KL_loss: 0.0343975533 MMD_loss: 0.0145473878 time: 76.8906s\n",
      "Begin training feature: 0233\n",
      "Feature: 0233 Epoch: 0000 Loss: 16.3144931421 MSE_Loss: 1.5170310079 Sparsity_loss: 4.9233675747 KL_loss: 0.0910258514 MMD_loss: 0.0132245571 time: 76.8974s\n",
      "Feature: 0233 Epoch: 0005 Loss: 2.2241778315 MSE_Loss: 1.7249894222 Sparsity_loss: 0.1566541286 KL_loss: 0.0370932436 MMD_loss: 0.0144275461 time: 76.8323s\n",
      "Feature: 0233 Epoch: 0010 Loss: 2.0811128719 MSE_Loss: 1.7711715769 Sparsity_loss: 0.0934890617 KL_loss: 0.0363137734 MMD_loss: 0.0145554899 time: 76.8361s\n",
      "Feature: 0233 Epoch: 0015 Loss: 2.0802975766 MSE_Loss: 1.7706676564 Sparsity_loss: 0.0933854574 KL_loss: 0.0363176111 MMD_loss: 0.0145551868 time: 76.8130s\n",
      "Begin training feature: 0234\n",
      "Feature: 0234 Epoch: 0000 Loss: 12.8473631858 MSE_Loss: 1.3239839510 Sparsity_loss: 3.8323123645 KL_loss: 0.0634772213 MMD_loss: 0.0129036799 time: 76.8854s\n",
      "Feature: 0234 Epoch: 0005 Loss: nan MSE_Loss: nan Sparsity_loss: nan KL_loss: nan MMD_loss: nan time: 76.0840s\n",
      "Feature: 0234 Epoch: 0010 Loss: nan MSE_Loss: nan Sparsity_loss: nan KL_loss: nan MMD_loss: nan time: 76.1346s\n",
      "Feature: 0234 Epoch: 0015 Loss: nan MSE_Loss: nan Sparsity_loss: nan KL_loss: nan MMD_loss: nan time: 76.1621s\n",
      "Begin training feature: 0235\n",
      "Feature: 0235 Epoch: 0000 Loss: 22.3719497270 MSE_Loss: 4.0763049543 Sparsity_loss: 6.0837144294 KL_loss: 0.0889546448 MMD_loss: 0.0218059834 time: 76.4982s\n",
      "Feature: 0235 Epoch: 0005 Loss: 6.0741594329 MSE_Loss: 4.0011940757 Sparsity_loss: 0.6752112333 KL_loss: 0.0383007112 MMD_loss: 0.0234743189 time: 76.4719s\n",
      "Feature: 0235 Epoch: 0010 Loss: 4.9769441485 MSE_Loss: 4.3228137250 Sparsity_loss: 0.2019703047 KL_loss: 0.0363517903 MMD_loss: 0.0239280011 time: 76.3861s\n",
      "Feature: 0235 Epoch: 0015 Loss: 4.8589319076 MSE_Loss: 4.4851277354 Sparsity_loss: 0.1084875323 KL_loss: 0.0360401247 MMD_loss: 0.0239905707 time: 76.4035s\n",
      "Begin training feature: 0236\n",
      "Feature: 0236 Epoch: 0000 Loss: 24.7376320001 MSE_Loss: 6.1217462368 Sparsity_loss: 6.1895750335 KL_loss: 0.0913239970 MMD_loss: 0.0231237189 time: 76.2189s\n",
      "Feature: 0236 Epoch: 0005 Loss: 6.3165139132 MSE_Loss: 5.8048272653 Sparsity_loss: 0.1536164763 KL_loss: 0.0338991747 MMD_loss: 0.0252491360 time: 76.2945s\n",
      "Feature: 0236 Epoch: 0010 Loss: 6.0940548561 MSE_Loss: 5.6149217871 Sparsity_loss: 0.1427738354 KL_loss: 0.0338960480 MMD_loss: 0.0252363199 time: 76.1701s\n",
      "Feature: 0236 Epoch: 0015 Loss: 5.9915022035 MSE_Loss: 5.5028060329 Sparsity_loss: 0.1459434539 KL_loss: 0.0338044073 MMD_loss: 0.0252638640 time: 76.3387s\n",
      "Begin training feature: 0237\n",
      "Feature: 0237 Epoch: 0000 Loss: 23.3653695468 MSE_Loss: 3.0257760949 Sparsity_loss: 6.7682841765 KL_loss: 0.0975178945 MMD_loss: 0.0168828412 time: 76.2315s\n",
      "Feature: 0237 Epoch: 0005 Loss: nan MSE_Loss: nan Sparsity_loss: nan KL_loss: nan MMD_loss: nan time: 75.7436s\n",
      "Feature: 0237 Epoch: 0010 Loss: nan MSE_Loss: nan Sparsity_loss: nan KL_loss: nan MMD_loss: nan time: 75.5123s\n",
      "Feature: 0237 Epoch: 0015 Loss: nan MSE_Loss: nan Sparsity_loss: nan KL_loss: nan MMD_loss: nan time: 75.1871s\n",
      "Begin training feature: 0238\n",
      "Feature: 0238 Epoch: 0000 Loss: 15.7224348770 MSE_Loss: 1.9783764715 Sparsity_loss: 4.5701360680 KL_loss: 0.0466188507 MMD_loss: 0.0165920288 time: 75.8596s\n",
      "Feature: 0238 Epoch: 0005 Loss: 3.6313218198 MSE_Loss: 2.0840324859 Sparsity_loss: 0.5051228845 KL_loss: 0.0050595165 MMD_loss: 0.0159350455 time: 76.8517s\n",
      "Feature: 0238 Epoch: 0010 Loss: nan MSE_Loss: nan Sparsity_loss: nan KL_loss: nan MMD_loss: nan time: 76.2521s\n",
      "Feature: 0238 Epoch: 0015 Loss: nan MSE_Loss: nan Sparsity_loss: nan KL_loss: nan MMD_loss: nan time: 76.1367s\n",
      "Begin training feature: 0239\n",
      "Feature: 0239 Epoch: 0000 Loss: 18.7152766348 MSE_Loss: 3.5208783421 Sparsity_loss: 5.0555713950 KL_loss: 0.0749361428 MMD_loss: 0.0134673474 time: 76.8682s\n",
      "Feature: 0239 Epoch: 0005 Loss: 5.4988092483 MSE_Loss: 3.3512956095 Sparsity_loss: 0.7054344387 KL_loss: 0.0471477949 MMD_loss: 0.0153694351 time: 76.9754s\n",
      "Feature: 0239 Epoch: 0010 Loss: 4.9502393015 MSE_Loss: 3.6121329804 Sparsity_loss: 0.4345847305 KL_loss: 0.0424054495 MMD_loss: 0.0169640295 time: 76.8140s\n",
      "Feature: 0239 Epoch: 0015 Loss: 4.5500423186 MSE_Loss: 3.8608239734 Sparsity_loss: 0.2173965330 KL_loss: 0.0385318092 MMD_loss: 0.0183217070 time: 76.8765s\n",
      "Begin training feature: 0240\n",
      "Feature: 0240 Epoch: 0000 Loss: 19.3250860736 MSE_Loss: 3.8291966952 Sparsity_loss: 5.1556448419 KL_loss: 0.0773933486 MMD_loss: 0.0140904657 time: 76.9137s\n",
      "Feature: 0240 Epoch: 0005 Loss: 5.9437755106 MSE_Loss: 3.6797587939 Sparsity_loss: 0.7434312906 KL_loss: 0.0478432694 MMD_loss: 0.0166222146 time: 76.8518s\n",
      "Feature: 0240 Epoch: 0010 Loss: 5.5716255262 MSE_Loss: 3.7679200237 Sparsity_loss: 0.5896118639 KL_loss: 0.0451729372 MMD_loss: 0.0172090844 time: 76.7846s\n",
      "Feature: 0240 Epoch: 0015 Loss: 5.5713565528 MSE_Loss: 3.7679281733 Sparsity_loss: 0.5895195415 KL_loss: 0.0451729952 MMD_loss: 0.0172089947 time: 76.9369s\n",
      "Begin training feature: 0241\n",
      "Feature: 0241 Epoch: 0000 Loss: 34.7119739283 MSE_Loss: 16.0732899273 Sparsity_loss: 6.1966134702 KL_loss: 0.0679516492 MMD_loss: 0.0240820736 time: 76.9321s\n",
      "Feature: 0241 Epoch: 0005 Loss: 16.5493435045 MSE_Loss: 15.1971329929 Sparsity_loss: 0.4318569681 KL_loss: 0.0302962779 MMD_loss: 0.0281683323 time: 76.7042s\n",
      "Feature: 0241 Epoch: 0010 Loss: 15.9573233603 MSE_Loss: 14.7252318977 Sparsity_loss: 0.3918679617 KL_loss: 0.0305869723 MMD_loss: 0.0280908010 time: 76.8936s\n",
      "Feature: 0241 Epoch: 0015 Loss: 15.6630967728 MSE_Loss: 14.5326888359 Sparsity_loss: 0.3579331815 KL_loss: 0.0293844104 MMD_loss: 0.0281571641 time: 76.5799s\n",
      "Begin training feature: 0242\n",
      "Feature: 0242 Epoch: 0000 Loss: 31.1735476168 MSE_Loss: 10.7119460508 Sparsity_loss: 6.8006922615 KL_loss: 0.0777149495 MMD_loss: 0.0293738588 time: 76.8825s\n",
      "Feature: 0242 Epoch: 0005 Loss: 12.8157724170 MSE_Loss: 9.1616966959 Sparsity_loss: 1.2000641895 KL_loss: 0.0559676510 MMD_loss: 0.0266617097 time: 76.8812s\n",
      "Feature: 0242 Epoch: 0010 Loss: 11.9680931561 MSE_Loss: 9.5175468707 Sparsity_loss: 0.7980729730 KL_loss: 0.0488447267 MMD_loss: 0.0279194960 time: 76.8006s\n",
      "Feature: 0242 Epoch: 0015 Loss: 11.6339020432 MSE_Loss: 10.4548784180 Sparsity_loss: 0.3724547718 KL_loss: 0.0412991162 MMD_loss: 0.0306231647 time: 76.7933s\n",
      "Begin training feature: 0243\n",
      "Feature: 0243 Epoch: 0000 Loss: 17.2107565057 MSE_Loss: 2.2814517600 Sparsity_loss: 4.9674203176 KL_loss: 0.0499792073 MMD_loss: 0.0132720044 time: 76.7779s\n",
      "Feature: 0243 Epoch: 0005 Loss: 3.3916604751 MSE_Loss: 2.1820505426 Sparsity_loss: 0.3940368288 KL_loss: 0.0416516908 MMD_loss: 0.0135414641 time: 76.7452s\n",
      "Feature: 0243 Epoch: 0010 Loss: 3.0200905704 MSE_Loss: 2.7104415490 Sparsity_loss: 0.0934333388 KL_loss: 0.0363178877 MMD_loss: 0.0144929164 time: 76.7977s\n",
      "Feature: 0243 Epoch: 0015 Loss: 3.0200059592 MSE_Loss: 2.7104936914 Sparsity_loss: 0.0933866465 KL_loss: 0.0363170627 MMD_loss: 0.0144945766 time: 76.7066s\n",
      "Begin training feature: 0244\n",
      "Feature: 0244 Epoch: 0000 Loss: 18.5984001398 MSE_Loss: 2.6954815148 Sparsity_loss: 5.2904380391 KL_loss: 0.0886178311 MMD_loss: 0.0153591554 time: 76.8703s\n",
      "Feature: 0244 Epoch: 0005 Loss: 3.7567464972 MSE_Loss: 3.0006031575 Sparsity_loss: 0.2393352995 KL_loss: 0.0391365381 MMD_loss: 0.0188730415 time: 76.8941s\n",
      "Feature: 0244 Epoch: 0010 Loss: 3.5309428938 MSE_Loss: 3.0263993055 Sparsity_loss: 0.1554396611 KL_loss: 0.0374240371 MMD_loss: 0.0189251942 time: 76.8204s\n",
      "Feature: 0244 Epoch: 0015 Loss: 3.5771095702 MSE_Loss: 3.0458771556 Sparsity_loss: 0.1643107542 KL_loss: 0.0376718874 MMD_loss: 0.0189617082 time: 76.8238s\n",
      "Begin training feature: 0245\n",
      "Feature: 0245 Epoch: 0000 Loss: 14.3477801321 MSE_Loss: 2.1734313245 Sparsity_loss: 4.0500569981 KL_loss: 0.0473784635 MMD_loss: 0.0118520057 time: 76.9482s\n",
      "Feature: 0245 Epoch: 0005 Loss: 1.9182332743 MSE_Loss: 1.6972436817 Sparsity_loss: 0.0661483536 KL_loss: 0.0339285940 MMD_loss: 0.0111026256 time: 76.6846s\n",
      "Feature: 0245 Epoch: 0010 Loss: 1.8251094700 MSE_Loss: 1.6141262765 Sparsity_loss: 0.0628402109 KL_loss: 0.0338461123 MMD_loss: 0.0110620462 time: 76.8544s\n",
      "Feature: 0245 Epoch: 0015 Loss: 1.7637079073 MSE_Loss: 1.5610834431 Sparsity_loss: 0.0600672132 KL_loss: 0.0338850671 MMD_loss: 0.0110419893 time: 76.7080s\n",
      "Begin training feature: 0246\n",
      "Feature: 0246 Epoch: 0000 Loss: 17.8700373467 MSE_Loss: 2.2228362862 Sparsity_loss: 5.2098192600 KL_loss: 0.0874174249 MMD_loss: 0.0084345464 time: 76.8573s\n",
      "Feature: 0246 Epoch: 0005 Loss: 3.2796319904 MSE_Loss: 2.3068122692 Sparsity_loss: 0.3170610727 KL_loss: 0.0399623428 MMD_loss: 0.0106184454 time: 76.8243s\n",
      "Feature: 0246 Epoch: 0010 Loss: 3.0583155018 MSE_Loss: 2.4761548833 Sparsity_loss: 0.1864774281 KL_loss: 0.0379782812 MMD_loss: 0.0111742598 time: 76.7034s\n",
      "Feature: 0246 Epoch: 0015 Loss: 2.9306340427 MSE_Loss: 2.5345303482 Sparsity_loss: 0.1243787577 KL_loss: 0.0368651837 MMD_loss: 0.0112993830 time: 76.8310s\n",
      "Begin training feature: 0247\n",
      "Feature: 0247 Epoch: 0000 Loss: 19.5541628705 MSE_Loss: 3.7128260496 Sparsity_loss: 5.2684470622 KL_loss: 0.0651834422 MMD_loss: 0.0176719272 time: 76.8388s\n",
      "Feature: 0247 Epoch: 0005 Loss: 5.3763272204 MSE_Loss: 3.3305655354 Sparsity_loss: 0.6710672643 KL_loss: 0.0090494224 MMD_loss: 0.0162346971 time: 76.8439s\n",
      "Feature: 0247 Epoch: 0010 Loss: 3.8357698463 MSE_Loss: 3.5189887089 Sparsity_loss: 0.0942816010 KL_loss: 0.0360030333 MMD_loss: 0.0167881457 time: 76.7477s\n",
      "Feature: 0247 Epoch: 0015 Loss: 3.6348380623 MSE_Loss: 3.2705647267 Sparsity_loss: 0.1101598695 KL_loss: 0.0354473416 MMD_loss: 0.0167196407 time: 76.8348s\n",
      "Begin training feature: 0248\n",
      "Feature: 0248 Epoch: 0000 Loss: 16.5064699215 MSE_Loss: 2.0994298346 Sparsity_loss: 4.7952113364 KL_loss: 0.0577108928 MMD_loss: 0.0104144596 time: 76.8310s\n",
      "Feature: 0248 Epoch: 0005 Loss: 2.3221297219 MSE_Loss: 1.8842747804 Sparsity_loss: 0.1388299879 KL_loss: 0.0365434859 MMD_loss: 0.0104997663 time: 76.7776s\n",
      "Feature: 0248 Epoch: 0010 Loss: 2.3396044958 MSE_Loss: 1.8708164088 Sparsity_loss: 0.1491699202 KL_loss: 0.0372547696 MMD_loss: 0.0104528805 time: 76.8571s\n",
      "Feature: 0248 Epoch: 0015 Loss: 2.3285888647 MSE_Loss: 1.8415430048 Sparsity_loss: 0.1553864720 KL_loss: 0.0374236431 MMD_loss: 0.0102561176 time: 76.8328s\n",
      "Begin training feature: 0249\n",
      "Feature: 0249 Epoch: 0000 Loss: 29.2062736830 MSE_Loss: 7.2089250047 Sparsity_loss: 7.3187081565 KL_loss: 0.0831017754 MMD_loss: 0.0201965494 time: 76.7925s\n",
      "Feature: 0249 Epoch: 0005 Loss: 12.2324450496 MSE_Loss: 5.6163759159 Sparsity_loss: 2.1929211881 KL_loss: 0.0153026964 MMD_loss: 0.0185762480 time: 76.8820s\n",
      "Feature: 0249 Epoch: 0010 Loss: 7.5903766726 MSE_Loss: 5.6329516958 Sparsity_loss: 0.6383430196 KL_loss: 0.0046269484 MMD_loss: 0.0211747953 time: 76.8387s\n",
      "Feature: 0249 Epoch: 0015 Loss: nan MSE_Loss: nan Sparsity_loss: nan KL_loss: nan MMD_loss: nan time: 76.1475s\n",
      "Begin training feature: 0250\n",
      "Feature: 0250 Epoch: 0000 Loss: 26.6281629524 MSE_Loss: 5.5714820504 Sparsity_loss: 7.0063900553 KL_loss: 0.0959774981 MMD_loss: 0.0182754923 time: 76.8588s\n",
      "Feature: 0250 Epoch: 0005 Loss: 9.1680774138 MSE_Loss: 5.1757376009 Sparsity_loss: 1.3187276207 KL_loss: 0.0177942803 MMD_loss: 0.0179895116 time: 76.8425s\n",
      "Feature: 0250 Epoch: 0010 Loss: 9.1132816673 MSE_Loss: 5.1285468138 Sparsity_loss: 1.3164088624 KL_loss: 0.0180394014 MMD_loss: 0.0176639233 time: 76.7901s\n",
      "Feature: 0250 Epoch: 0015 Loss: 7.7754756244 MSE_Loss: 5.1026937103 Sparsity_loss: 0.8789018961 KL_loss: 0.0435237450 MMD_loss: 0.0178205138 time: 76.8339s\n",
      "Begin training feature: 0251\n",
      "Feature: 0251 Epoch: 0000 Loss: 27.2513817194 MSE_Loss: 8.8982870944 Sparsity_loss: 6.1025537957 KL_loss: 0.0808231617 MMD_loss: 0.0223125304 time: 76.8641s\n",
      "Feature: 0251 Epoch: 0005 Loss: 12.2485124968 MSE_Loss: 8.5550420353 Sparsity_loss: 1.2136283124 KL_loss: 0.0424945049 MMD_loss: 0.0260802656 time: 76.8591s\n",
      "Feature: 0251 Epoch: 0010 Loss: 9.8364625057 MSE_Loss: 9.0489245350 Sparsity_loss: 0.2423534966 KL_loss: 0.0318939816 MMD_loss: 0.0300792731 time: 76.8134s\n",
      "Feature: 0251 Epoch: 0015 Loss: 9.4937980053 MSE_Loss: 8.7718944071 Sparsity_loss: 0.2203109310 KL_loss: 0.0318383187 MMD_loss: 0.0303262038 time: 76.8797s\n",
      "Begin training feature: 0252\n",
      "Feature: 0252 Epoch: 0000 Loss: 24.5214883563 MSE_Loss: 5.2991260072 Sparsity_loss: 6.3933090896 KL_loss: 0.0646164809 MMD_loss: 0.0208944101 time: 76.1969s\n",
      "Feature: 0252 Epoch: 0005 Loss: 6.1276423985 MSE_Loss: 4.3697034084 Sparsity_loss: 0.5734325503 KL_loss: 0.0454333958 MMD_loss: 0.0185934864 time: 76.3485s\n",
      "Feature: 0252 Epoch: 0010 Loss: 5.4574464879 MSE_Loss: 4.7168787212 Sparsity_loss: 0.2327910254 KL_loss: 0.0376221533 MMD_loss: 0.0209092286 time: 76.0285s\n",
      "Feature: 0252 Epoch: 0015 Loss: 5.0172185797 MSE_Loss: 4.3330193072 Sparsity_loss: 0.2136219271 KL_loss: 0.0354972648 MMD_loss: 0.0214892340 time: 76.4394s\n",
      "Begin training feature: 0253\n",
      "Feature: 0253 Epoch: 0000 Loss: 23.1339966421 MSE_Loss: 6.5964095700 Sparsity_loss: 5.4941531496 KL_loss: 0.0935662004 MMD_loss: 0.0270960270 time: 76.3516s\n",
      "Feature: 0253 Epoch: 0005 Loss: 9.8941707663 MSE_Loss: 7.1038155113 Sparsity_loss: 0.9098313566 KL_loss: 0.0505102470 MMD_loss: 0.0301780645 time: 75.9819s\n",
      "Feature: 0253 Epoch: 0010 Loss: 9.3057896107 MSE_Loss: 7.8285652711 Sparsity_loss: 0.4708662936 KL_loss: 0.0425738060 MMD_loss: 0.0320998836 time: 76.3518s\n",
      "Feature: 0253 Epoch: 0015 Loss: 8.8207004764 MSE_Loss: 8.0094778176 Sparsity_loss: 0.2484079493 KL_loss: 0.0390847178 MMD_loss: 0.0328039696 time: 76.0181s\n",
      "Begin training feature: 0254\n",
      "Feature: 0254 Epoch: 0000 Loss: 16.0261945717 MSE_Loss: 2.8955973993 Sparsity_loss: 4.3638601370 KL_loss: 0.0731340960 MMD_loss: 0.0191427272 time: 76.4362s\n",
      "Feature: 0254 Epoch: 0005 Loss: 3.9817209804 MSE_Loss: 3.5038967350 Sparsity_loss: 0.1442448707 KL_loss: 0.0358248625 MMD_loss: 0.0223656879 time: 76.3706s\n",
      "Feature: 0254 Epoch: 0010 Loss: 3.4368710222 MSE_Loss: 3.0962773291 Sparsity_loss: 0.0981205201 KL_loss: 0.0323797081 MMD_loss: 0.0229541656 time: 75.5399s\n",
      "Feature: 0254 Epoch: 0015 Loss: 3.3269549429 MSE_Loss: 2.9581046158 Sparsity_loss: 0.1075853941 KL_loss: 0.0323730037 MMD_loss: 0.0228852154 time: 76.7448s\n",
      "Begin training feature: 0255\n",
      "Feature: 0255 Epoch: 0000 Loss: 20.1454773038 MSE_Loss: 3.3044703083 Sparsity_loss: 5.5989501294 KL_loss: 0.0961474472 MMD_loss: 0.0215975173 time: 76.9384s\n",
      "Feature: 0255 Epoch: 0005 Loss: 5.9292290534 MSE_Loss: 3.1842099832 Sparsity_loss: 0.9003703086 KL_loss: 0.0494304377 MMD_loss: 0.0217069061 time: 76.0263s\n",
      "Feature: 0255 Epoch: 0010 Loss: 4.5186227523 MSE_Loss: 3.4485448543 Sparsity_loss: 0.3414790190 KL_loss: 0.0407444083 MMD_loss: 0.0226167203 time: 75.7136s\n",
      "Feature: 0255 Epoch: 0015 Loss: 4.3037342121 MSE_Loss: 3.7889379517 Sparsity_loss: 0.1556602707 KL_loss: 0.0374245376 MMD_loss: 0.0237206167 time: 76.3218s\n",
      "Begin training feature: 0256\n",
      "Feature: 0256 Epoch: 0000 Loss: 23.4848544170 MSE_Loss: 5.7731667998 Sparsity_loss: 5.8917259675 KL_loss: 0.0764016594 MMD_loss: 0.0178728886 time: 76.8754s\n",
      "Feature: 0256 Epoch: 0005 Loss: 7.6698312745 MSE_Loss: 5.6091655206 Sparsity_loss: 0.6716860185 KL_loss: 0.0382803995 MMD_loss: 0.0226124518 time: 75.7810s\n",
      "Feature: 0256 Epoch: 0010 Loss: 6.0159633803 MSE_Loss: 5.5304526346 Sparsity_loss: 0.1448022385 KL_loss: 0.0314490408 MMD_loss: 0.0253947793 time: 76.8180s\n",
      "Feature: 0256 Epoch: 0015 Loss: 5.9198121731 MSE_Loss: 5.4406112657 Sparsity_loss: 0.1427881403 KL_loss: 0.0314279667 MMD_loss: 0.0252610842 time: 76.8200s\n",
      "Begin training feature: 0257\n",
      "Feature: 0257 Epoch: 0000 Loss: 20.5325839538 MSE_Loss: 3.1888182893 Sparsity_loss: 5.7686615883 KL_loss: 0.0827399347 MMD_loss: 0.0184767568 time: 76.8561s\n",
      "Feature: 0257 Epoch: 0005 Loss: 5.7174619881 MSE_Loss: 3.1043486268 Sparsity_loss: 0.8571895537 KL_loss: 0.0498874944 MMD_loss: 0.0205229288 time: 76.8032s\n",
      "Feature: 0257 Epoch: 0010 Loss: 4.7137591804 MSE_Loss: 3.8292489383 Sparsity_loss: 0.2794862425 KL_loss: 0.0396382464 MMD_loss: 0.0228275469 time: 76.7518s\n",
      "Feature: 0257 Epoch: 0015 Loss: 4.6563472265 MSE_Loss: 3.8649044537 Sparsity_loss: 0.2484277173 KL_loss: 0.0390847872 MMD_loss: 0.0228843840 time: 76.7845s\n",
      "Begin training feature: 0258\n",
      "Feature: 0258 Epoch: 0000 Loss: 27.8055891894 MSE_Loss: 9.7546357861 Sparsity_loss: 6.0015547793 KL_loss: 0.0800340932 MMD_loss: 0.0227443336 time: 76.7841s\n",
      "Feature: 0258 Epoch: 0005 Loss: 13.1486223767 MSE_Loss: 9.4404721529 Sparsity_loss: 1.2201541225 KL_loss: 0.0563593905 MMD_loss: 0.0235621222 time: 76.7362s\n",
      "Feature: 0258 Epoch: 0010 Loss: 11.9620488560 MSE_Loss: 10.5281337087 Sparsity_loss: 0.4594134020 KL_loss: 0.0427625983 MMD_loss: 0.0276237035 time: 76.7676s\n",
      "Feature: 0258 Epoch: 0015 Loss: 11.8383456956 MSE_Loss: 10.8492117989 Sparsity_loss: 0.3105101533 KL_loss: 0.0401904130 MMD_loss: 0.0286007736 time: 76.8490s\n",
      "Begin training feature: 0259\n",
      "Feature: 0259 Epoch: 0000 Loss: 18.5072536662 MSE_Loss: 3.2123576737 Sparsity_loss: 5.0856233908 KL_loss: 0.0692383152 MMD_loss: 0.0186667305 time: 76.8551s\n",
      "Feature: 0259 Epoch: 0005 Loss: 6.2233935972 MSE_Loss: 3.4430006183 Sparsity_loss: 0.9127379092 KL_loss: 0.0122563101 MMD_loss: 0.0210283427 time: 76.8925s\n",
      "Feature: 0259 Epoch: 0010 Loss: 4.4037130562 MSE_Loss: 3.6307842981 Sparsity_loss: 0.2428682052 KL_loss: 0.0389710769 MMD_loss: 0.0219672092 time: 76.8143s\n",
      "Feature: 0259 Epoch: 0015 Loss: 4.3562890826 MSE_Loss: 3.6597075711 Sparsity_loss: 0.2173723514 KL_loss: 0.0385294837 MMD_loss: 0.0220395856 time: 76.8613s\n",
      "Begin training feature: 0260\n",
      "Feature: 0260 Epoch: 0000 Loss: 22.3833734971 MSE_Loss: 4.8769033222 Sparsity_loss: 5.8233382322 KL_loss: 0.0880687780 MMD_loss: 0.0177873698 time: 76.7332s\n",
      "Feature: 0260 Epoch: 0005 Loss: 6.1883653495 MSE_Loss: 4.7841238092 Sparsity_loss: 0.4541079455 KL_loss: 0.0425342017 MMD_loss: 0.0207461677 time: 76.8438s\n",
      "Feature: 0260 Epoch: 0010 Loss: 5.9168823122 MSE_Loss: 4.6158387064 Sparsity_loss: 0.4194817842 KL_loss: 0.0407580119 MMD_loss: 0.0210953449 time: 76.8238s\n",
      "Feature: 0260 Epoch: 0015 Loss: 5.3229349230 MSE_Loss: 4.4465242438 Sparsity_loss: 0.2769719864 KL_loss: 0.0357672057 MMD_loss: 0.0225685211 time: 76.7606s\n",
      "Begin training feature: 0261\n",
      "Feature: 0261 Epoch: 0000 Loss: 21.7440038851 MSE_Loss: 5.8476476111 Sparsity_loss: 5.2815918770 KL_loss: 0.0795925013 MMD_loss: 0.0253923711 time: 76.9374s\n",
      "Feature: 0261 Epoch: 0005 Loss: 6.7918723580 MSE_Loss: 5.4590038474 Sparsity_loss: 0.4259161536 KL_loss: 0.0376759069 MMD_loss: 0.0273716416 time: 76.3329s\n",
      "Feature: 0261 Epoch: 0010 Loss: 6.5888257361 MSE_Loss: 5.2549228981 Sparsity_loss: 0.4262788828 KL_loss: 0.0371104921 MMD_loss: 0.0273475426 time: 76.1199s\n",
      "Feature: 0261 Epoch: 0015 Loss: 5.9987569847 MSE_Loss: 5.3101436540 Sparsity_loss: 0.2099994686 KL_loss: 0.0312351057 MMD_loss: 0.0291512786 time: 76.2366s\n",
      "Begin training feature: 0262\n",
      "Feature: 0262 Epoch: 0000 Loss: 18.9249175603 MSE_Loss: 2.8583680975 Sparsity_loss: 5.3422682326 KL_loss: 0.0670254665 MMD_loss: 0.0195372321 time: 76.5428s\n",
      "Feature: 0262 Epoch: 0005 Loss: 4.0100724385 MSE_Loss: 2.9711121686 Sparsity_loss: 0.3315784503 KL_loss: 0.0366787525 MMD_loss: 0.0219290585 time: 73.8054s\n",
      "Feature: 0262 Epoch: 0010 Loss: 3.8415732161 MSE_Loss: 2.9766658714 Sparsity_loss: 0.2735621500 KL_loss: 0.0367142784 MMD_loss: 0.0219268779 time: 76.2598s\n",
      "Feature: 0262 Epoch: 0015 Loss: 3.2667654057 MSE_Loss: 3.0198347197 Sparsity_loss: 0.0670633021 KL_loss: 0.0341864737 MMD_loss: 0.0226994620 time: 76.2574s\n",
      "Begin training feature: 0263\n",
      "Feature: 0263 Epoch: 0000 Loss: 31.6794282791 MSE_Loss: 13.4462156587 Sparsity_loss: 6.0620704664 KL_loss: 0.0842134834 MMD_loss: 0.0230794925 time: 76.2281s\n",
      "Feature: 0263 Epoch: 0005 Loss: 18.3546517197 MSE_Loss: 12.8893602118 Sparsity_loss: 1.8070841505 KL_loss: 0.0667002044 MMD_loss: 0.0216860209 time: 76.2843s\n",
      "Feature: 0263 Epoch: 0010 Loss: 16.4642095860 MSE_Loss: 14.3622839827 Sparsity_loss: 0.6832063946 KL_loss: 0.0468266839 MMD_loss: 0.0259190461 time: 76.2908s\n",
      "Feature: 0263 Epoch: 0015 Loss: 16.4833241216 MSE_Loss: 14.5498055938 Sparsity_loss: 0.6263214533 KL_loss: 0.0463450041 MMD_loss: 0.0270453250 time: 76.2493s\n",
      "Begin training feature: 0264\n",
      "Feature: 0264 Epoch: 0000 Loss: 19.1524008485 MSE_Loss: 3.6030934122 Sparsity_loss: 5.1678593821 KL_loss: 0.0621557016 MMD_loss: 0.0225538749 time: 76.0719s\n",
      "Feature: 0264 Epoch: 0005 Loss: 5.3008293363 MSE_Loss: 3.5731032397 Sparsity_loss: 0.5594554066 KL_loss: 0.0446197652 MMD_loss: 0.0244568476 time: 76.2832s\n",
      "Feature: 0264 Epoch: 0010 Loss: 4.7320403112 MSE_Loss: 3.3700951041 Sparsity_loss: 0.4372733202 KL_loss: 0.0402802645 MMD_loss: 0.0248612226 time: 75.1558s\n",
      "Feature: 0264 Epoch: 0015 Loss: 4.1995243510 MSE_Loss: 3.3232352039 Sparsity_loss: 0.2747282962 KL_loss: 0.0358959090 MMD_loss: 0.0258726523 time: 76.2867s\n"
     ]
    }
   ],
   "source": [
    "log_file = os.path.join('/home/omnisky/Public/ChenRongfa/Intrer_VAE_result/help4_3', 'log_val.txt')\n",
    "log = open(log_file, 'w')\n",
    "for idx in range(230,264):\n",
    "    print('Begin training feature: {:04d}'.format(idx + 1))\n",
    "    decoder_file = 'decoder' + str(idx) + '.pt'\n",
    "    decoder_file = os.path.join('/home/omnisky/Public/ChenRongfa/Intrer_VAE_result/DLCR_PNC_emoid4', decoder_file)\n",
    "    encoder_file = 'encoder' + str(idx) + '.pt'\n",
    "    encoder_file = os.path.join('/home/omnisky/Public/ChenRongfa/Intrer_VAE_result/help4_3', encoder_file)\n",
    "\n",
    "    Inter_decoder = decoder(args.dims, args.decoder_hidden, args.time_step - 1, args.num_nodes, args.decoder_dropout, args.decoder_alpha)\n",
    "    Inter_decoder.load_state_dict(torch.load(decoder_file, map_location='cuda:1'))\n",
    "    Inter_decoder = Inter_decoder.cuda()\n",
    "    Inter_decoder.eval()\n",
    "\n",
    "    Inter_encoder = encoder(init_adj, args.dims, args.encoder_hidden, args.dims, args.time_step - 1, args.encoder_dropout, args.encoder_alpha)\n",
    "    Inter_encoder = Inter_encoder.cuda()\n",
    "\n",
    "    optimizer = optim.Adam(Inter_encoder.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
    "    loss_mse = nn.MSELoss()\n",
    "    best_loss = np.inf\n",
    "    \n",
    "    for epoch in range(20):\n",
    "        t = time.time()\n",
    "        Loss = []\n",
    "        MSE_loss = []\n",
    "        SPA_loss = []\n",
    "        KL_loss = []\n",
    "        MMD_loss = []\n",
    "        for batch_idx, data in enumerate(data_loader):\n",
    "            optimizer.zero_grad()\n",
    "            data = data.cuda()\n",
    "            target = data[:, idx, 1:, :]\n",
    "            inputs = data[:, :, :-1, :]\n",
    "\n",
    "            mu, log_var = Inter_encoder(inputs)  #Inter_encoder(inputs, adj)\n",
    "            sigma = torch.exp(log_var / 2)\n",
    "            # sigma2 = torch.exp(log_var2 / 2)\n",
    "            gamma = torch.randn(size = mu.size()).cuda()\n",
    "            # theta = torch.randn(size = mu2.size()).cuda()\n",
    "            gamma = mu + sigma * gamma\n",
    "            # theta = mu2 + sigma2 * theta\n",
    "            mask = torch.sigmoid(gamma) #* torch.sigmoid(theta) #* torch.sigmoid(theta + gamma)\n",
    "            # gamma = torch.sigmoid(gamma)\n",
    "            # theta = torch.sigmoid(theta)\n",
    "\n",
    "            inputs = mask_inputs(mask, inputs)\n",
    "            pred = Inter_decoder(inputs, idx)   #Inter_decoder(inputs, adj, idx)\n",
    "\n",
    "\n",
    "\n",
    "            mse_loss = loss_mse(pred, target)\n",
    "            spa_loss = loss_sparsity(mask, 'log_sum')\n",
    "            kl_loss = loss_divergence(mask, 'JS')\n",
    "            mmd_loss = loss_mmd(data[:, :, 1:, :], pred, idx)\n",
    "\n",
    "            loss = mse_loss + args.beta_sparsity * spa_loss + args.beta_kl * kl_loss + args.beta_mmd * mmd_loss\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            Loss.append(loss.item())\n",
    "            MSE_loss.append(mse_loss.item())\n",
    "            SPA_loss.append(spa_loss.item())\n",
    "            KL_loss.append(kl_loss.item())\n",
    "            MMD_loss.append(mmd_loss.item())\n",
    "        \n",
    "        # if epoch == 500:\n",
    "        #     optimizer.param_groups[0]['lr'] = args.lr/10\n",
    "\n",
    "        if epoch % 5 == 0:\n",
    "            print(  'Feature: {:04d}'.format(idx + 1),\n",
    "                    'Epoch: {:04d}'.format(epoch),\n",
    "                    'Loss: {:.10f}'.format(np.mean(Loss)),\n",
    "                    'MSE_Loss: {:.10f}'.format(np.mean(MSE_loss)),\n",
    "                    'Sparsity_loss: {:.10f}'.format(np.mean(SPA_loss)),\n",
    "                    'KL_loss: {:.10f}'.format(np.mean(KL_loss)),\n",
    "                    'MMD_loss: {:.10f}'.format(np.mean(MMD_loss)),\n",
    "                    'time: {:.4f}s'.format(time.time() - t))\n",
    "                \n",
    "        if np.mean(Loss) < best_loss:\n",
    "            best_loss = np.mean(Loss)\n",
    "            #M[idx, :] = \n",
    "            # gamma_matrix[idx, :] = gamma.squeeze().mean(dim=2).mean(dim=1)\n",
    "            # theta_matrix[idx, :] = theta.squeeze().mean(dim=2).mean(dim=1)\n",
    "            # torch.save({\n",
    "            #             'encoder_state_dict': Inter_encoder.state_dict(),\n",
    "            #             'decoder_state_dict': Inter_decoder.state_dict(),\n",
    "            #                 # 'adj' : adj\n",
    "\n",
    "            #             }, encoder_file)\n",
    "            torch.save(Inter_encoder.state_dict(), encoder_file)\n",
    "                # np.save(save_file + str(idx) + '.npy', mask.cpu().detach().numpy())\n",
    "\n",
    "            print('Feature: {:04d}'.format(idx + 1),\n",
    "                  'Epoch: {:04d}'.format(epoch),\n",
    "                  'Loss: {:.10f}'.format(np.mean(Loss)),\n",
    "                  'mse_loss: {:.10f}'.format(np.mean(MSE_loss)),\n",
    "                  'Sparsity_loss: {:.10f}'.format(np.mean(SPA_loss)),\n",
    "                  'KL_loss: {:.10f}'.format(np.mean(KL_loss)),\n",
    "                  'mmd_loss: {:.10f}'.format(np.mean(MMD_loss)),\n",
    "                  'time: {:.4f}s'.format(time.time() - t), file=log)\n",
    "        log.flush()\n",
    "log.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "ae5e1b78a42d3a00ffbad3028f2882e576ba2589a97bb9b799d13c9e22855bf5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
