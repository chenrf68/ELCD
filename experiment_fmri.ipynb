{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as scio\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "from Model import *\n",
    "from itertools import chain\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from synthetic import simulate_lorenz_96, simulate_var\n",
    "from utils import build_flags, time_split, save_result, evaluate_result, count_accuracy, loss_sparsity, loss_divergence, loss_mmd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = build_flags()\n",
    "args = parser.parse_args(args=[])\n",
    "args.seed = 2\n",
    "args.num_nodes = 50\n",
    "args.dims = 1\n",
    "args.threshold = 0.5\n",
    "args.time_length = 500\n",
    "args.time_step = 10\n",
    "args.epochs = 3000\n",
    "args.batch_size = 128\n",
    "args.lr = 1e-3\n",
    "args.weight_decay = 1e-3\n",
    "args.encoder_alpha = 0.02\n",
    "args.decoder_alpha = 0.02\n",
    "args.beta_sparsity = 0.05 #0.25   #log_sum  #1.25是F=40,N=30的最优值\n",
    "args.beta_kl = 0.01        #JS散度\n",
    "args.beta_mmd = 2      #MMD\n",
    "args.encoder_hidden = 20\n",
    "args.decoder_hidden = 20\n",
    "args.encoder_dropout = 0.1\n",
    "args.decoder_dropout = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim2 = scio.loadmat('/home/omnisky/Public/ChenRongfa/sim4.mat')\n",
    "X_np = sim2['ts']\n",
    "GC = sim2['net']\n",
    "GC = GC.mean(axis=0)\n",
    "GC[GC != 0] = 1\n",
    "GC[GC == 0] = 0\n",
    "X_np_ori = X_np\n",
    "X_np = X_np.transpose(1, 0)\n",
    "X_np = X_np[:, :, np.newaxis]\n",
    "X_np = np.array(time_split(X_np, step=10))\n",
    "X_np = torch.FloatTensor(X_np)\n",
    "data = X_np\n",
    "data_loader = DataLoader(data, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 1. 0. ... 0. 0. 0.]\n",
      " [0. 1. 1. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 1. 1. 0.]\n",
      " [0. 0. 0. ... 0. 1. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "print(GC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9991, 50, 10, 1])\n"
     ]
    }
   ],
   "source": [
    "print(X_np.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin training feature: 0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jing_xuzijian/crf/Intrer_VAE/Model.py:20: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n",
      "  nn.init.xavier_normal(m.weight.data, gain=1.414)\n",
      "/home/jing_xuzijian/anaconda3/envs/crf_base/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature: 0001 Epoch: 0000 Loss: 4.8551774357 MSE_Loss: 4.8551774357 time: 0.3910s\n",
      "Feature: 0001 Epoch: 0100 Loss: 1.2090841502 MSE_Loss: 1.2090841502 time: 0.3378s\n",
      "Feature: 0001 Epoch: 0200 Loss: 1.2123735826 MSE_Loss: 1.2123735826 time: 0.3387s\n",
      "Feature: 0001 Epoch: 0300 Loss: 1.1832028982 MSE_Loss: 1.1832028982 time: 0.3402s\n",
      "Feature: 0001 Epoch: 0400 Loss: 1.1763658591 MSE_Loss: 1.1763658591 time: 0.3404s\n",
      "Feature: 0001 Epoch: 0500 Loss: 1.1784881127 MSE_Loss: 1.1784881127 time: 0.3399s\n",
      "Feature: 0001 Epoch: 0600 Loss: 1.1752353899 MSE_Loss: 1.1752353899 time: 0.3396s\n",
      "Feature: 0001 Epoch: 0700 Loss: 1.1801268617 MSE_Loss: 1.1801268617 time: 0.3392s\n",
      "Feature: 0001 Epoch: 0800 Loss: 1.2245335752 MSE_Loss: 1.2245335752 time: 0.3389s\n",
      "Feature: 0001 Epoch: 0900 Loss: 1.1747200776 MSE_Loss: 1.1747200776 time: 0.3390s\n",
      "Feature: 0001 Epoch: 1000 Loss: 1.1659333200 MSE_Loss: 1.1659333200 time: 0.3385s\n",
      "Feature: 0001 Epoch: 1100 Loss: 1.1504379816 MSE_Loss: 1.1504379816 time: 0.3385s\n",
      "Feature: 0001 Epoch: 1200 Loss: 1.1912179084 MSE_Loss: 1.1912179084 time: 0.3388s\n",
      "Feature: 0001 Epoch: 1300 Loss: 1.1841635410 MSE_Loss: 1.1841635410 time: 0.3386s\n",
      "Feature: 0001 Epoch: 1400 Loss: 1.1808490783 MSE_Loss: 1.1808490783 time: 0.3387s\n",
      "Feature: 0001 Epoch: 1500 Loss: 1.1471722458 MSE_Loss: 1.1471722458 time: 0.3389s\n",
      "Feature: 0001 Epoch: 1600 Loss: 1.1825056370 MSE_Loss: 1.1825056370 time: 0.3388s\n",
      "Feature: 0001 Epoch: 1700 Loss: 1.1790402554 MSE_Loss: 1.1790402554 time: 0.3387s\n",
      "Feature: 0001 Epoch: 1800 Loss: 1.1665936056 MSE_Loss: 1.1665936056 time: 0.3387s\n",
      "Feature: 0001 Epoch: 1900 Loss: 1.1365259358 MSE_Loss: 1.1365259358 time: 0.3387s\n",
      "Begin training feature: 0002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jing_xuzijian/crf/Intrer_VAE/Model.py:20: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n",
      "  nn.init.xavier_normal(m.weight.data, gain=1.414)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature: 0002 Epoch: 0000 Loss: 5.3167497780 MSE_Loss: 5.3167497780 time: 0.3391s\n",
      "Feature: 0002 Epoch: 0100 Loss: 1.3171343781 MSE_Loss: 1.3171343781 time: 0.3386s\n",
      "Feature: 0002 Epoch: 0200 Loss: 1.3128853591 MSE_Loss: 1.3128853591 time: 0.3386s\n",
      "Feature: 0002 Epoch: 0300 Loss: 1.3410400680 MSE_Loss: 1.3410400680 time: 0.3385s\n",
      "Feature: 0002 Epoch: 0400 Loss: 1.3208794609 MSE_Loss: 1.3208794609 time: 0.3383s\n",
      "Feature: 0002 Epoch: 0500 Loss: 1.3279370462 MSE_Loss: 1.3279370462 time: 0.3386s\n",
      "Feature: 0002 Epoch: 0600 Loss: 1.3424132564 MSE_Loss: 1.3424132564 time: 0.3385s\n",
      "Feature: 0002 Epoch: 0700 Loss: 1.2972113649 MSE_Loss: 1.2972113649 time: 0.3386s\n",
      "Feature: 0002 Epoch: 0800 Loss: 1.3070105627 MSE_Loss: 1.3070105627 time: 0.3387s\n",
      "Feature: 0002 Epoch: 0900 Loss: 1.3059064889 MSE_Loss: 1.3059064889 time: 0.3389s\n",
      "Feature: 0002 Epoch: 1000 Loss: 1.2738546125 MSE_Loss: 1.2738546125 time: 0.3391s\n",
      "Feature: 0002 Epoch: 1100 Loss: 1.2825020680 MSE_Loss: 1.2825020680 time: 0.3392s\n",
      "Feature: 0002 Epoch: 1200 Loss: 1.2967395126 MSE_Loss: 1.2967395126 time: 0.3390s\n",
      "Feature: 0002 Epoch: 1300 Loss: 1.2988546615 MSE_Loss: 1.2988546615 time: 0.3392s\n",
      "Feature: 0002 Epoch: 1400 Loss: 1.2864703992 MSE_Loss: 1.2864703992 time: 0.3393s\n",
      "Feature: 0002 Epoch: 1500 Loss: 1.2820558374 MSE_Loss: 1.2820558374 time: 0.3390s\n",
      "Feature: 0002 Epoch: 1600 Loss: 1.2880550366 MSE_Loss: 1.2880550366 time: 0.3387s\n",
      "Feature: 0002 Epoch: 1700 Loss: 1.2641181919 MSE_Loss: 1.2641181919 time: 0.3386s\n",
      "Feature: 0002 Epoch: 1800 Loss: 1.2783933814 MSE_Loss: 1.2783933814 time: 0.3386s\n",
      "Feature: 0002 Epoch: 1900 Loss: 1.2595657963 MSE_Loss: 1.2595657963 time: 0.3386s\n",
      "Begin training feature: 0003\n",
      "Feature: 0003 Epoch: 0000 Loss: 4.8510745175 MSE_Loss: 4.8510745175 time: 0.3386s\n",
      "Feature: 0003 Epoch: 0100 Loss: 1.1710811167 MSE_Loss: 1.1710811167 time: 0.3387s\n",
      "Feature: 0003 Epoch: 0200 Loss: 1.1739499426 MSE_Loss: 1.1739499426 time: 0.3387s\n",
      "Feature: 0003 Epoch: 0300 Loss: 1.1712314879 MSE_Loss: 1.1712314879 time: 0.3391s\n",
      "Feature: 0003 Epoch: 0400 Loss: 1.1340729911 MSE_Loss: 1.1340729911 time: 0.3387s\n",
      "Feature: 0003 Epoch: 0500 Loss: 1.1196384694 MSE_Loss: 1.1196384694 time: 0.3390s\n",
      "Feature: 0003 Epoch: 0600 Loss: 1.1647470925 MSE_Loss: 1.1647470925 time: 0.3391s\n",
      "Feature: 0003 Epoch: 0700 Loss: 1.1306762197 MSE_Loss: 1.1306762197 time: 0.3392s\n",
      "Feature: 0003 Epoch: 0800 Loss: 1.1403155334 MSE_Loss: 1.1403155334 time: 0.3396s\n",
      "Feature: 0003 Epoch: 0900 Loss: 1.1549848982 MSE_Loss: 1.1549848982 time: 0.3392s\n",
      "Feature: 0003 Epoch: 1000 Loss: 1.1258554489 MSE_Loss: 1.1258554489 time: 0.3389s\n",
      "Feature: 0003 Epoch: 1100 Loss: 1.1454913118 MSE_Loss: 1.1454913118 time: 0.3389s\n",
      "Feature: 0003 Epoch: 1200 Loss: 1.1214146539 MSE_Loss: 1.1214146539 time: 0.3388s\n",
      "Feature: 0003 Epoch: 1300 Loss: 1.1292186115 MSE_Loss: 1.1292186115 time: 0.3386s\n",
      "Feature: 0003 Epoch: 1400 Loss: 1.1189928953 MSE_Loss: 1.1189928953 time: 0.3384s\n",
      "Feature: 0003 Epoch: 1500 Loss: 1.0989633987 MSE_Loss: 1.0989633987 time: 0.3386s\n",
      "Feature: 0003 Epoch: 1600 Loss: 1.1066959440 MSE_Loss: 1.1066959440 time: 0.3385s\n",
      "Feature: 0003 Epoch: 1700 Loss: 1.1248965626 MSE_Loss: 1.1248965626 time: 0.3388s\n",
      "Feature: 0003 Epoch: 1800 Loss: 1.0963338553 MSE_Loss: 1.0963338553 time: 0.3389s\n",
      "Feature: 0003 Epoch: 1900 Loss: 1.1140564167 MSE_Loss: 1.1140564167 time: 0.3391s\n",
      "Begin training feature: 0004\n",
      "Feature: 0004 Epoch: 0000 Loss: 5.0028131551 MSE_Loss: 5.0028131551 time: 0.3397s\n",
      "Feature: 0004 Epoch: 0100 Loss: 1.2505970318 MSE_Loss: 1.2505970318 time: 0.3392s\n",
      "Feature: 0004 Epoch: 0200 Loss: 1.2488120832 MSE_Loss: 1.2488120832 time: 0.3392s\n",
      "Feature: 0004 Epoch: 0300 Loss: 1.2985411407 MSE_Loss: 1.2985411407 time: 0.3393s\n",
      "Feature: 0004 Epoch: 0400 Loss: 1.2564302223 MSE_Loss: 1.2564302223 time: 0.3390s\n",
      "Feature: 0004 Epoch: 0500 Loss: 1.2642248341 MSE_Loss: 1.2642248341 time: 0.3390s\n",
      "Feature: 0004 Epoch: 0600 Loss: 1.2643195775 MSE_Loss: 1.2643195775 time: 0.3385s\n",
      "Feature: 0004 Epoch: 0700 Loss: 1.2114348019 MSE_Loss: 1.2114348019 time: 0.3387s\n",
      "Feature: 0004 Epoch: 0800 Loss: 1.2568631127 MSE_Loss: 1.2568631127 time: 0.3384s\n",
      "Feature: 0004 Epoch: 0900 Loss: 1.2516349252 MSE_Loss: 1.2516349252 time: 0.3386s\n",
      "Feature: 0004 Epoch: 1000 Loss: 1.2144688888 MSE_Loss: 1.2144688888 time: 0.3387s\n",
      "Feature: 0004 Epoch: 1100 Loss: 1.2706493275 MSE_Loss: 1.2706493275 time: 0.3388s\n",
      "Feature: 0004 Epoch: 1200 Loss: 1.2560946896 MSE_Loss: 1.2560946896 time: 0.3388s\n",
      "Feature: 0004 Epoch: 1300 Loss: 1.2248364755 MSE_Loss: 1.2248364755 time: 0.3389s\n",
      "Feature: 0004 Epoch: 1400 Loss: 1.2592959517 MSE_Loss: 1.2592959517 time: 0.3390s\n",
      "Feature: 0004 Epoch: 1500 Loss: 1.2778813424 MSE_Loss: 1.2778813424 time: 0.3388s\n",
      "Feature: 0004 Epoch: 1600 Loss: 1.2637904090 MSE_Loss: 1.2637904090 time: 0.3391s\n",
      "Feature: 0004 Epoch: 1700 Loss: 1.2172844516 MSE_Loss: 1.2172844516 time: 0.3394s\n",
      "Feature: 0004 Epoch: 1800 Loss: 1.2456781404 MSE_Loss: 1.2456781404 time: 0.3392s\n",
      "Feature: 0004 Epoch: 1900 Loss: 1.2652060910 MSE_Loss: 1.2652060910 time: 0.3393s\n",
      "Begin training feature: 0005\n",
      "Feature: 0005 Epoch: 0000 Loss: 5.1150920663 MSE_Loss: 5.1150920663 time: 0.3388s\n",
      "Feature: 0005 Epoch: 0100 Loss: 1.2701610757 MSE_Loss: 1.2701610757 time: 0.3386s\n",
      "Feature: 0005 Epoch: 0200 Loss: 1.2251976495 MSE_Loss: 1.2251976495 time: 0.3387s\n",
      "Feature: 0005 Epoch: 0300 Loss: 1.2623638156 MSE_Loss: 1.2623638156 time: 0.3386s\n",
      "Feature: 0005 Epoch: 0400 Loss: 1.2411701521 MSE_Loss: 1.2411701521 time: 0.3386s\n",
      "Feature: 0005 Epoch: 0500 Loss: 1.2479342871 MSE_Loss: 1.2479342871 time: 0.3386s\n",
      "Feature: 0005 Epoch: 0600 Loss: 1.2601122026 MSE_Loss: 1.2601122026 time: 0.3387s\n",
      "Feature: 0005 Epoch: 0700 Loss: 1.2701536500 MSE_Loss: 1.2701536500 time: 0.3374s\n",
      "Feature: 0005 Epoch: 0800 Loss: 1.2446865447 MSE_Loss: 1.2446865447 time: 0.3378s\n",
      "Feature: 0005 Epoch: 0900 Loss: 1.2697036862 MSE_Loss: 1.2697036862 time: 0.3379s\n",
      "Feature: 0005 Epoch: 1000 Loss: 1.2449550470 MSE_Loss: 1.2449550470 time: 0.3380s\n",
      "Feature: 0005 Epoch: 1100 Loss: 1.2831833016 MSE_Loss: 1.2831833016 time: 0.3380s\n",
      "Feature: 0005 Epoch: 1200 Loss: 1.2499082526 MSE_Loss: 1.2499082526 time: 0.3381s\n",
      "Feature: 0005 Epoch: 1300 Loss: 1.2985053794 MSE_Loss: 1.2985053794 time: 0.3382s\n",
      "Feature: 0005 Epoch: 1400 Loss: 1.2368064410 MSE_Loss: 1.2368064410 time: 0.3380s\n",
      "Feature: 0005 Epoch: 1500 Loss: 1.2746135940 MSE_Loss: 1.2746135940 time: 0.3377s\n",
      "Feature: 0005 Epoch: 1600 Loss: 1.2711363684 MSE_Loss: 1.2711363684 time: 0.3376s\n",
      "Feature: 0005 Epoch: 1700 Loss: 1.2426091855 MSE_Loss: 1.2426091855 time: 0.3377s\n",
      "Feature: 0005 Epoch: 1800 Loss: 1.2626117641 MSE_Loss: 1.2626117641 time: 0.3374s\n",
      "Feature: 0005 Epoch: 1900 Loss: 1.2643968863 MSE_Loss: 1.2643968863 time: 0.3373s\n",
      "Begin training feature: 0006\n",
      "Feature: 0006 Epoch: 0000 Loss: 5.0199969418 MSE_Loss: 5.0199969418 time: 0.3376s\n",
      "Feature: 0006 Epoch: 0100 Loss: 1.1942609632 MSE_Loss: 1.1942609632 time: 0.3374s\n",
      "Feature: 0006 Epoch: 0200 Loss: 1.2421819032 MSE_Loss: 1.2421819032 time: 0.3375s\n",
      "Feature: 0006 Epoch: 0300 Loss: 1.2778917943 MSE_Loss: 1.2778917943 time: 0.3383s\n",
      "Feature: 0006 Epoch: 0400 Loss: 1.2089423205 MSE_Loss: 1.2089423205 time: 0.3380s\n",
      "Feature: 0006 Epoch: 0500 Loss: 1.2137254861 MSE_Loss: 1.2137254861 time: 0.3378s\n",
      "Feature: 0006 Epoch: 0600 Loss: 1.2179286646 MSE_Loss: 1.2179286646 time: 0.3381s\n",
      "Feature: 0006 Epoch: 0700 Loss: 1.2300748478 MSE_Loss: 1.2300748478 time: 0.3384s\n",
      "Feature: 0006 Epoch: 0800 Loss: 1.2026883673 MSE_Loss: 1.2026883673 time: 0.3381s\n",
      "Feature: 0006 Epoch: 0900 Loss: 1.2129022034 MSE_Loss: 1.2129022034 time: 0.3379s\n",
      "Feature: 0006 Epoch: 1000 Loss: 1.2084226774 MSE_Loss: 1.2084226774 time: 0.3374s\n",
      "Feature: 0006 Epoch: 1100 Loss: 1.2477763808 MSE_Loss: 1.2477763808 time: 0.3376s\n",
      "Feature: 0006 Epoch: 1200 Loss: 1.2116599075 MSE_Loss: 1.2116599075 time: 0.3375s\n",
      "Feature: 0006 Epoch: 1300 Loss: 1.2056784901 MSE_Loss: 1.2056784901 time: 0.3374s\n",
      "Feature: 0006 Epoch: 1400 Loss: 1.1908985444 MSE_Loss: 1.1908985444 time: 0.3375s\n",
      "Feature: 0006 Epoch: 1500 Loss: 1.2547055272 MSE_Loss: 1.2547055272 time: 0.3377s\n",
      "Feature: 0006 Epoch: 1600 Loss: 1.1897484234 MSE_Loss: 1.1897484234 time: 0.3372s\n",
      "Feature: 0006 Epoch: 1700 Loss: 1.1876099585 MSE_Loss: 1.1876099585 time: 0.3377s\n",
      "Feature: 0006 Epoch: 1800 Loss: 1.2309877261 MSE_Loss: 1.2309877261 time: 0.3376s\n",
      "Feature: 0006 Epoch: 1900 Loss: 1.1742876519 MSE_Loss: 1.1742876519 time: 0.3378s\n",
      "Begin training feature: 0007\n",
      "Feature: 0007 Epoch: 0000 Loss: 5.3074741907 MSE_Loss: 5.3074741907 time: 0.3384s\n",
      "Feature: 0007 Epoch: 0100 Loss: 1.3548739930 MSE_Loss: 1.3548739930 time: 0.3377s\n",
      "Feature: 0007 Epoch: 0200 Loss: 1.3333486198 MSE_Loss: 1.3333486198 time: 0.3380s\n",
      "Feature: 0007 Epoch: 0300 Loss: 1.3112634383 MSE_Loss: 1.3112634383 time: 0.3380s\n",
      "Feature: 0007 Epoch: 0400 Loss: 1.3245087025 MSE_Loss: 1.3245087025 time: 0.3379s\n",
      "Feature: 0007 Epoch: 0500 Loss: 1.3286567527 MSE_Loss: 1.3286567527 time: 0.3374s\n",
      "Feature: 0007 Epoch: 0600 Loss: 1.3082386270 MSE_Loss: 1.3082386270 time: 0.3375s\n",
      "Feature: 0007 Epoch: 0700 Loss: 1.3279690245 MSE_Loss: 1.3279690245 time: 0.3372s\n",
      "Feature: 0007 Epoch: 0800 Loss: 1.2917062580 MSE_Loss: 1.2917062580 time: 0.3373s\n",
      "Feature: 0007 Epoch: 0900 Loss: 1.2858241985 MSE_Loss: 1.2858241985 time: 0.3374s\n",
      "Feature: 0007 Epoch: 1000 Loss: 1.2979791270 MSE_Loss: 1.2979791270 time: 0.3373s\n",
      "Feature: 0007 Epoch: 1100 Loss: 1.2805606376 MSE_Loss: 1.2805606376 time: 0.3372s\n",
      "Feature: 0007 Epoch: 1200 Loss: 1.2807484692 MSE_Loss: 1.2807484692 time: 0.3375s\n",
      "Feature: 0007 Epoch: 1300 Loss: 1.2885006266 MSE_Loss: 1.2885006266 time: 0.3375s\n",
      "Feature: 0007 Epoch: 1400 Loss: 1.2999942906 MSE_Loss: 1.2999942906 time: 0.3378s\n",
      "Feature: 0007 Epoch: 1500 Loss: 1.2491952543 MSE_Loss: 1.2491952543 time: 0.3378s\n",
      "Feature: 0007 Epoch: 1600 Loss: 1.2917555608 MSE_Loss: 1.2917555608 time: 0.3382s\n",
      "Feature: 0007 Epoch: 1700 Loss: 1.2898645303 MSE_Loss: 1.2898645303 time: 0.3384s\n",
      "Feature: 0007 Epoch: 1800 Loss: 1.2644333394 MSE_Loss: 1.2644333394 time: 0.3378s\n",
      "Feature: 0007 Epoch: 1900 Loss: 1.3052022849 MSE_Loss: 1.3052022849 time: 0.3374s\n",
      "Begin training feature: 0008\n",
      "Feature: 0008 Epoch: 0000 Loss: 4.7736168330 MSE_Loss: 4.7736168330 time: 0.3386s\n",
      "Feature: 0008 Epoch: 0100 Loss: 1.2031292251 MSE_Loss: 1.2031292251 time: 0.3385s\n",
      "Feature: 0008 Epoch: 0200 Loss: 1.1293459218 MSE_Loss: 1.1293459218 time: 0.3385s\n",
      "Feature: 0008 Epoch: 0300 Loss: 1.1502040964 MSE_Loss: 1.1502040964 time: 0.3383s\n",
      "Feature: 0008 Epoch: 0400 Loss: 1.1336031164 MSE_Loss: 1.1336031164 time: 0.3385s\n",
      "Feature: 0008 Epoch: 0500 Loss: 1.1213754091 MSE_Loss: 1.1213754091 time: 0.3383s\n",
      "Feature: 0008 Epoch: 0600 Loss: 1.1801208665 MSE_Loss: 1.1801208665 time: 0.3385s\n",
      "Feature: 0008 Epoch: 0700 Loss: 1.1342801799 MSE_Loss: 1.1342801799 time: 0.3389s\n",
      "Feature: 0008 Epoch: 0800 Loss: 1.1359871965 MSE_Loss: 1.1359871965 time: 0.3389s\n",
      "Feature: 0008 Epoch: 0900 Loss: 1.1639065841 MSE_Loss: 1.1639065841 time: 0.3390s\n",
      "Feature: 0008 Epoch: 1000 Loss: 1.1094022408 MSE_Loss: 1.1094022408 time: 0.3393s\n",
      "Feature: 0008 Epoch: 1100 Loss: 1.1280521185 MSE_Loss: 1.1280521185 time: 0.3391s\n",
      "Feature: 0008 Epoch: 1200 Loss: 1.1245161001 MSE_Loss: 1.1245161001 time: 0.3396s\n",
      "Feature: 0008 Epoch: 1300 Loss: 1.1045239100 MSE_Loss: 1.1045239100 time: 0.3392s\n",
      "Feature: 0008 Epoch: 1400 Loss: 1.1112507982 MSE_Loss: 1.1112507982 time: 0.3386s\n",
      "Feature: 0008 Epoch: 1500 Loss: 1.0651580080 MSE_Loss: 1.0651580080 time: 0.3657s\n",
      "Feature: 0008 Epoch: 1600 Loss: 1.1067929524 MSE_Loss: 1.1067929524 time: 0.3656s\n",
      "Feature: 0008 Epoch: 1700 Loss: 1.1073929268 MSE_Loss: 1.1073929268 time: 0.3657s\n",
      "Feature: 0008 Epoch: 1800 Loss: 1.0839960107 MSE_Loss: 1.0839960107 time: 0.3656s\n",
      "Feature: 0008 Epoch: 1900 Loss: 1.1308207738 MSE_Loss: 1.1308207738 time: 0.3651s\n",
      "Begin training feature: 0009\n",
      "Feature: 0009 Epoch: 0000 Loss: 4.8587892357 MSE_Loss: 4.8587892357 time: 0.3659s\n",
      "Feature: 0009 Epoch: 0100 Loss: 1.2562466913 MSE_Loss: 1.2562466913 time: 0.3661s\n",
      "Feature: 0009 Epoch: 0200 Loss: 1.2240388725 MSE_Loss: 1.2240388725 time: 0.3655s\n",
      "Feature: 0009 Epoch: 0300 Loss: 1.2309218668 MSE_Loss: 1.2309218668 time: 0.3655s\n",
      "Feature: 0009 Epoch: 0400 Loss: 1.1820387772 MSE_Loss: 1.1820387772 time: 0.3658s\n",
      "Feature: 0009 Epoch: 0500 Loss: 1.2050120725 MSE_Loss: 1.2050120725 time: 0.3661s\n",
      "Feature: 0009 Epoch: 0600 Loss: 1.2294839862 MSE_Loss: 1.2294839862 time: 0.3660s\n",
      "Feature: 0009 Epoch: 0700 Loss: 1.1994254400 MSE_Loss: 1.1994254400 time: 0.3653s\n",
      "Feature: 0009 Epoch: 0800 Loss: 1.1951112506 MSE_Loss: 1.1951112506 time: 0.3657s\n",
      "Feature: 0009 Epoch: 0900 Loss: 1.2024954754 MSE_Loss: 1.2024954754 time: 0.3651s\n",
      "Feature: 0009 Epoch: 1000 Loss: 1.2230188160 MSE_Loss: 1.2230188160 time: 0.3653s\n",
      "Feature: 0009 Epoch: 1100 Loss: 1.2263109352 MSE_Loss: 1.2263109352 time: 0.3654s\n",
      "Feature: 0009 Epoch: 1200 Loss: 1.2217206019 MSE_Loss: 1.2217206019 time: 0.3656s\n",
      "Feature: 0009 Epoch: 1300 Loss: 1.2031307568 MSE_Loss: 1.2031307568 time: 0.3656s\n",
      "Feature: 0009 Epoch: 1400 Loss: 1.1880471231 MSE_Loss: 1.1880471231 time: 0.3657s\n",
      "Feature: 0009 Epoch: 1500 Loss: 1.1918030245 MSE_Loss: 1.1918030245 time: 0.3659s\n",
      "Feature: 0009 Epoch: 1600 Loss: 1.2083100111 MSE_Loss: 1.2083100111 time: 0.3659s\n",
      "Feature: 0009 Epoch: 1700 Loss: 1.2110311887 MSE_Loss: 1.2110311887 time: 0.3660s\n",
      "Feature: 0009 Epoch: 1800 Loss: 1.1995333463 MSE_Loss: 1.1995333463 time: 0.3658s\n",
      "Feature: 0009 Epoch: 1900 Loss: 1.2001881939 MSE_Loss: 1.2001881939 time: 0.3660s\n",
      "Begin training feature: 0010\n",
      "Feature: 0010 Epoch: 0000 Loss: 5.2265773665 MSE_Loss: 5.2265773665 time: 0.3664s\n",
      "Feature: 0010 Epoch: 0100 Loss: 1.3510404184 MSE_Loss: 1.3510404184 time: 0.3653s\n",
      "Feature: 0010 Epoch: 0200 Loss: 1.3121260978 MSE_Loss: 1.3121260978 time: 0.3653s\n",
      "Feature: 0010 Epoch: 0300 Loss: 1.3459738297 MSE_Loss: 1.3459738297 time: 0.3654s\n",
      "Feature: 0010 Epoch: 0400 Loss: 1.3240182392 MSE_Loss: 1.3240182392 time: 0.3654s\n",
      "Feature: 0010 Epoch: 0500 Loss: 1.3341389027 MSE_Loss: 1.3341389027 time: 0.3650s\n",
      "Feature: 0010 Epoch: 0600 Loss: 1.3078291205 MSE_Loss: 1.3078291205 time: 0.3650s\n",
      "Feature: 0010 Epoch: 0700 Loss: 1.2488771934 MSE_Loss: 1.2488771934 time: 0.3657s\n",
      "Feature: 0010 Epoch: 0800 Loss: 1.3168057581 MSE_Loss: 1.3168057581 time: 0.3656s\n",
      "Feature: 0010 Epoch: 0900 Loss: 1.2579776796 MSE_Loss: 1.2579776796 time: 0.3658s\n",
      "Feature: 0010 Epoch: 1000 Loss: 1.3016152820 MSE_Loss: 1.3016152820 time: 0.3658s\n",
      "Feature: 0010 Epoch: 1100 Loss: 1.2918778930 MSE_Loss: 1.2918778930 time: 0.3660s\n",
      "Feature: 0010 Epoch: 1200 Loss: 1.3279545451 MSE_Loss: 1.3279545451 time: 0.3661s\n",
      "Feature: 0010 Epoch: 1300 Loss: 1.2855675024 MSE_Loss: 1.2855675024 time: 0.3656s\n",
      "Feature: 0010 Epoch: 1400 Loss: 1.2563078358 MSE_Loss: 1.2563078358 time: 0.3658s\n",
      "Feature: 0010 Epoch: 1500 Loss: 1.2750484144 MSE_Loss: 1.2750484144 time: 0.3657s\n",
      "Feature: 0010 Epoch: 1600 Loss: 1.3055279549 MSE_Loss: 1.3055279549 time: 0.3659s\n",
      "Feature: 0010 Epoch: 1700 Loss: 1.3068477436 MSE_Loss: 1.3068477436 time: 0.3655s\n",
      "Feature: 0010 Epoch: 1800 Loss: 1.2808817889 MSE_Loss: 1.2808817889 time: 0.3655s\n",
      "Feature: 0010 Epoch: 1900 Loss: 1.2410608862 MSE_Loss: 1.2410608862 time: 0.3652s\n",
      "Begin training feature: 0011\n",
      "Feature: 0011 Epoch: 0000 Loss: 5.2218615798 MSE_Loss: 5.2218615798 time: 0.3654s\n",
      "Feature: 0011 Epoch: 0100 Loss: 1.2930047512 MSE_Loss: 1.2930047512 time: 0.3650s\n",
      "Feature: 0011 Epoch: 0200 Loss: 1.2786652257 MSE_Loss: 1.2786652257 time: 0.3650s\n",
      "Feature: 0011 Epoch: 0300 Loss: 1.2622150832 MSE_Loss: 1.2622150832 time: 0.3660s\n",
      "Feature: 0011 Epoch: 0400 Loss: 1.2491613451 MSE_Loss: 1.2491613451 time: 0.3658s\n",
      "Feature: 0011 Epoch: 0500 Loss: 1.2791493580 MSE_Loss: 1.2791493580 time: 0.3655s\n",
      "Feature: 0011 Epoch: 0600 Loss: 1.2906800757 MSE_Loss: 1.2906800757 time: 0.3656s\n",
      "Feature: 0011 Epoch: 0700 Loss: 1.2716847213 MSE_Loss: 1.2716847213 time: 0.3655s\n",
      "Feature: 0011 Epoch: 0800 Loss: 1.2789504747 MSE_Loss: 1.2789504747 time: 0.3658s\n",
      "Feature: 0011 Epoch: 0900 Loss: 1.2629216668 MSE_Loss: 1.2629216668 time: 0.3656s\n",
      "Feature: 0011 Epoch: 1000 Loss: 1.2176198190 MSE_Loss: 1.2176198190 time: 0.3654s\n",
      "Feature: 0011 Epoch: 1100 Loss: 1.2450978907 MSE_Loss: 1.2450978907 time: 0.3654s\n",
      "Feature: 0011 Epoch: 1200 Loss: 1.2713971544 MSE_Loss: 1.2713971544 time: 0.3648s\n",
      "Feature: 0011 Epoch: 1300 Loss: 1.2458418327 MSE_Loss: 1.2458418327 time: 0.3652s\n",
      "Feature: 0011 Epoch: 1400 Loss: 1.2420728622 MSE_Loss: 1.2420728622 time: 0.3646s\n",
      "Feature: 0011 Epoch: 1500 Loss: 1.2617615137 MSE_Loss: 1.2617615137 time: 0.3657s\n",
      "Feature: 0011 Epoch: 1600 Loss: 1.2673416326 MSE_Loss: 1.2673416326 time: 0.3659s\n",
      "Feature: 0011 Epoch: 1700 Loss: 1.2409392580 MSE_Loss: 1.2409392580 time: 0.3653s\n",
      "Feature: 0011 Epoch: 1800 Loss: 1.2744556997 MSE_Loss: 1.2744556997 time: 0.3656s\n",
      "Feature: 0011 Epoch: 1900 Loss: 1.2581481066 MSE_Loss: 1.2581481066 time: 0.3661s\n",
      "Begin training feature: 0012\n",
      "Feature: 0012 Epoch: 0000 Loss: 5.3469988787 MSE_Loss: 5.3469988787 time: 0.3660s\n",
      "Feature: 0012 Epoch: 0100 Loss: 1.3343064589 MSE_Loss: 1.3343064589 time: 0.3664s\n",
      "Feature: 0012 Epoch: 0200 Loss: 1.3548435486 MSE_Loss: 1.3548435486 time: 0.3659s\n",
      "Feature: 0012 Epoch: 0300 Loss: 1.3046519138 MSE_Loss: 1.3046519138 time: 0.3654s\n",
      "Feature: 0012 Epoch: 0400 Loss: 1.3345264204 MSE_Loss: 1.3345264204 time: 0.3656s\n",
      "Feature: 0012 Epoch: 0500 Loss: 1.2865314552 MSE_Loss: 1.2865314552 time: 0.3656s\n",
      "Feature: 0012 Epoch: 0600 Loss: 1.2955670515 MSE_Loss: 1.2955670515 time: 0.3653s\n",
      "Feature: 0012 Epoch: 0700 Loss: 1.3097535221 MSE_Loss: 1.3097535221 time: 0.3653s\n",
      "Feature: 0012 Epoch: 0800 Loss: 1.3571181116 MSE_Loss: 1.3571181116 time: 0.3656s\n",
      "Feature: 0012 Epoch: 0900 Loss: 1.2823359861 MSE_Loss: 1.2823359861 time: 0.3656s\n",
      "Feature: 0012 Epoch: 1000 Loss: 1.3027290468 MSE_Loss: 1.3027290468 time: 0.3658s\n",
      "Feature: 0012 Epoch: 1100 Loss: 1.2874384070 MSE_Loss: 1.2874384070 time: 0.3657s\n",
      "Feature: 0012 Epoch: 1200 Loss: 1.2803417010 MSE_Loss: 1.2803417010 time: 0.3660s\n",
      "Feature: 0012 Epoch: 1300 Loss: 1.2893012853 MSE_Loss: 1.2893012853 time: 0.3655s\n",
      "Feature: 0012 Epoch: 1400 Loss: 1.2772169619 MSE_Loss: 1.2772169619 time: 0.3660s\n",
      "Feature: 0012 Epoch: 1500 Loss: 1.2974597608 MSE_Loss: 1.2974597608 time: 0.3661s\n",
      "Feature: 0012 Epoch: 1600 Loss: 1.3013481996 MSE_Loss: 1.3013481996 time: 0.3659s\n",
      "Feature: 0012 Epoch: 1700 Loss: 1.3074531374 MSE_Loss: 1.3074531374 time: 0.3658s\n",
      "Feature: 0012 Epoch: 1800 Loss: 1.2825663814 MSE_Loss: 1.2825663814 time: 0.3655s\n",
      "Feature: 0012 Epoch: 1900 Loss: 1.2455403480 MSE_Loss: 1.2455403480 time: 0.3656s\n",
      "Begin training feature: 0013\n",
      "Feature: 0013 Epoch: 0000 Loss: 4.9208271202 MSE_Loss: 4.9208271202 time: 0.3658s\n",
      "Feature: 0013 Epoch: 0100 Loss: 1.1633846171 MSE_Loss: 1.1633846171 time: 0.3654s\n",
      "Feature: 0013 Epoch: 0200 Loss: 1.1666595981 MSE_Loss: 1.1666595981 time: 0.3651s\n",
      "Feature: 0013 Epoch: 0300 Loss: 1.1126103273 MSE_Loss: 1.1126103273 time: 0.3656s\n",
      "Feature: 0013 Epoch: 0400 Loss: 1.1443031000 MSE_Loss: 1.1443031000 time: 0.3659s\n",
      "Feature: 0013 Epoch: 0500 Loss: 1.1247460261 MSE_Loss: 1.1247460261 time: 0.3658s\n",
      "Feature: 0013 Epoch: 0600 Loss: 1.1345347377 MSE_Loss: 1.1345347377 time: 0.3656s\n",
      "Feature: 0013 Epoch: 0700 Loss: 1.1059692891 MSE_Loss: 1.1059692891 time: 0.3661s\n",
      "Feature: 0013 Epoch: 0800 Loss: 1.1508774056 MSE_Loss: 1.1508774056 time: 0.3656s\n",
      "Feature: 0013 Epoch: 0900 Loss: 1.0930715407 MSE_Loss: 1.0930715407 time: 0.3657s\n",
      "Feature: 0013 Epoch: 1000 Loss: 1.1317183549 MSE_Loss: 1.1317183549 time: 0.3653s\n",
      "Feature: 0013 Epoch: 1100 Loss: 1.1364603850 MSE_Loss: 1.1364603850 time: 0.3654s\n",
      "Feature: 0013 Epoch: 1200 Loss: 1.1297890257 MSE_Loss: 1.1297890257 time: 0.3654s\n",
      "Feature: 0013 Epoch: 1300 Loss: 1.1257447291 MSE_Loss: 1.1257447291 time: 0.3650s\n",
      "Feature: 0013 Epoch: 1400 Loss: 1.1032709742 MSE_Loss: 1.1032709742 time: 0.3653s\n",
      "Feature: 0013 Epoch: 1500 Loss: 1.1137401281 MSE_Loss: 1.1137401281 time: 0.3653s\n",
      "Feature: 0013 Epoch: 1600 Loss: 1.0914000763 MSE_Loss: 1.0914000763 time: 0.3656s\n",
      "Feature: 0013 Epoch: 1700 Loss: 1.1148688241 MSE_Loss: 1.1148688241 time: 0.3658s\n",
      "Feature: 0013 Epoch: 1800 Loss: 1.1286911459 MSE_Loss: 1.1286911459 time: 0.3659s\n",
      "Feature: 0013 Epoch: 1900 Loss: 1.1067529577 MSE_Loss: 1.1067529577 time: 0.3658s\n",
      "Begin training feature: 0014\n",
      "Feature: 0014 Epoch: 0000 Loss: 4.6429648943 MSE_Loss: 4.6429648943 time: 0.3659s\n",
      "Feature: 0014 Epoch: 0100 Loss: 1.2194471555 MSE_Loss: 1.2194471555 time: 0.3661s\n",
      "Feature: 0014 Epoch: 0200 Loss: 1.1621222534 MSE_Loss: 1.1621222534 time: 0.3659s\n",
      "Feature: 0014 Epoch: 0300 Loss: 1.1903104480 MSE_Loss: 1.1903104480 time: 0.3660s\n",
      "Feature: 0014 Epoch: 0400 Loss: 1.1789555067 MSE_Loss: 1.1789555067 time: 0.3653s\n",
      "Feature: 0014 Epoch: 0500 Loss: 1.1384579403 MSE_Loss: 1.1384579403 time: 0.3651s\n",
      "Feature: 0014 Epoch: 0600 Loss: 1.1525788481 MSE_Loss: 1.1525788481 time: 0.3655s\n",
      "Feature: 0014 Epoch: 0700 Loss: 1.1806874524 MSE_Loss: 1.1806874524 time: 0.3655s\n",
      "Feature: 0014 Epoch: 0800 Loss: 1.1380985945 MSE_Loss: 1.1380985945 time: 0.3655s\n",
      "Feature: 0014 Epoch: 0900 Loss: 1.1430518348 MSE_Loss: 1.1430518348 time: 0.3656s\n",
      "Feature: 0014 Epoch: 1000 Loss: 1.1572746370 MSE_Loss: 1.1572746370 time: 0.3657s\n",
      "Feature: 0014 Epoch: 1100 Loss: 1.1395714788 MSE_Loss: 1.1395714788 time: 0.3658s\n",
      "Feature: 0014 Epoch: 1200 Loss: 1.1398039430 MSE_Loss: 1.1398039430 time: 0.3660s\n",
      "Feature: 0014 Epoch: 1300 Loss: 1.1390101857 MSE_Loss: 1.1390101857 time: 0.3656s\n",
      "Feature: 0014 Epoch: 1400 Loss: 1.1422670695 MSE_Loss: 1.1422670695 time: 0.3660s\n",
      "Feature: 0014 Epoch: 1500 Loss: 1.1237130957 MSE_Loss: 1.1237130957 time: 0.3658s\n",
      "Feature: 0014 Epoch: 1600 Loss: 1.1691186277 MSE_Loss: 1.1691186277 time: 0.3658s\n",
      "Feature: 0014 Epoch: 1700 Loss: 1.1166486650 MSE_Loss: 1.1166486650 time: 0.3655s\n",
      "Feature: 0014 Epoch: 1800 Loss: 1.1692571195 MSE_Loss: 1.1692571195 time: 0.3655s\n",
      "Feature: 0014 Epoch: 1900 Loss: 1.1335326426 MSE_Loss: 1.1335326426 time: 0.3655s\n",
      "Begin training feature: 0015\n",
      "Feature: 0015 Epoch: 0000 Loss: 5.0926157945 MSE_Loss: 5.0926157945 time: 0.3655s\n",
      "Feature: 0015 Epoch: 0100 Loss: 1.3271131093 MSE_Loss: 1.3271131093 time: 0.3654s\n",
      "Feature: 0015 Epoch: 0200 Loss: 1.2479212669 MSE_Loss: 1.2479212669 time: 0.3656s\n",
      "Feature: 0015 Epoch: 0300 Loss: 1.2756742616 MSE_Loss: 1.2756742616 time: 0.3656s\n",
      "Feature: 0015 Epoch: 0400 Loss: 1.2501812758 MSE_Loss: 1.2501812758 time: 0.3656s\n",
      "Feature: 0015 Epoch: 0500 Loss: 1.2821986698 MSE_Loss: 1.2821986698 time: 0.3657s\n",
      "Feature: 0015 Epoch: 0600 Loss: 1.2729561427 MSE_Loss: 1.2729561427 time: 0.3661s\n",
      "Feature: 0015 Epoch: 0700 Loss: 1.2803289815 MSE_Loss: 1.2803289815 time: 0.3666s\n",
      "Feature: 0015 Epoch: 0800 Loss: 1.3161040899 MSE_Loss: 1.3161040899 time: 0.3660s\n",
      "Feature: 0015 Epoch: 0900 Loss: 1.2680393955 MSE_Loss: 1.2680393955 time: 0.3661s\n",
      "Feature: 0015 Epoch: 1000 Loss: 1.2457038843 MSE_Loss: 1.2457038843 time: 0.3657s\n",
      "Feature: 0015 Epoch: 1100 Loss: 1.2755093914 MSE_Loss: 1.2755093914 time: 0.3659s\n",
      "Feature: 0015 Epoch: 1200 Loss: 1.2675671819 MSE_Loss: 1.2675671819 time: 0.3659s\n",
      "Feature: 0015 Epoch: 1300 Loss: 1.2292474139 MSE_Loss: 1.2292474139 time: 0.3657s\n",
      "Feature: 0015 Epoch: 1400 Loss: 1.2427115961 MSE_Loss: 1.2427115961 time: 0.3657s\n",
      "Feature: 0015 Epoch: 1500 Loss: 1.2486941241 MSE_Loss: 1.2486941241 time: 0.3659s\n",
      "Feature: 0015 Epoch: 1600 Loss: 1.2553945968 MSE_Loss: 1.2553945968 time: 0.3657s\n",
      "Feature: 0015 Epoch: 1700 Loss: 1.2527604880 MSE_Loss: 1.2527604880 time: 0.3656s\n",
      "Feature: 0015 Epoch: 1800 Loss: 1.2384626175 MSE_Loss: 1.2384626175 time: 0.3656s\n",
      "Feature: 0015 Epoch: 1900 Loss: 1.2091171907 MSE_Loss: 1.2091171907 time: 0.3661s\n",
      "Begin training feature: 0016\n",
      "Feature: 0016 Epoch: 0000 Loss: 4.8642026925 MSE_Loss: 4.8642026925 time: 0.3663s\n",
      "Feature: 0016 Epoch: 0100 Loss: 1.2011025095 MSE_Loss: 1.2011025095 time: 0.3659s\n",
      "Feature: 0016 Epoch: 0200 Loss: 1.1772228442 MSE_Loss: 1.1772228442 time: 0.3663s\n",
      "Feature: 0016 Epoch: 0300 Loss: 1.1729275562 MSE_Loss: 1.1729275562 time: 0.3665s\n",
      "Feature: 0016 Epoch: 0400 Loss: 1.1674827775 MSE_Loss: 1.1674827775 time: 0.3660s\n",
      "Feature: 0016 Epoch: 0500 Loss: 1.1632240120 MSE_Loss: 1.1632240120 time: 0.3659s\n",
      "Feature: 0016 Epoch: 0600 Loss: 1.1951120948 MSE_Loss: 1.1951120948 time: 0.3656s\n",
      "Feature: 0016 Epoch: 0700 Loss: 1.1571809204 MSE_Loss: 1.1571809204 time: 0.3656s\n",
      "Feature: 0016 Epoch: 0800 Loss: 1.1466208930 MSE_Loss: 1.1466208930 time: 0.3651s\n",
      "Feature: 0016 Epoch: 0900 Loss: 1.1521874681 MSE_Loss: 1.1521874681 time: 0.3655s\n",
      "Feature: 0016 Epoch: 1000 Loss: 1.1409037536 MSE_Loss: 1.1409037536 time: 0.3658s\n",
      "Feature: 0016 Epoch: 1100 Loss: 1.1285231076 MSE_Loss: 1.1285231076 time: 0.3655s\n",
      "Feature: 0016 Epoch: 1200 Loss: 1.1647424932 MSE_Loss: 1.1647424932 time: 0.3657s\n",
      "Feature: 0016 Epoch: 1300 Loss: 1.1611206758 MSE_Loss: 1.1611206758 time: 0.3661s\n",
      "Feature: 0016 Epoch: 1400 Loss: 1.1258302882 MSE_Loss: 1.1258302882 time: 0.3659s\n",
      "Feature: 0016 Epoch: 1500 Loss: 1.1353490255 MSE_Loss: 1.1353490255 time: 0.3661s\n",
      "Feature: 0016 Epoch: 1600 Loss: 1.1678873029 MSE_Loss: 1.1678873029 time: 0.3660s\n",
      "Feature: 0016 Epoch: 1700 Loss: 1.1451810553 MSE_Loss: 1.1451810553 time: 0.3660s\n",
      "Feature: 0016 Epoch: 1800 Loss: 1.1439439171 MSE_Loss: 1.1439439171 time: 0.3661s\n",
      "Feature: 0016 Epoch: 1900 Loss: 1.1141596195 MSE_Loss: 1.1141596195 time: 0.3657s\n",
      "Begin training feature: 0017\n",
      "Feature: 0017 Epoch: 0000 Loss: 5.3594061091 MSE_Loss: 5.3594061091 time: 0.3657s\n",
      "Feature: 0017 Epoch: 0100 Loss: 1.3567559500 MSE_Loss: 1.3567559500 time: 0.3653s\n",
      "Feature: 0017 Epoch: 0200 Loss: 1.3066962369 MSE_Loss: 1.3066962369 time: 0.3655s\n",
      "Feature: 0017 Epoch: 0300 Loss: 1.3486317924 MSE_Loss: 1.3486317924 time: 0.3652s\n",
      "Feature: 0017 Epoch: 0400 Loss: 1.3086130181 MSE_Loss: 1.3086130181 time: 0.3659s\n",
      "Feature: 0017 Epoch: 0500 Loss: 1.3131588305 MSE_Loss: 1.3131588305 time: 0.3658s\n",
      "Feature: 0017 Epoch: 0600 Loss: 1.3148508592 MSE_Loss: 1.3148508592 time: 0.3655s\n",
      "Feature: 0017 Epoch: 0700 Loss: 1.3438317217 MSE_Loss: 1.3438317217 time: 0.3653s\n",
      "Feature: 0017 Epoch: 0800 Loss: 1.3063313787 MSE_Loss: 1.3063313787 time: 0.3659s\n",
      "Feature: 0017 Epoch: 0900 Loss: 1.2863748194 MSE_Loss: 1.2863748194 time: 0.3653s\n",
      "Feature: 0017 Epoch: 1000 Loss: 1.3148865330 MSE_Loss: 1.3148865330 time: 0.3663s\n",
      "Feature: 0017 Epoch: 1100 Loss: 1.2882552879 MSE_Loss: 1.2882552879 time: 0.3657s\n",
      "Feature: 0017 Epoch: 1200 Loss: 1.2579063192 MSE_Loss: 1.2579063192 time: 0.3657s\n",
      "Feature: 0017 Epoch: 1300 Loss: 1.2986347238 MSE_Loss: 1.2986347238 time: 0.3655s\n",
      "Feature: 0017 Epoch: 1400 Loss: 1.2331622250 MSE_Loss: 1.2331622250 time: 0.3654s\n",
      "Feature: 0017 Epoch: 1500 Loss: 1.3037502841 MSE_Loss: 1.3037502841 time: 0.3655s\n",
      "Feature: 0017 Epoch: 1600 Loss: 1.2882670111 MSE_Loss: 1.2882670111 time: 0.3654s\n",
      "Feature: 0017 Epoch: 1700 Loss: 1.2866332908 MSE_Loss: 1.2866332908 time: 0.3654s\n",
      "Feature: 0017 Epoch: 1800 Loss: 1.2869333323 MSE_Loss: 1.2869333323 time: 0.3655s\n",
      "Feature: 0017 Epoch: 1900 Loss: 1.2825162486 MSE_Loss: 1.2825162486 time: 0.3654s\n",
      "Begin training feature: 0018\n",
      "Feature: 0018 Epoch: 0000 Loss: 4.3373219454 MSE_Loss: 4.3373219454 time: 0.3662s\n",
      "Feature: 0018 Epoch: 0100 Loss: 1.0405481480 MSE_Loss: 1.0405481480 time: 0.3656s\n",
      "Feature: 0018 Epoch: 0200 Loss: 1.0419811063 MSE_Loss: 1.0419811063 time: 0.3656s\n",
      "Feature: 0018 Epoch: 0300 Loss: 1.0819800436 MSE_Loss: 1.0819800436 time: 0.3655s\n",
      "Feature: 0018 Epoch: 0400 Loss: 1.0257799482 MSE_Loss: 1.0257799482 time: 0.3658s\n",
      "Feature: 0018 Epoch: 0500 Loss: 1.0195433656 MSE_Loss: 1.0195433656 time: 0.3656s\n",
      "Feature: 0018 Epoch: 0600 Loss: 1.0273165243 MSE_Loss: 1.0273165243 time: 0.3660s\n",
      "Feature: 0018 Epoch: 0700 Loss: 1.0139269255 MSE_Loss: 1.0139269255 time: 0.3654s\n",
      "Feature: 0018 Epoch: 0800 Loss: 1.0319286623 MSE_Loss: 1.0319286623 time: 0.3655s\n",
      "Feature: 0018 Epoch: 0900 Loss: 1.0142507945 MSE_Loss: 1.0142507945 time: 0.3653s\n",
      "Feature: 0018 Epoch: 1000 Loss: 1.0077258137 MSE_Loss: 1.0077258137 time: 0.3654s\n",
      "Feature: 0018 Epoch: 1100 Loss: 1.0344023622 MSE_Loss: 1.0344023622 time: 0.3648s\n",
      "Feature: 0018 Epoch: 1200 Loss: 1.0343016145 MSE_Loss: 1.0343016145 time: 0.3652s\n",
      "Feature: 0018 Epoch: 1300 Loss: 1.0176934351 MSE_Loss: 1.0176934351 time: 0.3650s\n",
      "Feature: 0018 Epoch: 1400 Loss: 1.0127951925 MSE_Loss: 1.0127951925 time: 0.3657s\n",
      "Feature: 0018 Epoch: 1500 Loss: 1.0198663277 MSE_Loss: 1.0198663277 time: 0.3654s\n",
      "Feature: 0018 Epoch: 1600 Loss: 0.9986321224 MSE_Loss: 0.9986321224 time: 0.3660s\n",
      "Feature: 0018 Epoch: 1700 Loss: 1.0232880274 MSE_Loss: 1.0232880274 time: 0.3657s\n",
      "Feature: 0018 Epoch: 1800 Loss: 1.0223985322 MSE_Loss: 1.0223985322 time: 0.3662s\n",
      "Feature: 0018 Epoch: 1900 Loss: 1.0162539316 MSE_Loss: 1.0162539316 time: 0.3667s\n",
      "Begin training feature: 0019\n",
      "Feature: 0019 Epoch: 0000 Loss: 4.5871987071 MSE_Loss: 4.5871987071 time: 0.3654s\n",
      "Feature: 0019 Epoch: 0100 Loss: 1.1186885736 MSE_Loss: 1.1186885736 time: 0.3654s\n",
      "Feature: 0019 Epoch: 0200 Loss: 1.0927938683 MSE_Loss: 1.0927938683 time: 0.3653s\n",
      "Feature: 0019 Epoch: 0300 Loss: 1.0795080722 MSE_Loss: 1.0795080722 time: 0.3655s\n",
      "Feature: 0019 Epoch: 0400 Loss: 1.1065083222 MSE_Loss: 1.1065083222 time: 0.3652s\n",
      "Feature: 0019 Epoch: 0500 Loss: 1.0947010221 MSE_Loss: 1.0947010221 time: 0.3655s\n",
      "Feature: 0019 Epoch: 0600 Loss: 1.1165410250 MSE_Loss: 1.1165410250 time: 0.3658s\n",
      "Feature: 0019 Epoch: 0700 Loss: 1.0767968335 MSE_Loss: 1.0767968335 time: 0.3658s\n",
      "Feature: 0019 Epoch: 0800 Loss: 1.0978239277 MSE_Loss: 1.0978239277 time: 0.3659s\n",
      "Feature: 0019 Epoch: 0900 Loss: 1.0918971987 MSE_Loss: 1.0918971987 time: 0.3658s\n",
      "Feature: 0019 Epoch: 1000 Loss: 1.0957954684 MSE_Loss: 1.0957954684 time: 0.3659s\n",
      "Feature: 0019 Epoch: 1100 Loss: 1.0862128161 MSE_Loss: 1.0862128161 time: 0.3661s\n",
      "Feature: 0019 Epoch: 1200 Loss: 1.0975497772 MSE_Loss: 1.0975497772 time: 0.3660s\n",
      "Feature: 0019 Epoch: 1300 Loss: 1.1151649997 MSE_Loss: 1.1151649997 time: 0.3650s\n",
      "Feature: 0019 Epoch: 1400 Loss: 1.0791695623 MSE_Loss: 1.0791695623 time: 0.3654s\n",
      "Feature: 0019 Epoch: 1500 Loss: 1.1079260616 MSE_Loss: 1.1079260616 time: 0.3654s\n",
      "Feature: 0019 Epoch: 1600 Loss: 1.1103262320 MSE_Loss: 1.1103262320 time: 0.3649s\n",
      "Feature: 0019 Epoch: 1700 Loss: 1.0846252728 MSE_Loss: 1.0846252728 time: 0.3654s\n",
      "Feature: 0019 Epoch: 1800 Loss: 1.1141096818 MSE_Loss: 1.1141096818 time: 0.3655s\n",
      "Feature: 0019 Epoch: 1900 Loss: 1.0772935197 MSE_Loss: 1.0772935197 time: 0.3655s\n",
      "Begin training feature: 0020\n",
      "Feature: 0020 Epoch: 0000 Loss: 4.8488112522 MSE_Loss: 4.8488112522 time: 0.3658s\n",
      "Feature: 0020 Epoch: 0100 Loss: 1.2231519682 MSE_Loss: 1.2231519682 time: 0.3656s\n",
      "Feature: 0020 Epoch: 0200 Loss: 1.2492330263 MSE_Loss: 1.2492330263 time: 0.3661s\n",
      "Feature: 0020 Epoch: 0300 Loss: 1.2092784092 MSE_Loss: 1.2092784092 time: 0.3657s\n",
      "Feature: 0020 Epoch: 0400 Loss: 1.2086746021 MSE_Loss: 1.2086746021 time: 0.3652s\n",
      "Feature: 0020 Epoch: 0500 Loss: 1.2108250404 MSE_Loss: 1.2108250404 time: 0.3658s\n",
      "Feature: 0020 Epoch: 0600 Loss: 1.2056353228 MSE_Loss: 1.2056353228 time: 0.3659s\n",
      "Feature: 0020 Epoch: 0700 Loss: 1.2070967910 MSE_Loss: 1.2070967910 time: 0.3657s\n",
      "Feature: 0020 Epoch: 0800 Loss: 1.2196743254 MSE_Loss: 1.2196743254 time: 0.3652s\n",
      "Feature: 0020 Epoch: 0900 Loss: 1.1796687638 MSE_Loss: 1.1796687638 time: 0.3653s\n",
      "Feature: 0020 Epoch: 1000 Loss: 1.1738190440 MSE_Loss: 1.1738190440 time: 0.3653s\n",
      "Feature: 0020 Epoch: 1100 Loss: 1.1862182444 MSE_Loss: 1.1862182444 time: 0.3658s\n",
      "Feature: 0020 Epoch: 1200 Loss: 1.1579480005 MSE_Loss: 1.1579480005 time: 0.3654s\n",
      "Feature: 0020 Epoch: 1300 Loss: 1.1688169279 MSE_Loss: 1.1688169279 time: 0.3655s\n",
      "Feature: 0020 Epoch: 1400 Loss: 1.1985055419 MSE_Loss: 1.1985055419 time: 0.3658s\n",
      "Feature: 0020 Epoch: 1500 Loss: 1.1570982246 MSE_Loss: 1.1570982246 time: 0.3658s\n",
      "Feature: 0020 Epoch: 1600 Loss: 1.1843034044 MSE_Loss: 1.1843034044 time: 0.3657s\n",
      "Feature: 0020 Epoch: 1700 Loss: 1.1273081733 MSE_Loss: 1.1273081733 time: 0.3658s\n",
      "Feature: 0020 Epoch: 1800 Loss: 1.1808282967 MSE_Loss: 1.1808282967 time: 0.3659s\n",
      "Feature: 0020 Epoch: 1900 Loss: 1.1858422047 MSE_Loss: 1.1858422047 time: 0.3662s\n",
      "Begin training feature: 0021\n",
      "Feature: 0021 Epoch: 0000 Loss: 5.0522674579 MSE_Loss: 5.0522674579 time: 0.3661s\n",
      "Feature: 0021 Epoch: 0100 Loss: 1.2147925179 MSE_Loss: 1.2147925179 time: 0.3654s\n",
      "Feature: 0021 Epoch: 0200 Loss: 1.2148689450 MSE_Loss: 1.2148689450 time: 0.3655s\n",
      "Feature: 0021 Epoch: 0300 Loss: 1.2283677972 MSE_Loss: 1.2283677972 time: 0.3658s\n",
      "Feature: 0021 Epoch: 0400 Loss: 1.1996971693 MSE_Loss: 1.1996971693 time: 0.3653s\n",
      "Feature: 0021 Epoch: 0500 Loss: 1.2102091441 MSE_Loss: 1.2102091441 time: 0.3651s\n",
      "Feature: 0021 Epoch: 0600 Loss: 1.1968518594 MSE_Loss: 1.1968518594 time: 0.3651s\n",
      "Feature: 0021 Epoch: 0700 Loss: 1.1874885816 MSE_Loss: 1.1874885816 time: 0.3655s\n",
      "Feature: 0021 Epoch: 0800 Loss: 1.2098840385 MSE_Loss: 1.2098840385 time: 0.3653s\n",
      "Feature: 0021 Epoch: 0900 Loss: 1.2199645835 MSE_Loss: 1.2199645835 time: 0.3655s\n",
      "Feature: 0021 Epoch: 1000 Loss: 1.1921892249 MSE_Loss: 1.1921892249 time: 0.3656s\n",
      "Feature: 0021 Epoch: 1100 Loss: 1.1871386110 MSE_Loss: 1.1871386110 time: 0.3654s\n",
      "Feature: 0021 Epoch: 1200 Loss: 1.1662587569 MSE_Loss: 1.1662587569 time: 0.3660s\n",
      "Feature: 0021 Epoch: 1300 Loss: 1.2009321127 MSE_Loss: 1.2009321127 time: 0.3659s\n",
      "Feature: 0021 Epoch: 1400 Loss: 1.2361525214 MSE_Loss: 1.2361525214 time: 0.3659s\n",
      "Feature: 0021 Epoch: 1500 Loss: 1.1763538575 MSE_Loss: 1.1763538575 time: 0.3655s\n",
      "Feature: 0021 Epoch: 1600 Loss: 1.1809025565 MSE_Loss: 1.1809025565 time: 0.3654s\n",
      "Feature: 0021 Epoch: 1700 Loss: 1.1986724787 MSE_Loss: 1.1986724787 time: 0.3656s\n",
      "Feature: 0021 Epoch: 1800 Loss: 1.1834602922 MSE_Loss: 1.1834602922 time: 0.3652s\n",
      "Feature: 0021 Epoch: 1900 Loss: 1.2024830814 MSE_Loss: 1.2024830814 time: 0.3652s\n",
      "Begin training feature: 0022\n",
      "Feature: 0022 Epoch: 0000 Loss: 5.6562861189 MSE_Loss: 5.6562861189 time: 0.3658s\n",
      "Feature: 0022 Epoch: 0100 Loss: 1.4065071691 MSE_Loss: 1.4065071691 time: 0.3656s\n",
      "Feature: 0022 Epoch: 0200 Loss: 1.4032696433 MSE_Loss: 1.4032696433 time: 0.3658s\n",
      "Feature: 0022 Epoch: 0300 Loss: 1.3935335633 MSE_Loss: 1.3935335633 time: 0.3654s\n",
      "Feature: 0022 Epoch: 0400 Loss: 1.3821714053 MSE_Loss: 1.3821714053 time: 0.3651s\n",
      "Feature: 0022 Epoch: 0500 Loss: 1.3954304140 MSE_Loss: 1.3954304140 time: 0.3658s\n",
      "Feature: 0022 Epoch: 0600 Loss: 1.3544277873 MSE_Loss: 1.3544277873 time: 0.3654s\n",
      "Feature: 0022 Epoch: 0700 Loss: 1.3925122883 MSE_Loss: 1.3925122883 time: 0.3660s\n",
      "Feature: 0022 Epoch: 0800 Loss: 1.3983099332 MSE_Loss: 1.3983099332 time: 0.3657s\n",
      "Feature: 0022 Epoch: 0900 Loss: 1.3783054450 MSE_Loss: 1.3783054450 time: 0.3654s\n",
      "Feature: 0022 Epoch: 1000 Loss: 1.3659558266 MSE_Loss: 1.3659558266 time: 0.3653s\n",
      "Feature: 0022 Epoch: 1100 Loss: 1.3327114748 MSE_Loss: 1.3327114748 time: 0.3655s\n",
      "Feature: 0022 Epoch: 1200 Loss: 1.3827311155 MSE_Loss: 1.3827311155 time: 0.3653s\n",
      "Feature: 0022 Epoch: 1300 Loss: 1.3560616510 MSE_Loss: 1.3560616510 time: 0.3656s\n",
      "Feature: 0022 Epoch: 1400 Loss: 1.3568146244 MSE_Loss: 1.3568146244 time: 0.3654s\n",
      "Feature: 0022 Epoch: 1500 Loss: 1.3355762544 MSE_Loss: 1.3355762544 time: 0.3656s\n",
      "Feature: 0022 Epoch: 1600 Loss: 1.4024264888 MSE_Loss: 1.4024264888 time: 0.3654s\n",
      "Feature: 0022 Epoch: 1700 Loss: 1.4164903104 MSE_Loss: 1.4164903104 time: 0.3658s\n",
      "Feature: 0022 Epoch: 1800 Loss: 1.3849815764 MSE_Loss: 1.3849815764 time: 0.3659s\n",
      "Feature: 0022 Epoch: 1900 Loss: 1.3593014879 MSE_Loss: 1.3593014879 time: 0.3652s\n",
      "Begin training feature: 0023\n",
      "Feature: 0023 Epoch: 0000 Loss: 4.1617780547 MSE_Loss: 4.1617780547 time: 0.3662s\n",
      "Feature: 0023 Epoch: 0100 Loss: 1.0552490346 MSE_Loss: 1.0552490346 time: 0.3655s\n",
      "Feature: 0023 Epoch: 0200 Loss: 1.0473916881 MSE_Loss: 1.0473916881 time: 0.3657s\n",
      "Feature: 0023 Epoch: 0300 Loss: 1.0238987647 MSE_Loss: 1.0238987647 time: 0.3653s\n",
      "Feature: 0023 Epoch: 0400 Loss: 1.0307913732 MSE_Loss: 1.0307913732 time: 0.3659s\n",
      "Feature: 0023 Epoch: 0500 Loss: 1.0534219704 MSE_Loss: 1.0534219704 time: 0.3657s\n",
      "Feature: 0023 Epoch: 0600 Loss: 1.0063217873 MSE_Loss: 1.0063217873 time: 0.3654s\n",
      "Feature: 0023 Epoch: 0700 Loss: 1.0323003568 MSE_Loss: 1.0323003568 time: 0.3658s\n",
      "Feature: 0023 Epoch: 0800 Loss: 1.0018450218 MSE_Loss: 1.0018450218 time: 0.3661s\n",
      "Feature: 0023 Epoch: 0900 Loss: 1.0462817708 MSE_Loss: 1.0462817708 time: 0.3662s\n",
      "Feature: 0023 Epoch: 1000 Loss: 1.0112111719 MSE_Loss: 1.0112111719 time: 0.3662s\n",
      "Feature: 0023 Epoch: 1100 Loss: 1.0120125892 MSE_Loss: 1.0120125892 time: 0.3662s\n",
      "Feature: 0023 Epoch: 1200 Loss: 1.0294841407 MSE_Loss: 1.0294841407 time: 0.3660s\n",
      "Feature: 0023 Epoch: 1300 Loss: 1.0068130535 MSE_Loss: 1.0068130535 time: 0.3665s\n",
      "Feature: 0023 Epoch: 1400 Loss: 1.0167823075 MSE_Loss: 1.0167823075 time: 0.3664s\n",
      "Feature: 0023 Epoch: 1500 Loss: 1.0242642059 MSE_Loss: 1.0242642059 time: 0.3664s\n",
      "Feature: 0023 Epoch: 1600 Loss: 0.9792451776 MSE_Loss: 0.9792451776 time: 0.3661s\n",
      "Feature: 0023 Epoch: 1700 Loss: 1.0453452030 MSE_Loss: 1.0453452030 time: 0.3658s\n",
      "Feature: 0023 Epoch: 1800 Loss: 0.9876711384 MSE_Loss: 0.9876711384 time: 0.3657s\n",
      "Feature: 0023 Epoch: 1900 Loss: 1.0223918685 MSE_Loss: 1.0223918685 time: 0.3656s\n",
      "Begin training feature: 0024\n",
      "Feature: 0024 Epoch: 0000 Loss: 4.3306087482 MSE_Loss: 4.3306087482 time: 0.3658s\n",
      "Feature: 0024 Epoch: 0100 Loss: 1.1121864281 MSE_Loss: 1.1121864281 time: 0.3653s\n",
      "Feature: 0024 Epoch: 0200 Loss: 1.0951234879 MSE_Loss: 1.0951234879 time: 0.3660s\n",
      "Feature: 0024 Epoch: 0300 Loss: 1.1143004253 MSE_Loss: 1.1143004253 time: 0.3658s\n",
      "Feature: 0024 Epoch: 0400 Loss: 1.0803069435 MSE_Loss: 1.0803069435 time: 0.3654s\n",
      "Feature: 0024 Epoch: 0500 Loss: 1.1102196152 MSE_Loss: 1.1102196152 time: 0.3658s\n",
      "Feature: 0024 Epoch: 0600 Loss: 1.0618607364 MSE_Loss: 1.0618607364 time: 0.3660s\n",
      "Feature: 0024 Epoch: 0700 Loss: 1.0907214669 MSE_Loss: 1.0907214669 time: 0.3655s\n",
      "Feature: 0024 Epoch: 0800 Loss: 1.0953414274 MSE_Loss: 1.0953414274 time: 0.3661s\n",
      "Feature: 0024 Epoch: 0900 Loss: 1.0899843657 MSE_Loss: 1.0899843657 time: 0.3667s\n",
      "Feature: 0024 Epoch: 1000 Loss: 1.0900470418 MSE_Loss: 1.0900470418 time: 0.3662s\n",
      "Feature: 0024 Epoch: 1100 Loss: 1.0823478133 MSE_Loss: 1.0823478133 time: 0.3658s\n",
      "Feature: 0024 Epoch: 1200 Loss: 1.0740788707 MSE_Loss: 1.0740788707 time: 0.3660s\n",
      "Feature: 0024 Epoch: 1300 Loss: 1.0822029559 MSE_Loss: 1.0822029559 time: 0.3658s\n",
      "Feature: 0024 Epoch: 1400 Loss: 1.0700596993 MSE_Loss: 1.0700596993 time: 0.3657s\n",
      "Feature: 0024 Epoch: 1500 Loss: 1.0948801388 MSE_Loss: 1.0948801388 time: 0.3652s\n",
      "Feature: 0024 Epoch: 1600 Loss: 1.0981502797 MSE_Loss: 1.0981502797 time: 0.3659s\n",
      "Feature: 0024 Epoch: 1700 Loss: 1.0983810666 MSE_Loss: 1.0983810666 time: 0.3659s\n",
      "Feature: 0024 Epoch: 1800 Loss: 1.0548381896 MSE_Loss: 1.0548381896 time: 0.3659s\n",
      "Feature: 0024 Epoch: 1900 Loss: 1.0875337358 MSE_Loss: 1.0875337358 time: 0.3663s\n",
      "Begin training feature: 0025\n",
      "Feature: 0025 Epoch: 0000 Loss: 4.7595546578 MSE_Loss: 4.7595546578 time: 0.3662s\n",
      "Feature: 0025 Epoch: 0100 Loss: 1.1946321175 MSE_Loss: 1.1946321175 time: 0.3662s\n",
      "Feature: 0025 Epoch: 0200 Loss: 1.2022003479 MSE_Loss: 1.2022003479 time: 0.3665s\n",
      "Feature: 0025 Epoch: 0300 Loss: 1.1803724449 MSE_Loss: 1.1803724449 time: 0.3659s\n",
      "Feature: 0025 Epoch: 0400 Loss: 1.1668322848 MSE_Loss: 1.1668322848 time: 0.3658s\n",
      "Feature: 0025 Epoch: 0500 Loss: 1.1970632054 MSE_Loss: 1.1970632054 time: 0.3657s\n",
      "Feature: 0025 Epoch: 0600 Loss: 1.1611538145 MSE_Loss: 1.1611538145 time: 0.3657s\n",
      "Feature: 0025 Epoch: 0700 Loss: 1.1523843973 MSE_Loss: 1.1523843973 time: 0.3659s\n",
      "Feature: 0025 Epoch: 0800 Loss: 1.1706705425 MSE_Loss: 1.1706705425 time: 0.3658s\n",
      "Feature: 0025 Epoch: 0900 Loss: 1.1787824435 MSE_Loss: 1.1787824435 time: 0.3659s\n",
      "Feature: 0025 Epoch: 1000 Loss: 1.1851375774 MSE_Loss: 1.1851375774 time: 0.3659s\n",
      "Feature: 0025 Epoch: 1100 Loss: 1.2070790786 MSE_Loss: 1.2070790786 time: 0.3663s\n",
      "Feature: 0025 Epoch: 1200 Loss: 1.1487259058 MSE_Loss: 1.1487259058 time: 0.3662s\n",
      "Feature: 0025 Epoch: 1300 Loss: 1.1320144768 MSE_Loss: 1.1320144768 time: 0.3663s\n",
      "Feature: 0025 Epoch: 1400 Loss: 1.1821315696 MSE_Loss: 1.1821315696 time: 0.3662s\n",
      "Feature: 0025 Epoch: 1500 Loss: 1.1435981359 MSE_Loss: 1.1435981359 time: 0.3659s\n",
      "Feature: 0025 Epoch: 1600 Loss: 1.1595063481 MSE_Loss: 1.1595063481 time: 0.3666s\n",
      "Feature: 0025 Epoch: 1700 Loss: 1.1768791706 MSE_Loss: 1.1768791706 time: 0.3665s\n",
      "Feature: 0025 Epoch: 1800 Loss: 1.1470814498 MSE_Loss: 1.1470814498 time: 0.3661s\n",
      "Feature: 0025 Epoch: 1900 Loss: 1.1790979557 MSE_Loss: 1.1790979557 time: 0.3665s\n",
      "Begin training feature: 0026\n",
      "Feature: 0026 Epoch: 0000 Loss: 5.1890082540 MSE_Loss: 5.1890082540 time: 0.3670s\n",
      "Feature: 0026 Epoch: 0100 Loss: 1.3568091536 MSE_Loss: 1.3568091536 time: 0.3669s\n",
      "Feature: 0026 Epoch: 0200 Loss: 1.2982451780 MSE_Loss: 1.2982451780 time: 0.3664s\n",
      "Feature: 0026 Epoch: 0300 Loss: 1.2677707981 MSE_Loss: 1.2677707981 time: 0.3667s\n",
      "Feature: 0026 Epoch: 0400 Loss: 1.3061198471 MSE_Loss: 1.3061198471 time: 0.3664s\n",
      "Feature: 0026 Epoch: 0500 Loss: 1.2761524410 MSE_Loss: 1.2761524410 time: 0.3667s\n",
      "Feature: 0026 Epoch: 0600 Loss: 1.2672308895 MSE_Loss: 1.2672308895 time: 0.3680s\n",
      "Feature: 0026 Epoch: 0700 Loss: 1.2665665587 MSE_Loss: 1.2665665587 time: 0.3666s\n",
      "Feature: 0026 Epoch: 0800 Loss: 1.2620544215 MSE_Loss: 1.2620544215 time: 0.3671s\n",
      "Feature: 0026 Epoch: 0900 Loss: 1.2839516477 MSE_Loss: 1.2839516477 time: 0.3680s\n",
      "Feature: 0026 Epoch: 1000 Loss: 1.2641215385 MSE_Loss: 1.2641215385 time: 0.3669s\n",
      "Feature: 0026 Epoch: 1100 Loss: 1.2483795519 MSE_Loss: 1.2483795519 time: 0.3673s\n",
      "Feature: 0026 Epoch: 1200 Loss: 1.2161375948 MSE_Loss: 1.2161375948 time: 0.3663s\n",
      "Feature: 0026 Epoch: 1300 Loss: 1.2923885063 MSE_Loss: 1.2923885063 time: 0.3663s\n",
      "Feature: 0026 Epoch: 1400 Loss: 1.2524966432 MSE_Loss: 1.2524966432 time: 0.3663s\n",
      "Feature: 0026 Epoch: 1500 Loss: 1.2239499447 MSE_Loss: 1.2239499447 time: 0.3663s\n",
      "Feature: 0026 Epoch: 1600 Loss: 1.2549884387 MSE_Loss: 1.2549884387 time: 0.3663s\n",
      "Feature: 0026 Epoch: 1700 Loss: 1.2858616906 MSE_Loss: 1.2858616906 time: 0.3656s\n",
      "Feature: 0026 Epoch: 1800 Loss: 1.2778682822 MSE_Loss: 1.2778682822 time: 0.3662s\n",
      "Feature: 0026 Epoch: 1900 Loss: 1.2520570242 MSE_Loss: 1.2520570242 time: 0.3659s\n",
      "Begin training feature: 0027\n",
      "Feature: 0027 Epoch: 0000 Loss: 5.6791424540 MSE_Loss: 5.6791424540 time: 0.3681s\n",
      "Feature: 0027 Epoch: 0100 Loss: 1.4445765324 MSE_Loss: 1.4445765324 time: 0.3664s\n",
      "Feature: 0027 Epoch: 0200 Loss: 1.3953120746 MSE_Loss: 1.3953120746 time: 0.3661s\n",
      "Feature: 0027 Epoch: 0300 Loss: 1.3849120763 MSE_Loss: 1.3849120763 time: 0.3663s\n",
      "Feature: 0027 Epoch: 0400 Loss: 1.3500855867 MSE_Loss: 1.3500855867 time: 0.3661s\n",
      "Feature: 0027 Epoch: 0500 Loss: 1.4227534548 MSE_Loss: 1.4227534548 time: 0.3665s\n",
      "Feature: 0027 Epoch: 0600 Loss: 1.3977591565 MSE_Loss: 1.3977591565 time: 0.3662s\n",
      "Feature: 0027 Epoch: 0700 Loss: 1.3766394212 MSE_Loss: 1.3766394212 time: 0.3666s\n",
      "Feature: 0027 Epoch: 0800 Loss: 1.3966670165 MSE_Loss: 1.3966670165 time: 0.3662s\n",
      "Feature: 0027 Epoch: 0900 Loss: 1.3682532741 MSE_Loss: 1.3682532741 time: 0.3668s\n",
      "Feature: 0027 Epoch: 1000 Loss: 1.3678239607 MSE_Loss: 1.3678239607 time: 0.3653s\n",
      "Feature: 0027 Epoch: 1100 Loss: 1.3407794147 MSE_Loss: 1.3407794147 time: 0.3658s\n",
      "Feature: 0027 Epoch: 1200 Loss: 1.3777514734 MSE_Loss: 1.3777514734 time: 0.3656s\n",
      "Feature: 0027 Epoch: 1300 Loss: 1.3800774609 MSE_Loss: 1.3800774609 time: 0.3670s\n",
      "Feature: 0027 Epoch: 1400 Loss: 1.3645367570 MSE_Loss: 1.3645367570 time: 0.3660s\n",
      "Feature: 0027 Epoch: 1500 Loss: 1.3672637494 MSE_Loss: 1.3672637494 time: 0.3661s\n",
      "Feature: 0027 Epoch: 1600 Loss: 1.3604799290 MSE_Loss: 1.3604799290 time: 0.3660s\n",
      "Feature: 0027 Epoch: 1700 Loss: 1.3962440298 MSE_Loss: 1.3962440298 time: 0.3658s\n",
      "Feature: 0027 Epoch: 1800 Loss: 1.3861295453 MSE_Loss: 1.3861295453 time: 0.3661s\n",
      "Feature: 0027 Epoch: 1900 Loss: 1.3611589255 MSE_Loss: 1.3611589255 time: 0.3658s\n",
      "Begin training feature: 0028\n",
      "Feature: 0028 Epoch: 0000 Loss: 4.8277802226 MSE_Loss: 4.8277802226 time: 0.3660s\n",
      "Feature: 0028 Epoch: 0100 Loss: 1.1622009632 MSE_Loss: 1.1622009632 time: 0.3659s\n",
      "Feature: 0028 Epoch: 0200 Loss: 1.1664830401 MSE_Loss: 1.1664830401 time: 0.3651s\n",
      "Feature: 0028 Epoch: 0300 Loss: 1.1264177621 MSE_Loss: 1.1264177621 time: 0.3653s\n",
      "Feature: 0028 Epoch: 0400 Loss: 1.1348469461 MSE_Loss: 1.1348469461 time: 0.3655s\n",
      "Feature: 0028 Epoch: 0500 Loss: 1.1313829094 MSE_Loss: 1.1313829094 time: 0.3664s\n",
      "Feature: 0028 Epoch: 0600 Loss: 1.1148230879 MSE_Loss: 1.1148230879 time: 0.3662s\n",
      "Feature: 0028 Epoch: 0700 Loss: 1.1288407694 MSE_Loss: 1.1288407694 time: 0.3662s\n",
      "Feature: 0028 Epoch: 0800 Loss: 1.1284119924 MSE_Loss: 1.1284119924 time: 0.3666s\n",
      "Feature: 0028 Epoch: 0900 Loss: 1.1455418196 MSE_Loss: 1.1455418196 time: 0.3659s\n",
      "Feature: 0028 Epoch: 1000 Loss: 1.1411316123 MSE_Loss: 1.1411316123 time: 0.3668s\n",
      "Feature: 0028 Epoch: 1100 Loss: 1.1321296466 MSE_Loss: 1.1321296466 time: 0.3668s\n",
      "Feature: 0028 Epoch: 1200 Loss: 1.1177225513 MSE_Loss: 1.1177225513 time: 0.3669s\n",
      "Feature: 0028 Epoch: 1300 Loss: 1.1297812567 MSE_Loss: 1.1297812567 time: 0.3668s\n",
      "Feature: 0028 Epoch: 1400 Loss: 1.1221729532 MSE_Loss: 1.1221729532 time: 0.3667s\n",
      "Feature: 0028 Epoch: 1500 Loss: 1.1131053929 MSE_Loss: 1.1131053929 time: 0.3663s\n",
      "Feature: 0028 Epoch: 1600 Loss: 1.1008549287 MSE_Loss: 1.1008549287 time: 0.3663s\n",
      "Feature: 0028 Epoch: 1700 Loss: 1.1371131479 MSE_Loss: 1.1371131479 time: 0.3658s\n",
      "Feature: 0028 Epoch: 1800 Loss: 1.1214067725 MSE_Loss: 1.1214067725 time: 0.3662s\n",
      "Feature: 0028 Epoch: 1900 Loss: 1.1150664201 MSE_Loss: 1.1150664201 time: 0.3663s\n",
      "Begin training feature: 0029\n",
      "Feature: 0029 Epoch: 0000 Loss: 4.8330564288 MSE_Loss: 4.8330564288 time: 0.3663s\n",
      "Feature: 0029 Epoch: 0100 Loss: 1.2243749220 MSE_Loss: 1.2243749220 time: 0.3664s\n",
      "Feature: 0029 Epoch: 0200 Loss: 1.2190526626 MSE_Loss: 1.2190526626 time: 0.3665s\n",
      "Feature: 0029 Epoch: 0300 Loss: 1.2363870129 MSE_Loss: 1.2363870129 time: 0.3661s\n",
      "Feature: 0029 Epoch: 0400 Loss: 1.2358986124 MSE_Loss: 1.2358986124 time: 0.3664s\n",
      "Feature: 0029 Epoch: 0500 Loss: 1.1926328615 MSE_Loss: 1.1926328615 time: 0.3660s\n",
      "Feature: 0029 Epoch: 0600 Loss: 1.1912809263 MSE_Loss: 1.1912809263 time: 0.3664s\n",
      "Feature: 0029 Epoch: 0700 Loss: 1.1780663102 MSE_Loss: 1.1780663102 time: 0.3662s\n",
      "Feature: 0029 Epoch: 0800 Loss: 1.2144381208 MSE_Loss: 1.2144381208 time: 0.3666s\n",
      "Feature: 0029 Epoch: 0900 Loss: 1.1978313402 MSE_Loss: 1.1978313402 time: 0.3665s\n",
      "Feature: 0029 Epoch: 1000 Loss: 1.1974320382 MSE_Loss: 1.1974320382 time: 0.3662s\n",
      "Feature: 0029 Epoch: 1100 Loss: 1.2105063328 MSE_Loss: 1.2105063328 time: 0.3657s\n",
      "Feature: 0029 Epoch: 1200 Loss: 1.1973746713 MSE_Loss: 1.1973746713 time: 0.3661s\n",
      "Feature: 0029 Epoch: 1300 Loss: 1.1872878656 MSE_Loss: 1.1872878656 time: 0.3663s\n",
      "Feature: 0029 Epoch: 1400 Loss: 1.2040280006 MSE_Loss: 1.2040280006 time: 0.3661s\n",
      "Feature: 0029 Epoch: 1500 Loss: 1.1703025751 MSE_Loss: 1.1703025751 time: 0.3664s\n",
      "Feature: 0029 Epoch: 1600 Loss: 1.1516824712 MSE_Loss: 1.1516824712 time: 0.3665s\n",
      "Feature: 0029 Epoch: 1700 Loss: 1.1878407356 MSE_Loss: 1.1878407356 time: 0.3665s\n",
      "Feature: 0029 Epoch: 1800 Loss: 1.1458274081 MSE_Loss: 1.1458274081 time: 0.3666s\n",
      "Feature: 0029 Epoch: 1900 Loss: 1.1641085457 MSE_Loss: 1.1641085457 time: 0.3666s\n",
      "Begin training feature: 0030\n",
      "Feature: 0030 Epoch: 0000 Loss: 4.9428041404 MSE_Loss: 4.9428041404 time: 0.3671s\n",
      "Feature: 0030 Epoch: 0100 Loss: 1.2802799291 MSE_Loss: 1.2802799291 time: 0.3665s\n",
      "Feature: 0030 Epoch: 0200 Loss: 1.2274514376 MSE_Loss: 1.2274514376 time: 0.3668s\n",
      "Feature: 0030 Epoch: 0300 Loss: 1.2359312675 MSE_Loss: 1.2359312675 time: 0.3663s\n",
      "Feature: 0030 Epoch: 0400 Loss: 1.1831408158 MSE_Loss: 1.1831408158 time: 0.3662s\n",
      "Feature: 0030 Epoch: 0500 Loss: 1.2152466985 MSE_Loss: 1.2152466985 time: 0.3662s\n",
      "Feature: 0030 Epoch: 0600 Loss: 1.1959420096 MSE_Loss: 1.1959420096 time: 0.3654s\n",
      "Feature: 0030 Epoch: 0700 Loss: 1.2074558825 MSE_Loss: 1.2074558825 time: 0.3656s\n",
      "Feature: 0030 Epoch: 0800 Loss: 1.2342588920 MSE_Loss: 1.2342588920 time: 0.3661s\n",
      "Feature: 0030 Epoch: 0900 Loss: 1.2045927712 MSE_Loss: 1.2045927712 time: 0.3662s\n",
      "Feature: 0030 Epoch: 1000 Loss: 1.2196351203 MSE_Loss: 1.2196351203 time: 0.3666s\n",
      "Feature: 0030 Epoch: 1100 Loss: 1.2184052637 MSE_Loss: 1.2184052637 time: 0.3664s\n",
      "Feature: 0030 Epoch: 1200 Loss: 1.2049497128 MSE_Loss: 1.2049497128 time: 0.3668s\n",
      "Feature: 0030 Epoch: 1300 Loss: 1.2150632332 MSE_Loss: 1.2150632332 time: 0.3663s\n",
      "Feature: 0030 Epoch: 1400 Loss: 1.2312195395 MSE_Loss: 1.2312195395 time: 0.3669s\n",
      "Feature: 0030 Epoch: 1500 Loss: 1.2081795086 MSE_Loss: 1.2081795086 time: 0.3665s\n",
      "Feature: 0030 Epoch: 1600 Loss: 1.1876705636 MSE_Loss: 1.1876705636 time: 0.3666s\n",
      "Feature: 0030 Epoch: 1700 Loss: 1.1975825263 MSE_Loss: 1.1975825263 time: 0.3662s\n",
      "Feature: 0030 Epoch: 1800 Loss: 1.1732570401 MSE_Loss: 1.1732570401 time: 0.3660s\n",
      "Feature: 0030 Epoch: 1900 Loss: 1.1659510422 MSE_Loss: 1.1659510422 time: 0.3665s\n",
      "Begin training feature: 0031\n",
      "Feature: 0031 Epoch: 0000 Loss: 5.1483367666 MSE_Loss: 5.1483367666 time: 0.3669s\n",
      "Feature: 0031 Epoch: 0100 Loss: 1.2514035238 MSE_Loss: 1.2514035238 time: 0.3663s\n",
      "Feature: 0031 Epoch: 0200 Loss: 1.2576823846 MSE_Loss: 1.2576823846 time: 0.3664s\n",
      "Feature: 0031 Epoch: 0300 Loss: 1.2429780749 MSE_Loss: 1.2429780749 time: 0.3659s\n",
      "Feature: 0031 Epoch: 0400 Loss: 1.2298000327 MSE_Loss: 1.2298000327 time: 0.3663s\n",
      "Feature: 0031 Epoch: 0500 Loss: 1.2207277742 MSE_Loss: 1.2207277742 time: 0.3665s\n",
      "Feature: 0031 Epoch: 0600 Loss: 1.2097918580 MSE_Loss: 1.2097918580 time: 0.3666s\n",
      "Feature: 0031 Epoch: 0700 Loss: 1.1945757489 MSE_Loss: 1.1945757489 time: 0.3668s\n",
      "Feature: 0031 Epoch: 0800 Loss: 1.2530891526 MSE_Loss: 1.2530891526 time: 0.3669s\n",
      "Feature: 0031 Epoch: 0900 Loss: 1.2345257477 MSE_Loss: 1.2345257477 time: 0.3667s\n",
      "Feature: 0031 Epoch: 1000 Loss: 1.2255234122 MSE_Loss: 1.2255234122 time: 0.3399s\n",
      "Feature: 0031 Epoch: 1100 Loss: 1.2250973514 MSE_Loss: 1.2250973514 time: 0.3398s\n",
      "Feature: 0031 Epoch: 1200 Loss: 1.1702290604 MSE_Loss: 1.1702290604 time: 0.3395s\n",
      "Feature: 0031 Epoch: 1300 Loss: 1.1885187498 MSE_Loss: 1.1885187498 time: 0.3391s\n",
      "Feature: 0031 Epoch: 1400 Loss: 1.1998353419 MSE_Loss: 1.1998353419 time: 0.3391s\n",
      "Feature: 0031 Epoch: 1500 Loss: 1.2074274677 MSE_Loss: 1.2074274677 time: 0.3392s\n",
      "Feature: 0031 Epoch: 1600 Loss: 1.2049334072 MSE_Loss: 1.2049334072 time: 0.3390s\n",
      "Feature: 0031 Epoch: 1700 Loss: 1.2598844148 MSE_Loss: 1.2598844148 time: 0.3392s\n",
      "Feature: 0031 Epoch: 1800 Loss: 1.1874537589 MSE_Loss: 1.1874537589 time: 0.3391s\n",
      "Feature: 0031 Epoch: 1900 Loss: 1.2132935286 MSE_Loss: 1.2132935286 time: 0.3393s\n",
      "Begin training feature: 0032\n",
      "Feature: 0032 Epoch: 0000 Loss: 5.1407886215 MSE_Loss: 5.1407886215 time: 0.3399s\n",
      "Feature: 0032 Epoch: 0100 Loss: 1.3004440499 MSE_Loss: 1.3004440499 time: 0.3396s\n",
      "Feature: 0032 Epoch: 0200 Loss: 1.2880090400 MSE_Loss: 1.2880090400 time: 0.3392s\n",
      "Feature: 0032 Epoch: 0300 Loss: 1.2754489742 MSE_Loss: 1.2754489742 time: 0.3396s\n",
      "Feature: 0032 Epoch: 0400 Loss: 1.2867827778 MSE_Loss: 1.2867827778 time: 0.3393s\n",
      "Feature: 0032 Epoch: 0500 Loss: 1.2386636432 MSE_Loss: 1.2386636432 time: 0.3394s\n",
      "Feature: 0032 Epoch: 0600 Loss: 1.2345850302 MSE_Loss: 1.2345850302 time: 0.3396s\n",
      "Feature: 0032 Epoch: 0700 Loss: 1.2731106304 MSE_Loss: 1.2731106304 time: 0.3390s\n",
      "Feature: 0032 Epoch: 0800 Loss: 1.2925307124 MSE_Loss: 1.2925307124 time: 0.3391s\n",
      "Feature: 0032 Epoch: 0900 Loss: 1.2661601720 MSE_Loss: 1.2661601720 time: 0.3390s\n",
      "Feature: 0032 Epoch: 1000 Loss: 1.2608361961 MSE_Loss: 1.2608361961 time: 0.3389s\n",
      "Feature: 0032 Epoch: 1100 Loss: 1.3034557180 MSE_Loss: 1.3034557180 time: 0.3390s\n",
      "Feature: 0032 Epoch: 1200 Loss: 1.2553946248 MSE_Loss: 1.2553946248 time: 0.3388s\n",
      "Feature: 0032 Epoch: 1300 Loss: 1.2202595880 MSE_Loss: 1.2202595880 time: 0.3390s\n",
      "Feature: 0032 Epoch: 1400 Loss: 1.2307280189 MSE_Loss: 1.2307280189 time: 0.3393s\n",
      "Feature: 0032 Epoch: 1500 Loss: 1.2310118313 MSE_Loss: 1.2310118313 time: 0.3394s\n",
      "Feature: 0032 Epoch: 1600 Loss: 1.2362445910 MSE_Loss: 1.2362445910 time: 0.3394s\n",
      "Feature: 0032 Epoch: 1700 Loss: 1.2633863620 MSE_Loss: 1.2633863620 time: 0.3396s\n",
      "Feature: 0032 Epoch: 1800 Loss: 1.2806578007 MSE_Loss: 1.2806578007 time: 0.3397s\n",
      "Feature: 0032 Epoch: 1900 Loss: 1.2205221796 MSE_Loss: 1.2205221796 time: 0.3396s\n",
      "Begin training feature: 0033\n",
      "Feature: 0033 Epoch: 0000 Loss: 4.6454015774 MSE_Loss: 4.6454015774 time: 0.3396s\n",
      "Feature: 0033 Epoch: 0100 Loss: 1.1315802373 MSE_Loss: 1.1315802373 time: 0.3396s\n",
      "Feature: 0033 Epoch: 0200 Loss: 1.0944415874 MSE_Loss: 1.0944415874 time: 0.3393s\n",
      "Feature: 0033 Epoch: 0300 Loss: 1.1010370741 MSE_Loss: 1.1010370741 time: 0.3391s\n",
      "Feature: 0033 Epoch: 0400 Loss: 1.1121135399 MSE_Loss: 1.1121135399 time: 0.3391s\n",
      "Feature: 0033 Epoch: 0500 Loss: 1.1332046405 MSE_Loss: 1.1332046405 time: 0.3391s\n",
      "Feature: 0033 Epoch: 0600 Loss: 1.0799267383 MSE_Loss: 1.0799267383 time: 0.3389s\n",
      "Feature: 0033 Epoch: 0700 Loss: 1.1173084273 MSE_Loss: 1.1173084273 time: 0.3390s\n",
      "Feature: 0033 Epoch: 0800 Loss: 1.1180484204 MSE_Loss: 1.1180484204 time: 0.3391s\n",
      "Feature: 0033 Epoch: 0900 Loss: 1.1059822772 MSE_Loss: 1.1059822772 time: 0.3392s\n",
      "Feature: 0033 Epoch: 1000 Loss: 1.0758819606 MSE_Loss: 1.0758819606 time: 0.3394s\n",
      "Feature: 0033 Epoch: 1100 Loss: 1.0875262663 MSE_Loss: 1.0875262663 time: 0.3393s\n",
      "Feature: 0033 Epoch: 1200 Loss: 1.0847411918 MSE_Loss: 1.0847411918 time: 0.3392s\n",
      "Feature: 0033 Epoch: 1300 Loss: 1.1043275142 MSE_Loss: 1.1043275142 time: 0.3394s\n",
      "Feature: 0033 Epoch: 1400 Loss: 1.0737477847 MSE_Loss: 1.0737477847 time: 0.3393s\n",
      "Feature: 0033 Epoch: 1500 Loss: 1.0809664040 MSE_Loss: 1.0809664040 time: 0.3393s\n",
      "Feature: 0033 Epoch: 1600 Loss: 1.0880046419 MSE_Loss: 1.0880046419 time: 0.3395s\n",
      "Feature: 0033 Epoch: 1700 Loss: 1.0625994334 MSE_Loss: 1.0625994334 time: 0.3392s\n",
      "Feature: 0033 Epoch: 1800 Loss: 1.0792654391 MSE_Loss: 1.0792654391 time: 0.3391s\n",
      "Feature: 0033 Epoch: 1900 Loss: 1.0813373052 MSE_Loss: 1.0813373052 time: 0.3391s\n",
      "Begin training feature: 0034\n",
      "Feature: 0034 Epoch: 0000 Loss: 4.6181185940 MSE_Loss: 4.6181185940 time: 0.3391s\n",
      "Feature: 0034 Epoch: 0100 Loss: 1.1519539537 MSE_Loss: 1.1519539537 time: 0.3390s\n",
      "Feature: 0034 Epoch: 0200 Loss: 1.1551979245 MSE_Loss: 1.1551979245 time: 0.3390s\n",
      "Feature: 0034 Epoch: 0300 Loss: 1.1505714564 MSE_Loss: 1.1505714564 time: 0.3390s\n",
      "Feature: 0034 Epoch: 0400 Loss: 1.1583620129 MSE_Loss: 1.1583620129 time: 0.3392s\n",
      "Feature: 0034 Epoch: 0500 Loss: 1.1265772604 MSE_Loss: 1.1265772604 time: 0.3395s\n",
      "Feature: 0034 Epoch: 0600 Loss: 1.1487931534 MSE_Loss: 1.1487931534 time: 0.3395s\n",
      "Feature: 0034 Epoch: 0700 Loss: 1.1351912746 MSE_Loss: 1.1351912746 time: 0.3396s\n",
      "Feature: 0034 Epoch: 0800 Loss: 1.1385681531 MSE_Loss: 1.1385681531 time: 0.3395s\n",
      "Feature: 0034 Epoch: 0900 Loss: 1.1440443442 MSE_Loss: 1.1440443442 time: 0.3396s\n",
      "Feature: 0034 Epoch: 1000 Loss: 1.1471765079 MSE_Loss: 1.1471765079 time: 0.3394s\n",
      "Feature: 0034 Epoch: 1100 Loss: 1.1025248660 MSE_Loss: 1.1025248660 time: 0.3398s\n",
      "Feature: 0034 Epoch: 1200 Loss: 1.1211470916 MSE_Loss: 1.1211470916 time: 0.3394s\n",
      "Feature: 0034 Epoch: 1300 Loss: 1.1359145189 MSE_Loss: 1.1359145189 time: 0.3391s\n",
      "Feature: 0034 Epoch: 1400 Loss: 1.1253124049 MSE_Loss: 1.1253124049 time: 0.3390s\n",
      "Feature: 0034 Epoch: 1500 Loss: 1.1083608300 MSE_Loss: 1.1083608300 time: 0.3392s\n",
      "Feature: 0034 Epoch: 1600 Loss: 1.1155691154 MSE_Loss: 1.1155691154 time: 0.3390s\n",
      "Feature: 0034 Epoch: 1700 Loss: 1.1092341305 MSE_Loss: 1.1092341305 time: 0.3391s\n",
      "Feature: 0034 Epoch: 1800 Loss: 1.1211622901 MSE_Loss: 1.1211622901 time: 0.3390s\n",
      "Feature: 0034 Epoch: 1900 Loss: 1.1316265532 MSE_Loss: 1.1316265532 time: 0.3392s\n",
      "Begin training feature: 0035\n",
      "Feature: 0035 Epoch: 0000 Loss: 4.8549161953 MSE_Loss: 4.8549161953 time: 0.3397s\n",
      "Feature: 0035 Epoch: 0100 Loss: 1.2207555137 MSE_Loss: 1.2207555137 time: 0.3384s\n",
      "Feature: 0035 Epoch: 0200 Loss: 1.1904339602 MSE_Loss: 1.1904339602 time: 0.3383s\n",
      "Feature: 0035 Epoch: 0300 Loss: 1.1633821522 MSE_Loss: 1.1633821522 time: 0.3383s\n",
      "Feature: 0035 Epoch: 0400 Loss: 1.1503986973 MSE_Loss: 1.1503986973 time: 0.3383s\n",
      "Feature: 0035 Epoch: 0500 Loss: 1.2304012866 MSE_Loss: 1.2304012866 time: 0.3385s\n",
      "Feature: 0035 Epoch: 0600 Loss: 1.1801300901 MSE_Loss: 1.1801300901 time: 0.3384s\n",
      "Feature: 0035 Epoch: 0700 Loss: 1.2035627237 MSE_Loss: 1.2035627237 time: 0.3394s\n",
      "Feature: 0035 Epoch: 0800 Loss: 1.1605446912 MSE_Loss: 1.1605446912 time: 0.3379s\n",
      "Feature: 0035 Epoch: 0900 Loss: 1.1829198101 MSE_Loss: 1.1829198101 time: 0.3377s\n",
      "Feature: 0035 Epoch: 1000 Loss: 1.1494537962 MSE_Loss: 1.1494537962 time: 0.3380s\n",
      "Feature: 0035 Epoch: 1100 Loss: 1.1727276784 MSE_Loss: 1.1727276784 time: 0.3379s\n",
      "Feature: 0035 Epoch: 1200 Loss: 1.1477831270 MSE_Loss: 1.1477831270 time: 0.3377s\n",
      "Feature: 0035 Epoch: 1300 Loss: 1.1970776968 MSE_Loss: 1.1970776968 time: 0.3379s\n",
      "Feature: 0035 Epoch: 1400 Loss: 1.1675433788 MSE_Loss: 1.1675433788 time: 0.3379s\n",
      "Feature: 0035 Epoch: 1500 Loss: 1.1495965757 MSE_Loss: 1.1495965757 time: 0.3380s\n",
      "Feature: 0035 Epoch: 1600 Loss: 1.1659525316 MSE_Loss: 1.1659525316 time: 0.3384s\n",
      "Feature: 0035 Epoch: 1700 Loss: 1.1804411419 MSE_Loss: 1.1804411419 time: 0.3383s\n",
      "Feature: 0035 Epoch: 1800 Loss: 1.1485218617 MSE_Loss: 1.1485218617 time: 0.3385s\n",
      "Feature: 0035 Epoch: 1900 Loss: 1.1890095910 MSE_Loss: 1.1890095910 time: 0.3382s\n",
      "Begin training feature: 0036\n",
      "Feature: 0036 Epoch: 0000 Loss: 5.1647093809 MSE_Loss: 5.1647093809 time: 0.3382s\n",
      "Feature: 0036 Epoch: 0100 Loss: 1.2975599883 MSE_Loss: 1.2975599883 time: 0.3385s\n",
      "Feature: 0036 Epoch: 0200 Loss: 1.2962535386 MSE_Loss: 1.2962535386 time: 0.3381s\n",
      "Feature: 0036 Epoch: 0300 Loss: 1.2351373547 MSE_Loss: 1.2351373547 time: 0.3380s\n",
      "Feature: 0036 Epoch: 0400 Loss: 1.2779997112 MSE_Loss: 1.2779997112 time: 0.3379s\n",
      "Feature: 0036 Epoch: 0500 Loss: 1.2510625565 MSE_Loss: 1.2510625565 time: 0.3378s\n",
      "Feature: 0036 Epoch: 0600 Loss: 1.2823848238 MSE_Loss: 1.2823848238 time: 0.3378s\n",
      "Feature: 0036 Epoch: 0700 Loss: 1.3019745282 MSE_Loss: 1.3019745282 time: 0.3377s\n",
      "Feature: 0036 Epoch: 0800 Loss: 1.2335171503 MSE_Loss: 1.2335171503 time: 0.3376s\n",
      "Feature: 0036 Epoch: 0900 Loss: 1.2253612408 MSE_Loss: 1.2253612408 time: 0.3377s\n",
      "Feature: 0036 Epoch: 1000 Loss: 1.2464280826 MSE_Loss: 1.2464280826 time: 0.3380s\n",
      "Feature: 0036 Epoch: 1100 Loss: 1.2270595786 MSE_Loss: 1.2270595786 time: 0.3382s\n",
      "Feature: 0036 Epoch: 1200 Loss: 1.2095752121 MSE_Loss: 1.2095752121 time: 0.3383s\n",
      "Feature: 0036 Epoch: 1300 Loss: 1.2169443105 MSE_Loss: 1.2169443105 time: 0.3385s\n",
      "Feature: 0036 Epoch: 1400 Loss: 1.2316378838 MSE_Loss: 1.2316378838 time: 0.3384s\n",
      "Feature: 0036 Epoch: 1500 Loss: 1.2453485177 MSE_Loss: 1.2453485177 time: 0.3386s\n",
      "Feature: 0036 Epoch: 1600 Loss: 1.2052382782 MSE_Loss: 1.2052382782 time: 0.3382s\n",
      "Feature: 0036 Epoch: 1700 Loss: 1.2422060966 MSE_Loss: 1.2422060966 time: 0.3381s\n",
      "Feature: 0036 Epoch: 1800 Loss: 1.2427097940 MSE_Loss: 1.2427097940 time: 0.3381s\n",
      "Feature: 0036 Epoch: 1900 Loss: 1.2254263857 MSE_Loss: 1.2254263857 time: 0.3379s\n",
      "Begin training feature: 0037\n",
      "Feature: 0037 Epoch: 0000 Loss: 5.6379492404 MSE_Loss: 5.6379492404 time: 0.3380s\n",
      "Feature: 0037 Epoch: 0100 Loss: 1.4028614111 MSE_Loss: 1.4028614111 time: 0.3389s\n",
      "Feature: 0037 Epoch: 0200 Loss: 1.4167714557 MSE_Loss: 1.4167714557 time: 0.3387s\n",
      "Feature: 0037 Epoch: 0300 Loss: 1.4105476066 MSE_Loss: 1.4105476066 time: 0.3388s\n",
      "Feature: 0037 Epoch: 0400 Loss: 1.3476271214 MSE_Loss: 1.3476271214 time: 0.3389s\n",
      "Feature: 0037 Epoch: 0500 Loss: 1.3870741740 MSE_Loss: 1.3870741740 time: 0.3391s\n",
      "Feature: 0037 Epoch: 0600 Loss: 1.3511095790 MSE_Loss: 1.3511095790 time: 0.3392s\n",
      "Feature: 0037 Epoch: 0700 Loss: 1.3682346895 MSE_Loss: 1.3682346895 time: 0.3394s\n",
      "Feature: 0037 Epoch: 0800 Loss: 1.3888082942 MSE_Loss: 1.3888082942 time: 0.3392s\n",
      "Feature: 0037 Epoch: 0900 Loss: 1.3940159528 MSE_Loss: 1.3940159528 time: 0.3392s\n",
      "Feature: 0037 Epoch: 1000 Loss: 1.3694031027 MSE_Loss: 1.3694031027 time: 0.3391s\n",
      "Feature: 0037 Epoch: 1100 Loss: 1.3809474680 MSE_Loss: 1.3809474680 time: 0.3395s\n",
      "Feature: 0037 Epoch: 1200 Loss: 1.3907147676 MSE_Loss: 1.3907147676 time: 0.3395s\n",
      "Feature: 0037 Epoch: 1300 Loss: 1.3607650113 MSE_Loss: 1.3607650113 time: 0.3392s\n",
      "Feature: 0037 Epoch: 1400 Loss: 1.3713672901 MSE_Loss: 1.3713672901 time: 0.3390s\n",
      "Feature: 0037 Epoch: 1500 Loss: 1.3860105842 MSE_Loss: 1.3860105842 time: 0.3389s\n",
      "Feature: 0037 Epoch: 1600 Loss: 1.3558865654 MSE_Loss: 1.3558865654 time: 0.3389s\n",
      "Feature: 0037 Epoch: 1700 Loss: 1.3514839701 MSE_Loss: 1.3514839701 time: 0.3389s\n",
      "Feature: 0037 Epoch: 1800 Loss: 1.3414813159 MSE_Loss: 1.3414813159 time: 0.3387s\n",
      "Feature: 0037 Epoch: 1900 Loss: 1.3602147517 MSE_Loss: 1.3602147517 time: 0.3389s\n",
      "Begin training feature: 0038\n",
      "Feature: 0038 Epoch: 0000 Loss: 4.2507207877 MSE_Loss: 4.2507207877 time: 0.3390s\n",
      "Feature: 0038 Epoch: 0100 Loss: 1.0524123620 MSE_Loss: 1.0524123620 time: 0.3393s\n",
      "Feature: 0038 Epoch: 0200 Loss: 1.0505348663 MSE_Loss: 1.0505348663 time: 0.3394s\n",
      "Feature: 0038 Epoch: 0300 Loss: 1.0294252178 MSE_Loss: 1.0294252178 time: 0.3393s\n",
      "Feature: 0038 Epoch: 0400 Loss: 1.0590802031 MSE_Loss: 1.0590802031 time: 0.3397s\n",
      "Feature: 0038 Epoch: 0500 Loss: 1.0527444686 MSE_Loss: 1.0527444686 time: 0.3392s\n",
      "Feature: 0038 Epoch: 0600 Loss: 1.0632210819 MSE_Loss: 1.0632210819 time: 0.3392s\n",
      "Feature: 0038 Epoch: 0700 Loss: 1.0384125740 MSE_Loss: 1.0384125740 time: 0.3399s\n",
      "Feature: 0038 Epoch: 0800 Loss: 1.0407327459 MSE_Loss: 1.0407327459 time: 0.3394s\n",
      "Feature: 0038 Epoch: 0900 Loss: 1.0508704261 MSE_Loss: 1.0508704261 time: 0.3393s\n",
      "Feature: 0038 Epoch: 1000 Loss: 1.0362933882 MSE_Loss: 1.0362933882 time: 0.3392s\n",
      "Feature: 0038 Epoch: 1100 Loss: 1.0270706438 MSE_Loss: 1.0270706438 time: 0.3389s\n",
      "Feature: 0038 Epoch: 1200 Loss: 1.0175969672 MSE_Loss: 1.0175969672 time: 0.3388s\n",
      "Feature: 0038 Epoch: 1300 Loss: 1.0107524900 MSE_Loss: 1.0107524900 time: 0.3389s\n",
      "Feature: 0038 Epoch: 1400 Loss: 1.0010679153 MSE_Loss: 1.0010679153 time: 0.3390s\n",
      "Feature: 0038 Epoch: 1500 Loss: 1.0184859958 MSE_Loss: 1.0184859958 time: 0.3390s\n",
      "Feature: 0038 Epoch: 1600 Loss: 1.0002080991 MSE_Loss: 1.0002080991 time: 0.3393s\n",
      "Feature: 0038 Epoch: 1700 Loss: 1.0084608435 MSE_Loss: 1.0084608435 time: 0.3390s\n",
      "Feature: 0038 Epoch: 1800 Loss: 1.0576417733 MSE_Loss: 1.0576417733 time: 0.3395s\n",
      "Feature: 0038 Epoch: 1900 Loss: 1.0064811031 MSE_Loss: 1.0064811031 time: 0.3394s\n",
      "Begin training feature: 0039\n",
      "Feature: 0039 Epoch: 0000 Loss: 4.8398672056 MSE_Loss: 4.8398672056 time: 0.3398s\n",
      "Feature: 0039 Epoch: 0100 Loss: 1.2142313787 MSE_Loss: 1.2142313787 time: 0.3397s\n",
      "Feature: 0039 Epoch: 0200 Loss: 1.2022761039 MSE_Loss: 1.2022761039 time: 0.3398s\n",
      "Feature: 0039 Epoch: 0300 Loss: 1.2009422772 MSE_Loss: 1.2009422772 time: 0.3394s\n",
      "Feature: 0039 Epoch: 0400 Loss: 1.1873844794 MSE_Loss: 1.1873844794 time: 0.3394s\n",
      "Feature: 0039 Epoch: 0500 Loss: 1.2092487887 MSE_Loss: 1.2092487887 time: 0.3393s\n",
      "Feature: 0039 Epoch: 0600 Loss: 1.2008875069 MSE_Loss: 1.2008875069 time: 0.3391s\n",
      "Feature: 0039 Epoch: 0700 Loss: 1.1732507175 MSE_Loss: 1.1732507175 time: 0.3390s\n",
      "Feature: 0039 Epoch: 0800 Loss: 1.1760342944 MSE_Loss: 1.1760342944 time: 0.3390s\n",
      "Feature: 0039 Epoch: 0900 Loss: 1.1774337654 MSE_Loss: 1.1774337654 time: 0.3389s\n",
      "Feature: 0039 Epoch: 1000 Loss: 1.2130477493 MSE_Loss: 1.2130477493 time: 0.3389s\n",
      "Feature: 0039 Epoch: 1100 Loss: 1.1811404636 MSE_Loss: 1.1811404636 time: 0.3392s\n",
      "Feature: 0039 Epoch: 1200 Loss: 1.1413706149 MSE_Loss: 1.1413706149 time: 0.3394s\n",
      "Feature: 0039 Epoch: 1300 Loss: 1.1571908831 MSE_Loss: 1.1571908831 time: 0.3394s\n",
      "Feature: 0039 Epoch: 1400 Loss: 1.1706079086 MSE_Loss: 1.1706079086 time: 0.3393s\n",
      "Feature: 0039 Epoch: 1500 Loss: 1.1772517932 MSE_Loss: 1.1772517932 time: 0.3397s\n",
      "Feature: 0039 Epoch: 1600 Loss: 1.1464896964 MSE_Loss: 1.1464896964 time: 0.3395s\n",
      "Feature: 0039 Epoch: 1700 Loss: 1.1812705667 MSE_Loss: 1.1812705667 time: 0.3397s\n",
      "Feature: 0039 Epoch: 1800 Loss: 1.1822878635 MSE_Loss: 1.1822878635 time: 0.3395s\n",
      "Feature: 0039 Epoch: 1900 Loss: 1.1455598798 MSE_Loss: 1.1455598798 time: 0.3395s\n",
      "Begin training feature: 0040\n",
      "Feature: 0040 Epoch: 0000 Loss: 5.1207335599 MSE_Loss: 5.1207335599 time: 0.3392s\n",
      "Feature: 0040 Epoch: 0100 Loss: 1.3082391861 MSE_Loss: 1.3082391861 time: 0.3390s\n",
      "Feature: 0040 Epoch: 0200 Loss: 1.2960673201 MSE_Loss: 1.2960673201 time: 0.3391s\n",
      "Feature: 0040 Epoch: 0300 Loss: 1.2997624897 MSE_Loss: 1.2997624897 time: 0.3390s\n",
      "Feature: 0040 Epoch: 0400 Loss: 1.2640451034 MSE_Loss: 1.2640451034 time: 0.3390s\n",
      "Feature: 0040 Epoch: 0500 Loss: 1.2495393414 MSE_Loss: 1.2495393414 time: 0.3391s\n",
      "Feature: 0040 Epoch: 0600 Loss: 1.2437450086 MSE_Loss: 1.2437450086 time: 0.3391s\n",
      "Feature: 0040 Epoch: 0700 Loss: 1.2960760028 MSE_Loss: 1.2960760028 time: 0.3392s\n",
      "Feature: 0040 Epoch: 0800 Loss: 1.2673800422 MSE_Loss: 1.2673800422 time: 0.3394s\n",
      "Feature: 0040 Epoch: 0900 Loss: 1.2669131824 MSE_Loss: 1.2669131824 time: 0.3395s\n",
      "Feature: 0040 Epoch: 1000 Loss: 1.2829885098 MSE_Loss: 1.2829885098 time: 0.3396s\n",
      "Feature: 0040 Epoch: 1100 Loss: 1.2421062619 MSE_Loss: 1.2421062619 time: 0.3396s\n",
      "Feature: 0040 Epoch: 1200 Loss: 1.2766425059 MSE_Loss: 1.2766425059 time: 0.3395s\n",
      "Feature: 0040 Epoch: 1300 Loss: 1.2229029212 MSE_Loss: 1.2229029212 time: 0.3396s\n",
      "Feature: 0040 Epoch: 1400 Loss: 1.2386449689 MSE_Loss: 1.2386449689 time: 0.3395s\n",
      "Feature: 0040 Epoch: 1500 Loss: 1.2392901069 MSE_Loss: 1.2392901069 time: 0.3394s\n",
      "Feature: 0040 Epoch: 1600 Loss: 1.2458661018 MSE_Loss: 1.2458661018 time: 0.3392s\n",
      "Feature: 0040 Epoch: 1700 Loss: 1.2277722276 MSE_Loss: 1.2277722276 time: 0.3391s\n",
      "Feature: 0040 Epoch: 1800 Loss: 1.2836076595 MSE_Loss: 1.2836076595 time: 0.3389s\n",
      "Feature: 0040 Epoch: 1900 Loss: 1.2200452150 MSE_Loss: 1.2200452150 time: 0.3389s\n",
      "Begin training feature: 0041\n",
      "Feature: 0041 Epoch: 0000 Loss: 4.9267156335 MSE_Loss: 4.9267156335 time: 0.3391s\n",
      "Feature: 0041 Epoch: 0100 Loss: 1.2139667974 MSE_Loss: 1.2139667974 time: 0.3391s\n",
      "Feature: 0041 Epoch: 0200 Loss: 1.2119388309 MSE_Loss: 1.2119388309 time: 0.3389s\n",
      "Feature: 0041 Epoch: 0300 Loss: 1.2053903236 MSE_Loss: 1.2053903236 time: 0.3397s\n",
      "Feature: 0041 Epoch: 0400 Loss: 1.1763447274 MSE_Loss: 1.1763447274 time: 0.3396s\n",
      "Feature: 0041 Epoch: 0500 Loss: 1.1921121878 MSE_Loss: 1.1921121878 time: 0.3393s\n",
      "Feature: 0041 Epoch: 0600 Loss: 1.2406262484 MSE_Loss: 1.2406262484 time: 0.3393s\n",
      "Feature: 0041 Epoch: 0700 Loss: 1.1856232135 MSE_Loss: 1.1856232135 time: 0.3392s\n",
      "Feature: 0041 Epoch: 0800 Loss: 1.1762205890 MSE_Loss: 1.1762205890 time: 0.3397s\n",
      "Feature: 0041 Epoch: 0900 Loss: 1.1301591551 MSE_Loss: 1.1301591551 time: 0.3396s\n",
      "Feature: 0041 Epoch: 1000 Loss: 1.1855597360 MSE_Loss: 1.1855597360 time: 0.3395s\n",
      "Feature: 0041 Epoch: 1100 Loss: 1.1898408932 MSE_Loss: 1.1898408932 time: 0.3393s\n",
      "Feature: 0041 Epoch: 1200 Loss: 1.1886160268 MSE_Loss: 1.1886160268 time: 0.3392s\n",
      "Feature: 0041 Epoch: 1300 Loss: 1.1718863204 MSE_Loss: 1.1718863204 time: 0.3390s\n",
      "Feature: 0041 Epoch: 1400 Loss: 1.1755056830 MSE_Loss: 1.1755056830 time: 0.3391s\n",
      "Feature: 0041 Epoch: 1500 Loss: 1.1635974804 MSE_Loss: 1.1635974804 time: 0.3392s\n",
      "Feature: 0041 Epoch: 1600 Loss: 1.2195053550 MSE_Loss: 1.2195053550 time: 0.3392s\n",
      "Feature: 0041 Epoch: 1700 Loss: 1.1600976579 MSE_Loss: 1.1600976579 time: 0.3392s\n",
      "Feature: 0041 Epoch: 1800 Loss: 1.1946235639 MSE_Loss: 1.1946235639 time: 0.3390s\n",
      "Feature: 0041 Epoch: 1900 Loss: 1.1631284939 MSE_Loss: 1.1631284939 time: 0.3394s\n",
      "Begin training feature: 0042\n",
      "Feature: 0042 Epoch: 0000 Loss: 5.4784332197 MSE_Loss: 5.4784332197 time: 0.3399s\n",
      "Feature: 0042 Epoch: 0100 Loss: 1.3452687226 MSE_Loss: 1.3452687226 time: 0.3395s\n",
      "Feature: 0042 Epoch: 0200 Loss: 1.3526568918 MSE_Loss: 1.3526568918 time: 0.3393s\n",
      "Feature: 0042 Epoch: 0300 Loss: 1.3193942482 MSE_Loss: 1.3193942482 time: 0.3397s\n",
      "Feature: 0042 Epoch: 0400 Loss: 1.3101748485 MSE_Loss: 1.3101748485 time: 0.3396s\n",
      "Feature: 0042 Epoch: 0500 Loss: 1.2983527795 MSE_Loss: 1.2983527795 time: 0.3394s\n",
      "Feature: 0042 Epoch: 0600 Loss: 1.3010132622 MSE_Loss: 1.3010132622 time: 0.3392s\n",
      "Feature: 0042 Epoch: 0700 Loss: 1.3365252101 MSE_Loss: 1.3365252101 time: 0.3393s\n",
      "Feature: 0042 Epoch: 0800 Loss: 1.3193950419 MSE_Loss: 1.3193950419 time: 0.3390s\n",
      "Feature: 0042 Epoch: 0900 Loss: 1.3194575212 MSE_Loss: 1.3194575212 time: 0.3390s\n",
      "Feature: 0042 Epoch: 1000 Loss: 1.2984359219 MSE_Loss: 1.2984359219 time: 0.3391s\n",
      "Feature: 0042 Epoch: 1100 Loss: 1.3014925484 MSE_Loss: 1.3014925484 time: 0.3389s\n",
      "Feature: 0042 Epoch: 1200 Loss: 1.3063764097 MSE_Loss: 1.3063764097 time: 0.3390s\n",
      "Feature: 0042 Epoch: 1300 Loss: 1.3286942704 MSE_Loss: 1.3286942704 time: 0.3391s\n",
      "Feature: 0042 Epoch: 1400 Loss: 1.2986321012 MSE_Loss: 1.2986321012 time: 0.3393s\n",
      "Feature: 0042 Epoch: 1500 Loss: 1.2884715086 MSE_Loss: 1.2884715086 time: 0.3394s\n",
      "Feature: 0042 Epoch: 1600 Loss: 1.2783249956 MSE_Loss: 1.2783249956 time: 0.3395s\n",
      "Feature: 0042 Epoch: 1700 Loss: 1.2632553683 MSE_Loss: 1.2632553683 time: 0.3395s\n",
      "Feature: 0042 Epoch: 1800 Loss: 1.3182888348 MSE_Loss: 1.3182888348 time: 0.3395s\n",
      "Feature: 0042 Epoch: 1900 Loss: 1.3180726631 MSE_Loss: 1.3180726631 time: 0.3395s\n",
      "Begin training feature: 0043\n",
      "Feature: 0043 Epoch: 0000 Loss: 4.3812389313 MSE_Loss: 4.3812389313 time: 0.3401s\n",
      "Feature: 0043 Epoch: 0100 Loss: 1.0563641243 MSE_Loss: 1.0563641243 time: 0.3394s\n",
      "Feature: 0043 Epoch: 0200 Loss: 1.0340946920 MSE_Loss: 1.0340946920 time: 0.3390s\n",
      "Feature: 0043 Epoch: 0300 Loss: 1.0678599386 MSE_Loss: 1.0678599386 time: 0.3391s\n",
      "Feature: 0043 Epoch: 0400 Loss: 1.0420616141 MSE_Loss: 1.0420616141 time: 0.3390s\n",
      "Feature: 0043 Epoch: 0500 Loss: 1.0567711313 MSE_Loss: 1.0567711313 time: 0.3389s\n",
      "Feature: 0043 Epoch: 0600 Loss: 1.0412953941 MSE_Loss: 1.0412953941 time: 0.3387s\n",
      "Feature: 0043 Epoch: 0700 Loss: 1.0447663680 MSE_Loss: 1.0447663680 time: 0.3391s\n",
      "Feature: 0043 Epoch: 0800 Loss: 1.0624017904 MSE_Loss: 1.0624017904 time: 0.3395s\n",
      "Feature: 0043 Epoch: 0900 Loss: 1.0746137213 MSE_Loss: 1.0746137213 time: 0.3395s\n",
      "Feature: 0043 Epoch: 1000 Loss: 1.0534973054 MSE_Loss: 1.0534973054 time: 0.3393s\n",
      "Feature: 0043 Epoch: 1100 Loss: 1.0680463390 MSE_Loss: 1.0680463390 time: 0.3394s\n",
      "Feature: 0043 Epoch: 1200 Loss: 1.0285660572 MSE_Loss: 1.0285660572 time: 0.3395s\n",
      "Feature: 0043 Epoch: 1300 Loss: 1.0458864804 MSE_Loss: 1.0458864804 time: 0.3394s\n",
      "Feature: 0043 Epoch: 1400 Loss: 1.0591095663 MSE_Loss: 1.0591095663 time: 0.3394s\n",
      "Feature: 0043 Epoch: 1500 Loss: 1.0616677094 MSE_Loss: 1.0616677094 time: 0.3396s\n",
      "Feature: 0043 Epoch: 1600 Loss: 1.0420221724 MSE_Loss: 1.0420221724 time: 0.3393s\n",
      "Feature: 0043 Epoch: 1700 Loss: 1.0553414784 MSE_Loss: 1.0553414784 time: 0.3392s\n",
      "Feature: 0043 Epoch: 1800 Loss: 1.0419387516 MSE_Loss: 1.0419387516 time: 0.3390s\n",
      "Feature: 0043 Epoch: 1900 Loss: 1.0130645882 MSE_Loss: 1.0130645882 time: 0.3389s\n",
      "Begin training feature: 0044\n",
      "Feature: 0044 Epoch: 0000 Loss: 4.6943811405 MSE_Loss: 4.6943811405 time: 0.3389s\n",
      "Feature: 0044 Epoch: 0100 Loss: 1.2086764974 MSE_Loss: 1.2086764974 time: 0.3388s\n",
      "Feature: 0044 Epoch: 0200 Loss: 1.2052655929 MSE_Loss: 1.2052655929 time: 0.3389s\n",
      "Feature: 0044 Epoch: 0300 Loss: 1.2052054390 MSE_Loss: 1.2052054390 time: 0.3391s\n",
      "Feature: 0044 Epoch: 0400 Loss: 1.1906477568 MSE_Loss: 1.1906477568 time: 0.3392s\n",
      "Feature: 0044 Epoch: 0500 Loss: 1.1508556589 MSE_Loss: 1.1508556589 time: 0.3394s\n",
      "Feature: 0044 Epoch: 0600 Loss: 1.1783422005 MSE_Loss: 1.1783422005 time: 0.3392s\n",
      "Feature: 0044 Epoch: 0700 Loss: 1.1644253618 MSE_Loss: 1.1644253618 time: 0.3393s\n",
      "Feature: 0044 Epoch: 0800 Loss: 1.1517386504 MSE_Loss: 1.1517386504 time: 0.3395s\n",
      "Feature: 0044 Epoch: 0900 Loss: 1.1432992212 MSE_Loss: 1.1432992212 time: 0.3393s\n",
      "Feature: 0044 Epoch: 1000 Loss: 1.1421069050 MSE_Loss: 1.1421069050 time: 0.3394s\n",
      "Feature: 0044 Epoch: 1100 Loss: 1.1481258462 MSE_Loss: 1.1481258462 time: 0.3392s\n",
      "Feature: 0044 Epoch: 1200 Loss: 1.1409622793 MSE_Loss: 1.1409622793 time: 0.3390s\n",
      "Feature: 0044 Epoch: 1300 Loss: 1.1681341868 MSE_Loss: 1.1681341868 time: 0.3390s\n",
      "Feature: 0044 Epoch: 1400 Loss: 1.1430709875 MSE_Loss: 1.1430709875 time: 0.3390s\n",
      "Feature: 0044 Epoch: 1500 Loss: 1.1146389540 MSE_Loss: 1.1146389540 time: 0.3387s\n",
      "Feature: 0044 Epoch: 1600 Loss: 1.1159512650 MSE_Loss: 1.1159512650 time: 0.3388s\n",
      "Feature: 0044 Epoch: 1700 Loss: 1.1768877740 MSE_Loss: 1.1768877740 time: 0.3387s\n",
      "Feature: 0044 Epoch: 1800 Loss: 1.1363726319 MSE_Loss: 1.1363726319 time: 0.3662s\n",
      "Feature: 0044 Epoch: 1900 Loss: 1.1455592309 MSE_Loss: 1.1455592309 time: 0.3663s\n",
      "Begin training feature: 0045\n",
      "Feature: 0045 Epoch: 0000 Loss: 5.3289415021 MSE_Loss: 5.3289415021 time: 0.3661s\n",
      "Feature: 0045 Epoch: 0100 Loss: 1.3788072233 MSE_Loss: 1.3788072233 time: 0.3662s\n",
      "Feature: 0045 Epoch: 0200 Loss: 1.2889597937 MSE_Loss: 1.2889597937 time: 0.3662s\n",
      "Feature: 0045 Epoch: 0300 Loss: 1.3138933612 MSE_Loss: 1.3138933612 time: 0.3663s\n",
      "Feature: 0045 Epoch: 0400 Loss: 1.2917605325 MSE_Loss: 1.2917605325 time: 0.3663s\n",
      "Feature: 0045 Epoch: 0500 Loss: 1.3077463455 MSE_Loss: 1.3077463455 time: 0.3662s\n",
      "Feature: 0045 Epoch: 0600 Loss: 1.2873837254 MSE_Loss: 1.2873837254 time: 0.3660s\n",
      "Feature: 0045 Epoch: 0700 Loss: 1.3075503458 MSE_Loss: 1.3075503458 time: 0.3661s\n",
      "Feature: 0045 Epoch: 0800 Loss: 1.3169211255 MSE_Loss: 1.3169211255 time: 0.3658s\n",
      "Feature: 0045 Epoch: 0900 Loss: 1.2565582168 MSE_Loss: 1.2565582168 time: 0.3658s\n",
      "Feature: 0045 Epoch: 1000 Loss: 1.2856471584 MSE_Loss: 1.2856471584 time: 0.3659s\n",
      "Feature: 0045 Epoch: 1100 Loss: 1.3068342737 MSE_Loss: 1.3068342737 time: 0.3658s\n",
      "Feature: 0045 Epoch: 1200 Loss: 1.3064517733 MSE_Loss: 1.3064517733 time: 0.3656s\n",
      "Feature: 0045 Epoch: 1300 Loss: 1.2994274429 MSE_Loss: 1.2994274429 time: 0.3657s\n",
      "Feature: 0045 Epoch: 1400 Loss: 1.2919626583 MSE_Loss: 1.2919626583 time: 0.3652s\n",
      "Feature: 0045 Epoch: 1500 Loss: 1.2950996470 MSE_Loss: 1.2950996470 time: 0.3653s\n",
      "Feature: 0045 Epoch: 1600 Loss: 1.2789922583 MSE_Loss: 1.2789922583 time: 0.3656s\n",
      "Feature: 0045 Epoch: 1700 Loss: 1.2962312698 MSE_Loss: 1.2962312698 time: 0.3657s\n",
      "Feature: 0045 Epoch: 1800 Loss: 1.2756969767 MSE_Loss: 1.2756969767 time: 0.3666s\n",
      "Feature: 0045 Epoch: 1900 Loss: 1.2798506369 MSE_Loss: 1.2798506369 time: 0.3657s\n",
      "Begin training feature: 0046\n",
      "Feature: 0046 Epoch: 0000 Loss: 4.9916165992 MSE_Loss: 4.9916165992 time: 0.3659s\n",
      "Feature: 0046 Epoch: 0100 Loss: 1.1995537817 MSE_Loss: 1.1995537817 time: 0.3658s\n",
      "Feature: 0046 Epoch: 0200 Loss: 1.2315644267 MSE_Loss: 1.2315644267 time: 0.3653s\n",
      "Feature: 0046 Epoch: 0300 Loss: 1.2125435057 MSE_Loss: 1.2125435057 time: 0.3653s\n",
      "Feature: 0046 Epoch: 0400 Loss: 1.2030559144 MSE_Loss: 1.2030559144 time: 0.3655s\n",
      "Feature: 0046 Epoch: 0500 Loss: 1.2112829730 MSE_Loss: 1.2112829730 time: 0.3651s\n",
      "Feature: 0046 Epoch: 0600 Loss: 1.1981493046 MSE_Loss: 1.1981493046 time: 0.3650s\n",
      "Feature: 0046 Epoch: 0700 Loss: 1.2165099201 MSE_Loss: 1.2165099201 time: 0.3655s\n",
      "Feature: 0046 Epoch: 0800 Loss: 1.2072470860 MSE_Loss: 1.2072470860 time: 0.3661s\n",
      "Feature: 0046 Epoch: 0900 Loss: 1.1788727520 MSE_Loss: 1.1788727520 time: 0.3659s\n",
      "Feature: 0046 Epoch: 1000 Loss: 1.1808480288 MSE_Loss: 1.1808480288 time: 0.3656s\n",
      "Feature: 0046 Epoch: 1100 Loss: 1.1591065224 MSE_Loss: 1.1591065224 time: 0.3660s\n",
      "Feature: 0046 Epoch: 1200 Loss: 1.1756817495 MSE_Loss: 1.1756817495 time: 0.3658s\n",
      "Feature: 0046 Epoch: 1300 Loss: 1.1891956065 MSE_Loss: 1.1891956065 time: 0.3655s\n",
      "Feature: 0046 Epoch: 1400 Loss: 1.1746870800 MSE_Loss: 1.1746870800 time: 0.3653s\n",
      "Feature: 0046 Epoch: 1500 Loss: 1.1944986235 MSE_Loss: 1.1944986235 time: 0.3659s\n",
      "Feature: 0046 Epoch: 1600 Loss: 1.1760137639 MSE_Loss: 1.1760137639 time: 0.3648s\n",
      "Feature: 0046 Epoch: 1700 Loss: 1.2006087839 MSE_Loss: 1.2006087839 time: 0.3652s\n",
      "Feature: 0046 Epoch: 1800 Loss: 1.2310012473 MSE_Loss: 1.2310012473 time: 0.3653s\n",
      "Feature: 0046 Epoch: 1900 Loss: 1.1659159879 MSE_Loss: 1.1659159879 time: 0.3653s\n",
      "Begin training feature: 0047\n",
      "Feature: 0047 Epoch: 0000 Loss: 5.1503509630 MSE_Loss: 5.1503509630 time: 0.3659s\n",
      "Feature: 0047 Epoch: 0100 Loss: 1.2836275968 MSE_Loss: 1.2836275968 time: 0.3651s\n",
      "Feature: 0047 Epoch: 0200 Loss: 1.2685606495 MSE_Loss: 1.2685606495 time: 0.3655s\n",
      "Feature: 0047 Epoch: 0300 Loss: 1.2827854277 MSE_Loss: 1.2827854277 time: 0.3652s\n",
      "Feature: 0047 Epoch: 0400 Loss: 1.2901343899 MSE_Loss: 1.2901343899 time: 0.3656s\n",
      "Feature: 0047 Epoch: 0500 Loss: 1.2827677402 MSE_Loss: 1.2827677402 time: 0.3657s\n",
      "Feature: 0047 Epoch: 0600 Loss: 1.3007504299 MSE_Loss: 1.3007504299 time: 0.3650s\n",
      "Feature: 0047 Epoch: 0700 Loss: 1.2385493532 MSE_Loss: 1.2385493532 time: 0.3658s\n",
      "Feature: 0047 Epoch: 0800 Loss: 1.2503043134 MSE_Loss: 1.2503043134 time: 0.3656s\n",
      "Feature: 0047 Epoch: 0900 Loss: 1.2635308225 MSE_Loss: 1.2635308225 time: 0.3653s\n",
      "Feature: 0047 Epoch: 1000 Loss: 1.2899357918 MSE_Loss: 1.2899357918 time: 0.3657s\n",
      "Feature: 0047 Epoch: 1100 Loss: 1.2428023845 MSE_Loss: 1.2428023845 time: 0.3655s\n",
      "Feature: 0047 Epoch: 1200 Loss: 1.2447952572 MSE_Loss: 1.2447952572 time: 0.3653s\n",
      "Feature: 0047 Epoch: 1300 Loss: 1.2412942814 MSE_Loss: 1.2412942814 time: 0.3652s\n",
      "Feature: 0047 Epoch: 1400 Loss: 1.2398372889 MSE_Loss: 1.2398372889 time: 0.3656s\n",
      "Feature: 0047 Epoch: 1500 Loss: 1.2336232779 MSE_Loss: 1.2336232779 time: 0.3654s\n",
      "Feature: 0047 Epoch: 1600 Loss: 1.2229054583 MSE_Loss: 1.2229054583 time: 0.3658s\n",
      "Feature: 0047 Epoch: 1700 Loss: 1.1844981354 MSE_Loss: 1.1844981354 time: 0.3656s\n",
      "Feature: 0047 Epoch: 1800 Loss: 1.2609530310 MSE_Loss: 1.2609530310 time: 0.3659s\n",
      "Feature: 0047 Epoch: 1900 Loss: 1.2079793502 MSE_Loss: 1.2079793502 time: 0.3659s\n",
      "Begin training feature: 0048\n",
      "Feature: 0048 Epoch: 0000 Loss: 4.3224628786 MSE_Loss: 4.3224628786 time: 0.3661s\n",
      "Feature: 0048 Epoch: 0100 Loss: 1.0915496380 MSE_Loss: 1.0915496380 time: 0.3655s\n",
      "Feature: 0048 Epoch: 0200 Loss: 1.0716771476 MSE_Loss: 1.0716771476 time: 0.3656s\n",
      "Feature: 0048 Epoch: 0300 Loss: 1.0625997103 MSE_Loss: 1.0625997103 time: 0.3657s\n",
      "Feature: 0048 Epoch: 0400 Loss: 1.0442365717 MSE_Loss: 1.0442365717 time: 0.3652s\n",
      "Feature: 0048 Epoch: 0500 Loss: 1.0553200539 MSE_Loss: 1.0553200539 time: 0.3654s\n",
      "Feature: 0048 Epoch: 0600 Loss: 1.0540707360 MSE_Loss: 1.0540707360 time: 0.3653s\n",
      "Feature: 0048 Epoch: 0700 Loss: 1.0462713219 MSE_Loss: 1.0462713219 time: 0.3654s\n",
      "Feature: 0048 Epoch: 0800 Loss: 1.0128813778 MSE_Loss: 1.0128813778 time: 0.3652s\n",
      "Feature: 0048 Epoch: 0900 Loss: 1.0543498420 MSE_Loss: 1.0543498420 time: 0.3656s\n",
      "Feature: 0048 Epoch: 1000 Loss: 1.0150089339 MSE_Loss: 1.0150089339 time: 0.3655s\n",
      "Feature: 0048 Epoch: 1100 Loss: 1.0232237162 MSE_Loss: 1.0232237162 time: 0.3662s\n",
      "Feature: 0048 Epoch: 1200 Loss: 1.0182393057 MSE_Loss: 1.0182393057 time: 0.3655s\n",
      "Feature: 0048 Epoch: 1300 Loss: 1.0652899576 MSE_Loss: 1.0652899576 time: 0.3656s\n",
      "Feature: 0048 Epoch: 1400 Loss: 1.0208424715 MSE_Loss: 1.0208424715 time: 0.3658s\n",
      "Feature: 0048 Epoch: 1500 Loss: 1.0364107226 MSE_Loss: 1.0364107226 time: 0.3657s\n",
      "Feature: 0048 Epoch: 1600 Loss: 1.0258051572 MSE_Loss: 1.0258051572 time: 0.3664s\n",
      "Feature: 0048 Epoch: 1700 Loss: 1.0428541408 MSE_Loss: 1.0428541408 time: 0.3661s\n",
      "Feature: 0048 Epoch: 1800 Loss: 1.0267107958 MSE_Loss: 1.0267107958 time: 0.3653s\n",
      "Feature: 0048 Epoch: 1900 Loss: 1.0207762288 MSE_Loss: 1.0207762288 time: 0.3658s\n",
      "Begin training feature: 0049\n",
      "Feature: 0049 Epoch: 0000 Loss: 4.3785180170 MSE_Loss: 4.3785180170 time: 0.3663s\n",
      "Feature: 0049 Epoch: 0100 Loss: 1.0952798536 MSE_Loss: 1.0952798536 time: 0.3658s\n",
      "Feature: 0049 Epoch: 0200 Loss: 1.0983835542 MSE_Loss: 1.0983835542 time: 0.3657s\n",
      "Feature: 0049 Epoch: 0300 Loss: 1.0981869773 MSE_Loss: 1.0981869773 time: 0.3659s\n",
      "Feature: 0049 Epoch: 0400 Loss: 1.0709301905 MSE_Loss: 1.0709301905 time: 0.3662s\n",
      "Feature: 0049 Epoch: 0500 Loss: 1.0532400925 MSE_Loss: 1.0532400925 time: 0.3661s\n",
      "Feature: 0049 Epoch: 0600 Loss: 1.0711131413 MSE_Loss: 1.0711131413 time: 0.3664s\n",
      "Feature: 0049 Epoch: 0700 Loss: 1.0415428688 MSE_Loss: 1.0415428688 time: 0.3663s\n",
      "Feature: 0049 Epoch: 0800 Loss: 1.0941740531 MSE_Loss: 1.0941740531 time: 0.3663s\n",
      "Feature: 0049 Epoch: 0900 Loss: 1.1054037259 MSE_Loss: 1.1054037259 time: 0.3660s\n",
      "Feature: 0049 Epoch: 1000 Loss: 1.0798509743 MSE_Loss: 1.0798509743 time: 0.3661s\n",
      "Feature: 0049 Epoch: 1100 Loss: 1.0643376738 MSE_Loss: 1.0643376738 time: 0.3662s\n",
      "Feature: 0049 Epoch: 1200 Loss: 1.1002060077 MSE_Loss: 1.1002060077 time: 0.3655s\n",
      "Feature: 0049 Epoch: 1300 Loss: 1.0705336272 MSE_Loss: 1.0705336272 time: 0.3661s\n",
      "Feature: 0049 Epoch: 1400 Loss: 1.0479852259 MSE_Loss: 1.0479852259 time: 0.3655s\n",
      "Feature: 0049 Epoch: 1500 Loss: 1.0737932143 MSE_Loss: 1.0737932143 time: 0.3661s\n",
      "Feature: 0049 Epoch: 1600 Loss: 1.0867085736 MSE_Loss: 1.0867085736 time: 0.3658s\n",
      "Feature: 0049 Epoch: 1700 Loss: 1.1058498100 MSE_Loss: 1.1058498100 time: 0.3660s\n",
      "Feature: 0049 Epoch: 1800 Loss: 1.0707618443 MSE_Loss: 1.0707618443 time: 0.3658s\n",
      "Feature: 0049 Epoch: 1900 Loss: 1.1050863002 MSE_Loss: 1.1050863002 time: 0.3662s\n",
      "Begin training feature: 0050\n",
      "Feature: 0050 Epoch: 0000 Loss: 4.8093479193 MSE_Loss: 4.8093479193 time: 0.3668s\n",
      "Feature: 0050 Epoch: 0100 Loss: 1.2137173316 MSE_Loss: 1.2137173316 time: 0.3663s\n",
      "Feature: 0050 Epoch: 0200 Loss: 1.2638947805 MSE_Loss: 1.2638947805 time: 0.3662s\n",
      "Feature: 0050 Epoch: 0300 Loss: 1.2196327282 MSE_Loss: 1.2196327282 time: 0.3664s\n",
      "Feature: 0050 Epoch: 0400 Loss: 1.1922795410 MSE_Loss: 1.1922795410 time: 0.3659s\n",
      "Feature: 0050 Epoch: 0500 Loss: 1.2102716105 MSE_Loss: 1.2102716105 time: 0.3664s\n",
      "Feature: 0050 Epoch: 0600 Loss: 1.2518601931 MSE_Loss: 1.2518601931 time: 0.3661s\n",
      "Feature: 0050 Epoch: 0700 Loss: 1.1684905170 MSE_Loss: 1.1684905170 time: 0.3660s\n",
      "Feature: 0050 Epoch: 0800 Loss: 1.1895561550 MSE_Loss: 1.1895561550 time: 0.3662s\n",
      "Feature: 0050 Epoch: 0900 Loss: 1.2186283711 MSE_Loss: 1.2186283711 time: 0.3660s\n",
      "Feature: 0050 Epoch: 1000 Loss: 1.1724775781 MSE_Loss: 1.1724775781 time: 0.3660s\n",
      "Feature: 0050 Epoch: 1100 Loss: 1.1844931637 MSE_Loss: 1.1844931637 time: 0.3657s\n",
      "Feature: 0050 Epoch: 1200 Loss: 1.2281353459 MSE_Loss: 1.2281353459 time: 0.3659s\n",
      "Feature: 0050 Epoch: 1300 Loss: 1.2084877687 MSE_Loss: 1.2084877687 time: 0.3664s\n",
      "Feature: 0050 Epoch: 1400 Loss: 1.1936933225 MSE_Loss: 1.1936933225 time: 0.3666s\n",
      "Feature: 0050 Epoch: 1500 Loss: 1.2000963711 MSE_Loss: 1.2000963711 time: 0.3661s\n",
      "Feature: 0050 Epoch: 1600 Loss: 1.1637659978 MSE_Loss: 1.1637659978 time: 0.3659s\n",
      "Feature: 0050 Epoch: 1700 Loss: 1.1993678974 MSE_Loss: 1.1993678974 time: 0.3669s\n",
      "Feature: 0050 Epoch: 1800 Loss: 1.1679035727 MSE_Loss: 1.1679035727 time: 0.3657s\n",
      "Feature: 0050 Epoch: 1900 Loss: 1.1646726358 MSE_Loss: 1.1646726358 time: 0.3663s\n"
     ]
    }
   ],
   "source": [
    "for idx in range(50):\n",
    "    print('Begin training feature: {:04d}'.format(idx + 1))\n",
    "    decoder_file = 'decoder' + str(idx) + '.pt'\n",
    "    decoder_file = os.path.join('/home/jing_xuzijian/crf/Intrer_VAE_result/fmri_sim4', decoder_file)\n",
    "    Inter_decoder = decoder(args.dims, args.decoder_hidden, args.time_step - 1, args.num_nodes, args.decoder_dropout, args.decoder_alpha)\n",
    "    Inter_decoder = Inter_decoder.cuda()\n",
    "    optimizer = optim.Adam(params = Inter_decoder.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
    "    scheduler = lr_scheduler.StepLR(optimizer, step_size=1000, gamma=0.5)\n",
    "    loss_val = nn.MSELoss()\n",
    "    best_loss = np.Inf\n",
    "    for epoch in range(2000):\n",
    "        scheduler.step()\n",
    "        t = time.time()\n",
    "        Loss = []\n",
    "        mse_loss = []\n",
    "        for batch_idx, data in enumerate(data_loader):\n",
    "            data = data.cuda()\n",
    "            target = data[:, idx, 1:, :]\n",
    "            optimizer.zero_grad()\n",
    "            inputs = data[:, :, :-1, :]\n",
    "            pred = Inter_decoder(inputs, idx)\n",
    "            mse = loss_val(pred, target)\n",
    "            loss = mse\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "            Loss.append(loss.item())\n",
    "            mse_loss.append(mse.item())\n",
    "\n",
    "        if epoch % 100 == 0:\n",
    "            print('Feature: {:04d}'.format(idx + 1),\n",
    "                'Epoch: {:04d}'.format(epoch),\n",
    "                'Loss: {:.10f}'.format(np.mean(Loss)),\n",
    "                'MSE_Loss: {:.10f}'.format(np.mean(mse_loss)),\n",
    "                'time: {:.4f}s'.format(time.time() - t))\n",
    "\n",
    "            \n",
    "        if np.mean(mse_loss) < best_loss:\n",
    "            best_loss = np.mean(mse_loss)\n",
    "            torch.save(Inter_decoder.state_dict(), decoder_file)\n",
    "            # print('Feature: {:04d}'.format(idx + 1),\n",
    "            #       'Epoch: {:04d}'.format(epoch),\n",
    "            #       'Loss: {:.10f}'.format(np.mean(Loss)),\n",
    "            #       'mse_loss: {:.10f}'.format(np.mean(mse_loss)),\n",
    "            #       'mmd_loss: {:.10f}'.format(np.mean(mmd_loss)),\n",
    "            #       'time: {:.4f}s'.format(time.time() - t), file=log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/omnisky/Public/ChenRongfa/Intrer_VAE/Model.py:20: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n",
      "  nn.init.xavier_normal(m.weight.data, gain=1.414)\n"
     ]
    }
   ],
   "source": [
    "adj = []\n",
    "for idx in range(50):\n",
    "    decoder_file = 'decoder' + str(idx) + '.pt'\n",
    "    decoder_file = os.path.join('/home/omnisky/Public/ChenRongfa/fmri_sim4', decoder_file)\n",
    "    decoder_net = decoder(args.dims, args.decoder_hidden, args.time_step - 1, args.num_nodes, args.decoder_dropout, args.decoder_alpha)\n",
    "    decoder_net.load_state_dict(torch.load(decoder_file))\n",
    "    adj.append(decoder_net.adj[idx, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 50])\n"
     ]
    }
   ],
   "source": [
    "init_adj = torch.cat([temp.unsqueeze(0) for temp in adj], dim=0)\n",
    "init_adj = init_adj.clone().detach()\n",
    "print(init_adj.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.9756, 'precision': 1.0, 'recall': 0.45045045045045046, 'F1': 0.6211180124223602, 'ROC_AUC': 0.9934911889704691, 'PR_AUC': 0.8701276465472852}\n"
     ]
    }
   ],
   "source": [
    "result, _ = evaluate_result(GC, init_adj.detach().numpy(), 0.5)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin training feature: 0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jing_xuzijian/crf/Intrer_VAE/Model.py:20: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n",
      "  nn.init.xavier_normal(m.weight.data, gain=1.414)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature: 0001 Epoch: 0000 Loss: 1.8476178299 MSE_Loss: 1.3537910249 Sparsity_loss: 8.2673076919 KL_loss: 0.0762382840 MMD_loss: 0.0398495171 time: 0.8757s\n",
      "Feature: 0001 Epoch: 0100 Loss: 1.2310018343 MSE_Loss: 1.1195286663 Sparsity_loss: 0.7552484304 KL_loss: 0.0488326616 MMD_loss: 0.0366112141 time: 0.8209s\n",
      "Feature: 0001 Epoch: 0200 Loss: 1.2309947301 MSE_Loss: 1.1195175399 Sparsity_loss: 0.7553394475 KL_loss: 0.0488469109 MMD_loss: 0.0366108716 time: 0.8232s\n",
      "Feature: 0001 Epoch: 0300 Loss: 1.2309896584 MSE_Loss: 1.1195117839 Sparsity_loss: 0.7553491215 KL_loss: 0.0488573655 MMD_loss: 0.0366109157 time: 0.8209s\n",
      "Feature: 0001 Epoch: 0400 Loss: 1.2309890382 MSE_Loss: 1.1195665387 Sparsity_loss: 0.7542707573 KL_loss: 0.0486605840 MMD_loss: 0.0366111749 time: 0.8247s\n",
      "Feature: 0001 Epoch: 0500 Loss: 1.2309903510 MSE_Loss: 1.1195101512 Sparsity_loss: 0.7553975069 KL_loss: 0.0488720748 MMD_loss: 0.0366108014 time: 0.8216s\n",
      "Feature: 0001 Epoch: 0600 Loss: 1.2309904559 MSE_Loss: 1.1195167394 Sparsity_loss: 0.7552802133 KL_loss: 0.0488453654 MMD_loss: 0.0366106279 time: 0.8226s\n",
      "Feature: 0001 Epoch: 0700 Loss: 1.2244057203 MSE_Loss: 1.1221115348 Sparsity_loss: 0.5717166459 KL_loss: 0.0453747146 MMD_loss: 0.0366273047 time: 0.8209s\n",
      "Feature: 0001 Epoch: 0800 Loss: 1.2244056048 MSE_Loss: 1.1221114246 Sparsity_loss: 0.5717225120 KL_loss: 0.0453716758 MMD_loss: 0.0366271674 time: 0.8215s\n",
      "Feature: 0001 Epoch: 0900 Loss: 1.2244079867 MSE_Loss: 1.1221132316 Sparsity_loss: 0.5717338573 KL_loss: 0.0453735824 MMD_loss: 0.0366271610 time: 0.8198s\n",
      "Feature: 0001 Epoch: 1000 Loss: 1.2244107610 MSE_Loss: 1.1221160572 Sparsity_loss: 0.5717302214 KL_loss: 0.0453732657 MMD_loss: 0.0366272311 time: 0.8206s\n",
      "Feature: 0001 Epoch: 1100 Loss: 1.2244140702 MSE_Loss: 1.1221185817 Sparsity_loss: 0.5717485185 KL_loss: 0.0453702476 MMD_loss: 0.0366271813 time: 0.8215s\n",
      "Feature: 0001 Epoch: 1200 Loss: 1.2244110620 MSE_Loss: 1.1221142977 Sparsity_loss: 0.5717743182 KL_loss: 0.0453744177 MMD_loss: 0.0366271542 time: 0.8211s\n",
      "Feature: 0001 Epoch: 1300 Loss: 1.2244086228 MSE_Loss: 1.1221153796 Sparsity_loss: 0.5717079119 KL_loss: 0.0453687063 MMD_loss: 0.0366270848 time: 0.8195s\n",
      "Feature: 0001 Epoch: 1400 Loss: 1.2244058161 MSE_Loss: 1.1221120305 Sparsity_loss: 0.5717152772 KL_loss: 0.0453725228 MMD_loss: 0.0366271448 time: 0.8186s\n",
      "Feature: 0001 Epoch: 1500 Loss: 1.2244084477 MSE_Loss: 1.1221136624 Sparsity_loss: 0.5717286452 KL_loss: 0.0453730308 MMD_loss: 0.0366273119 time: 0.8199s\n",
      "Feature: 0001 Epoch: 1600 Loss: 1.2244076578 MSE_Loss: 1.1221126997 Sparsity_loss: 0.5717341228 KL_loss: 0.0453730391 MMD_loss: 0.0366272643 time: 0.8184s\n",
      "Feature: 0001 Epoch: 1700 Loss: 1.2244097440 MSE_Loss: 1.1221150748 Sparsity_loss: 0.5717307593 KL_loss: 0.0453736243 MMD_loss: 0.0366272021 time: 0.8207s\n",
      "Feature: 0001 Epoch: 1800 Loss: 1.2244102246 MSE_Loss: 1.1221167143 Sparsity_loss: 0.5717051716 KL_loss: 0.0453730354 MMD_loss: 0.0366272579 time: 0.8148s\n",
      "Feature: 0001 Epoch: 1900 Loss: 1.2244076125 MSE_Loss: 1.1221132460 Sparsity_loss: 0.5717256009 KL_loss: 0.0453721795 MMD_loss: 0.0366271840 time: 0.8148s\n",
      "Begin training feature: 0002\n",
      "Feature: 0002 Epoch: 0000 Loss: 2.1317197009 MSE_Loss: 1.6251493164 Sparsity_loss: 8.4130821107 KL_loss: 0.0834605678 MMD_loss: 0.0425408390 time: 0.8141s\n",
      "Feature: 0002 Epoch: 0100 Loss: 1.3896281930 MSE_Loss: 1.2240936417 Sparsity_loss: 1.8176710319 KL_loss: 0.0665028637 MMD_loss: 0.0369929841 time: 0.8136s\n",
      "Feature: 0002 Epoch: 0200 Loss: 1.3896345760 MSE_Loss: 1.2241013556 Sparsity_loss: 1.8176387684 KL_loss: 0.0664924473 MMD_loss: 0.0369931725 time: 0.8138s\n",
      "Feature: 0002 Epoch: 0300 Loss: 1.3896315965 MSE_Loss: 1.2241011850 Sparsity_loss: 1.8175833497 KL_loss: 0.0664798481 MMD_loss: 0.0369932248 time: 0.8145s\n",
      "Feature: 0002 Epoch: 0400 Loss: 1.3896445519 MSE_Loss: 1.2241153249 Sparsity_loss: 1.8175646595 KL_loss: 0.0664777700 MMD_loss: 0.0369931031 time: 0.8149s\n",
      "Feature: 0002 Epoch: 0500 Loss: 1.3896281621 MSE_Loss: 1.2240969003 Sparsity_loss: 1.8176058923 KL_loss: 0.0664916001 MMD_loss: 0.0369930258 time: 0.8231s\n",
      "Feature: 0002 Epoch: 0600 Loss: 1.3896288676 MSE_Loss: 1.2240982493 Sparsity_loss: 1.8175865759 KL_loss: 0.0664876458 MMD_loss: 0.0369931988 time: 0.8204s\n",
      "Feature: 0002 Epoch: 0700 Loss: 1.3896362525 MSE_Loss: 1.2241083082 Sparsity_loss: 1.8175413322 KL_loss: 0.0664765271 MMD_loss: 0.0369930650 time: 0.8195s\n",
      "Feature: 0002 Epoch: 0800 Loss: 1.3896253283 MSE_Loss: 1.2240962590 Sparsity_loss: 1.8175656464 KL_loss: 0.0664926685 MMD_loss: 0.0369929305 time: 0.8200s\n",
      "Feature: 0002 Epoch: 0900 Loss: 1.3896306715 MSE_Loss: 1.2241003083 Sparsity_loss: 1.8175905113 KL_loss: 0.0664936253 MMD_loss: 0.0369929477 time: 0.8212s\n",
      "Feature: 0002 Epoch: 1000 Loss: 1.3896282685 MSE_Loss: 1.2240988582 Sparsity_loss: 1.8175698051 KL_loss: 0.0664862837 MMD_loss: 0.0369930284 time: 0.8210s\n",
      "Feature: 0002 Epoch: 1100 Loss: 1.3896271156 MSE_Loss: 1.2240965170 Sparsity_loss: 1.8175903815 KL_loss: 0.0664962073 MMD_loss: 0.0369930554 time: 0.8215s\n",
      "Feature: 0002 Epoch: 1200 Loss: 1.3896291248 MSE_Loss: 1.2240970572 Sparsity_loss: 1.8176138054 KL_loss: 0.0664978154 MMD_loss: 0.0369931980 time: 0.8209s\n",
      "Feature: 0002 Epoch: 1300 Loss: 1.3896273661 MSE_Loss: 1.2240959738 Sparsity_loss: 1.8176085889 KL_loss: 0.0664986051 MMD_loss: 0.0369929903 time: 0.8209s\n",
      "Feature: 0002 Epoch: 1400 Loss: 1.3896286291 MSE_Loss: 1.2240982108 Sparsity_loss: 1.8175889269 KL_loss: 0.0664916934 MMD_loss: 0.0369930305 time: 0.8199s\n",
      "Feature: 0002 Epoch: 1500 Loss: 1.3896268840 MSE_Loss: 1.2240964333 Sparsity_loss: 1.8175829619 KL_loss: 0.0664896804 MMD_loss: 0.0369932076 time: 0.8212s\n",
      "Feature: 0002 Epoch: 1600 Loss: 1.3896296092 MSE_Loss: 1.2240984214 Sparsity_loss: 1.8175976382 KL_loss: 0.0664949376 MMD_loss: 0.0369931803 time: 0.8201s\n",
      "Feature: 0002 Epoch: 1700 Loss: 1.3896270877 MSE_Loss: 1.2240975492 Sparsity_loss: 1.8175668973 KL_loss: 0.0664902165 MMD_loss: 0.0369931450 time: 0.8219s\n",
      "Feature: 0002 Epoch: 1800 Loss: 1.3896262374 MSE_Loss: 1.2240969984 Sparsity_loss: 1.8175655362 KL_loss: 0.0664898961 MMD_loss: 0.0369930298 time: 0.8221s\n",
      "Feature: 0002 Epoch: 1900 Loss: 1.3896265626 MSE_Loss: 1.2240985368 Sparsity_loss: 1.8175410741 KL_loss: 0.0664841053 MMD_loss: 0.0369930682 time: 0.8220s\n",
      "Begin training feature: 0003\n",
      "Feature: 0003 Epoch: 0000 Loss: 2.1271384004 MSE_Loss: 1.6355789953 Sparsity_loss: 8.1599805265 KL_loss: 0.0748600057 MMD_loss: 0.0414058892 time: 0.8192s\n",
      "Feature: 0003 Epoch: 0100 Loss: 1.2063281264 MSE_Loss: 1.0728926500 Sparsity_loss: 1.3064873083 KL_loss: 0.0594187239 MMD_loss: 0.0337584636 time: 0.8195s\n",
      "Feature: 0003 Epoch: 0200 Loss: 1.2063282562 MSE_Loss: 1.0728653873 Sparsity_loss: 1.3069714625 KL_loss: 0.0595344261 MMD_loss: 0.0337594744 time: 0.8210s\n",
      "Feature: 0003 Epoch: 0300 Loss: 1.2063100911 MSE_Loss: 1.0729668276 Sparsity_loss: 1.3047538407 KL_loss: 0.0590994622 MMD_loss: 0.0337572892 time: 0.8220s\n",
      "Feature: 0003 Epoch: 0400 Loss: 1.1996522727 MSE_Loss: 1.0753160504 Sparsity_loss: 1.1234839295 KL_loss: 0.0560799741 MMD_loss: 0.0338006101 time: 0.8209s\n",
      "Feature: 0003 Epoch: 0500 Loss: 1.1996543792 MSE_Loss: 1.0753199933 Sparsity_loss: 1.1234459077 KL_loss: 0.0560750591 MMD_loss: 0.0338006676 time: 0.8200s\n",
      "Feature: 0003 Epoch: 0600 Loss: 1.1996588232 MSE_Loss: 1.0753236934 Sparsity_loss: 1.1234576944 KL_loss: 0.0560682088 MMD_loss: 0.0338007825 time: 0.8211s\n",
      "Feature: 0003 Epoch: 0700 Loss: 1.1996524598 MSE_Loss: 1.0753182724 Sparsity_loss: 1.1234406942 KL_loss: 0.0560733517 MMD_loss: 0.0338007129 time: 0.8193s\n",
      "Feature: 0003 Epoch: 0800 Loss: 1.1996525790 MSE_Loss: 1.0753218222 Sparsity_loss: 1.1233818169 KL_loss: 0.0560633072 MMD_loss: 0.0338005212 time: 0.8209s\n",
      "Feature: 0003 Epoch: 0900 Loss: 1.1996516359 MSE_Loss: 1.0753195135 Sparsity_loss: 1.1234141196 KL_loss: 0.0560693150 MMD_loss: 0.0338003649 time: 0.8225s\n",
      "Feature: 0003 Epoch: 1000 Loss: 1.1996511417 MSE_Loss: 1.0753171399 Sparsity_loss: 1.1234448801 KL_loss: 0.0560744274 MMD_loss: 0.0338005061 time: 0.8222s\n",
      "Feature: 0003 Epoch: 1100 Loss: 1.1996473293 MSE_Loss: 1.0753194154 Sparsity_loss: 1.1233294433 KL_loss: 0.0560545989 MMD_loss: 0.0338004488 time: 0.8213s\n",
      "Feature: 0003 Epoch: 1200 Loss: 1.1996499511 MSE_Loss: 1.0753195369 Sparsity_loss: 1.1233786873 KL_loss: 0.0560668539 MMD_loss: 0.0338004016 time: 0.8210s\n",
      "Feature: 0003 Epoch: 1300 Loss: 1.1996466042 MSE_Loss: 1.0753148734 Sparsity_loss: 1.1233989332 KL_loss: 0.0560719879 MMD_loss: 0.0338005333 time: 0.8210s\n",
      "Feature: 0003 Epoch: 1400 Loss: 1.1996580249 MSE_Loss: 1.0753183312 Sparsity_loss: 1.1235467890 KL_loss: 0.0560864706 MMD_loss: 0.0338007425 time: 0.8212s\n",
      "Feature: 0003 Epoch: 1500 Loss: 1.1996536964 MSE_Loss: 1.0753231728 Sparsity_loss: 1.1233806233 KL_loss: 0.0560627277 MMD_loss: 0.0338004363 time: 0.8218s\n",
      "Feature: 0003 Epoch: 1600 Loss: 1.1996480732 MSE_Loss: 1.0753155268 Sparsity_loss: 1.1234093497 KL_loss: 0.0560773596 MMD_loss: 0.0338006474 time: 0.8215s\n",
      "Feature: 0003 Epoch: 1700 Loss: 1.1996458950 MSE_Loss: 1.0753116811 Sparsity_loss: 1.1234398024 KL_loss: 0.0560765149 MMD_loss: 0.0338007298 time: 0.8222s\n",
      "Feature: 0003 Epoch: 1800 Loss: 1.1996525383 MSE_Loss: 1.0753228936 Sparsity_loss: 1.1233569912 KL_loss: 0.0560586711 MMD_loss: 0.0338006055 time: 0.8203s\n",
      "Feature: 0003 Epoch: 1900 Loss: 1.1996489680 MSE_Loss: 1.0753167641 Sparsity_loss: 1.1234067890 KL_loss: 0.0560747381 MMD_loss: 0.0338005565 time: 0.8202s\n",
      "Begin training feature: 0004\n",
      "Feature: 0004 Epoch: 0000 Loss: 1.8245578461 MSE_Loss: 1.3323317691 Sparsity_loss: 8.3293950165 KL_loss: 0.0747866573 MMD_loss: 0.0375042339 time: 0.8206s\n",
      "Feature: 0004 Epoch: 0100 Loss: 1.1693242026 MSE_Loss: 1.0501877816 Sparsity_loss: 1.0296004845 KL_loss: 0.0525368232 MMD_loss: 0.0335655175 time: 0.8139s\n",
      "Feature: 0004 Epoch: 0200 Loss: 1.1693522357 MSE_Loss: 1.0501588409 Sparsity_loss: 1.0307500604 KL_loss: 0.0525256109 MMD_loss: 0.0335653155 time: 0.8142s\n",
      "Feature: 0004 Epoch: 0300 Loss: 1.1692706226 MSE_Loss: 1.0501499365 Sparsity_loss: 1.0293048907 KL_loss: 0.0525329453 MMD_loss: 0.0335650553 time: 0.8145s\n",
      "Feature: 0004 Epoch: 0400 Loss: 1.1692837265 MSE_Loss: 1.0501706517 Sparsity_loss: 1.0291785409 KL_loss: 0.0525608591 MMD_loss: 0.0335642769 time: 0.8129s\n",
      "Feature: 0004 Epoch: 0500 Loss: 1.1692631026 MSE_Loss: 1.0501395962 Sparsity_loss: 1.0293527630 KL_loss: 0.0525586195 MMD_loss: 0.0335651444 time: 0.8139s\n",
      "Feature: 0004 Epoch: 0600 Loss: 1.1825525300 MSE_Loss: 1.0646896294 Sparsity_loss: 1.0007010472 KL_loss: 0.0520789251 MMD_loss: 0.0336535267 time: 0.8138s\n",
      "Feature: 0004 Epoch: 0700 Loss: 1.1825466835 MSE_Loss: 1.0646841518 Sparsity_loss: 1.0006981379 KL_loss: 0.0520828928 MMD_loss: 0.0336533973 time: 0.8142s\n",
      "Feature: 0004 Epoch: 0800 Loss: 1.1825494208 MSE_Loss: 1.0646869080 Sparsity_loss: 1.0006945360 KL_loss: 0.0520846208 MMD_loss: 0.0336534724 time: 0.8153s\n",
      "Feature: 0004 Epoch: 0900 Loss: 1.1825494261 MSE_Loss: 1.0646872536 Sparsity_loss: 1.0006823193 KL_loss: 0.0520830641 MMD_loss: 0.0336536116 time: 0.8146s\n",
      "Feature: 0004 Epoch: 1000 Loss: 1.1825481110 MSE_Loss: 1.0646862320 Sparsity_loss: 1.0006842568 KL_loss: 0.0520799948 MMD_loss: 0.0336534371 time: 0.8144s\n",
      "Feature: 0004 Epoch: 1100 Loss: 1.1825523799 MSE_Loss: 1.0646906797 Sparsity_loss: 1.0006775177 KL_loss: 0.0520758477 MMD_loss: 0.0336535388 time: 0.8137s\n",
      "Feature: 0004 Epoch: 1200 Loss: 1.1825476357 MSE_Loss: 1.0646855439 Sparsity_loss: 1.0006801207 KL_loss: 0.0520796711 MMD_loss: 0.0336536431 time: 0.8132s\n",
      "Feature: 0004 Epoch: 1300 Loss: 1.1825475112 MSE_Loss: 1.0646841609 Sparsity_loss: 1.0007131085 KL_loss: 0.0520781274 MMD_loss: 0.0336534549 time: 0.8134s\n",
      "Feature: 0004 Epoch: 1400 Loss: 1.1825474478 MSE_Loss: 1.0646864425 Sparsity_loss: 1.0006688018 KL_loss: 0.0520794553 MMD_loss: 0.0336533826 time: 0.8140s\n",
      "Feature: 0004 Epoch: 1500 Loss: 1.1825483849 MSE_Loss: 1.0646861867 Sparsity_loss: 1.0006893828 KL_loss: 0.0520822070 MMD_loss: 0.0336534541 time: 0.8142s\n",
      "Feature: 0004 Epoch: 1600 Loss: 1.1825445709 MSE_Loss: 1.0646825161 Sparsity_loss: 1.0006838961 KL_loss: 0.0520859983 MMD_loss: 0.0336534979 time: 0.8150s\n",
      "Feature: 0004 Epoch: 1700 Loss: 1.1825473324 MSE_Loss: 1.0646856389 Sparsity_loss: 1.0006822136 KL_loss: 0.0520804311 MMD_loss: 0.0336533907 time: 0.8133s\n",
      "Feature: 0004 Epoch: 1800 Loss: 1.1825497196 MSE_Loss: 1.0646878353 Sparsity_loss: 1.0006801358 KL_loss: 0.0520818677 MMD_loss: 0.0336535311 time: 0.8138s\n",
      "Feature: 0004 Epoch: 1900 Loss: 1.1825479593 MSE_Loss: 1.0646829590 Sparsity_loss: 1.0007418530 KL_loss: 0.0520832662 MMD_loss: 0.0336535403 time: 0.8134s\n",
      "Begin training feature: 0005\n",
      "Feature: 0005 Epoch: 0000 Loss: 2.2250100812 MSE_Loss: 1.7102049567 Sparsity_loss: 8.3809873605 KL_loss: 0.0831709428 MMD_loss: 0.0474620199 time: 0.8145s\n",
      "Feature: 0005 Epoch: 0100 Loss: 1.3425270500 MSE_Loss: 1.2182334782 Sparsity_loss: 0.8688892843 KL_loss: 0.0497859392 MMD_loss: 0.0401756290 time: 0.8152s\n",
      "Feature: 0005 Epoch: 0200 Loss: 1.3520357473 MSE_Loss: 1.2293348765 Sparsity_loss: 0.8373988625 KL_loss: 0.0492119229 MMD_loss: 0.0401694026 time: 0.8143s\n",
      "Feature: 0005 Epoch: 0300 Loss: 1.3520323491 MSE_Loss: 1.2293329692 Sparsity_loss: 0.8373681080 KL_loss: 0.0492154171 MMD_loss: 0.0401694086 time: 0.8149s\n",
      "Feature: 0005 Epoch: 0400 Loss: 1.3520302735 MSE_Loss: 1.2293311614 Sparsity_loss: 0.8373647491 KL_loss: 0.0492149927 MMD_loss: 0.0401693566 time: 0.8138s\n",
      "Feature: 0005 Epoch: 0500 Loss: 1.3520293953 MSE_Loss: 1.2293305933 Sparsity_loss: 0.8373591153 KL_loss: 0.0492184349 MMD_loss: 0.0401693381 time: 0.8140s\n",
      "Feature: 0005 Epoch: 0600 Loss: 1.3520303074 MSE_Loss: 1.2293322162 Sparsity_loss: 0.8373458642 KL_loss: 0.0492177046 MMD_loss: 0.0401693079 time: 0.8146s\n",
      "Feature: 0005 Epoch: 0700 Loss: 1.3520305421 MSE_Loss: 1.2293323294 Sparsity_loss: 0.8373479036 KL_loss: 0.0492169927 MMD_loss: 0.0401693237 time: 0.8143s\n",
      "Feature: 0005 Epoch: 0800 Loss: 1.3520253293 MSE_Loss: 1.2293279918 Sparsity_loss: 0.8373335011 KL_loss: 0.0492167568 MMD_loss: 0.0401692426 time: 0.8147s\n",
      "Feature: 0005 Epoch: 0900 Loss: 1.3520292572 MSE_Loss: 1.2293309320 Sparsity_loss: 0.8373535140 KL_loss: 0.0492175130 MMD_loss: 0.0401692362 time: 0.8144s\n",
      "Feature: 0005 Epoch: 1000 Loss: 1.3520309404 MSE_Loss: 1.2293317695 Sparsity_loss: 0.8373633487 KL_loss: 0.0492164931 MMD_loss: 0.0401694233 time: 0.8137s\n",
      "Feature: 0005 Epoch: 1100 Loss: 1.3520293160 MSE_Loss: 1.2293311833 Sparsity_loss: 0.8373502289 KL_loss: 0.0492165500 MMD_loss: 0.0401692245 time: 0.8143s\n",
      "Feature: 0005 Epoch: 1200 Loss: 1.3520278199 MSE_Loss: 1.2293304446 Sparsity_loss: 0.8373305375 KL_loss: 0.0492157436 MMD_loss: 0.0401693418 time: 0.8133s\n",
      "Feature: 0005 Epoch: 1300 Loss: 1.3520287230 MSE_Loss: 1.2293305789 Sparsity_loss: 0.8373487207 KL_loss: 0.0492180928 MMD_loss: 0.0401692630 time: 0.8147s\n",
      "Feature: 0005 Epoch: 1400 Loss: 1.3520265697 MSE_Loss: 1.2293285833 Sparsity_loss: 0.8373439644 KL_loss: 0.0492173179 MMD_loss: 0.0401693079 time: 0.8151s\n",
      "Feature: 0005 Epoch: 1500 Loss: 1.3520280183 MSE_Loss: 1.2293299799 Sparsity_loss: 0.8373402485 KL_loss: 0.0492170744 MMD_loss: 0.0401694214 time: 0.8155s\n",
      "Feature: 0005 Epoch: 1600 Loss: 1.3520266715 MSE_Loss: 1.2293276576 Sparsity_loss: 0.8373649316 KL_loss: 0.0492199924 MMD_loss: 0.0401692837 time: 0.8135s\n",
      "Feature: 0005 Epoch: 1700 Loss: 1.3520301135 MSE_Loss: 1.2293313749 Sparsity_loss: 0.8373597476 KL_loss: 0.0492186104 MMD_loss: 0.0401692830 time: 0.8139s\n",
      "Feature: 0005 Epoch: 1800 Loss: 1.3520297597 MSE_Loss: 1.2293317590 Sparsity_loss: 0.8373492481 KL_loss: 0.0492154345 MMD_loss: 0.0401691943 time: 0.8143s\n",
      "Feature: 0005 Epoch: 1900 Loss: 1.3520290293 MSE_Loss: 1.2293307661 Sparsity_loss: 0.8373486173 KL_loss: 0.0492152175 MMD_loss: 0.0401693426 time: 0.8143s\n",
      "Begin training feature: 0006\n",
      "Feature: 0006 Epoch: 0000 Loss: 1.9886072255 MSE_Loss: 1.4942614187 Sparsity_loss: 8.3084070713 KL_loss: 0.0818833384 MMD_loss: 0.0390533071 time: 0.8151s\n",
      "Feature: 0006 Epoch: 0100 Loss: 1.1880283280 MSE_Loss: 1.0644449561 Sparsity_loss: 1.1638266482 KL_loss: 0.0548860609 MMD_loss: 0.0324215898 time: 0.8145s\n",
      "Feature: 0006 Epoch: 0200 Loss: 1.1880322936 MSE_Loss: 1.0644515715 Sparsity_loss: 1.1637674072 KL_loss: 0.0548721599 MMD_loss: 0.0324218152 time: 0.8141s\n",
      "Feature: 0006 Epoch: 0300 Loss: 1.1880459793 MSE_Loss: 1.0644640945 Sparsity_loss: 1.1637914981 KL_loss: 0.0548734092 MMD_loss: 0.0324217851 time: 0.8135s\n",
      "Feature: 0006 Epoch: 0400 Loss: 1.1880228972 MSE_Loss: 1.0644419970 Sparsity_loss: 1.1637782024 KL_loss: 0.0548769114 MMD_loss: 0.0324216104 time: 0.8139s\n",
      "Feature: 0006 Epoch: 0500 Loss: 1.1742747290 MSE_Loss: 1.0670182916 Sparsity_loss: 0.8372337290 KL_loss: 0.0491810906 MMD_loss: 0.0324514736 time: 0.8145s\n",
      "Feature: 0006 Epoch: 0600 Loss: 1.1742727862 MSE_Loss: 1.0670161096 Sparsity_loss: 0.8372431918 KL_loss: 0.0491848361 MMD_loss: 0.0324513292 time: 0.8149s\n",
      "Feature: 0006 Epoch: 0700 Loss: 1.1742765617 MSE_Loss: 1.0670174738 Sparsity_loss: 0.8372853097 KL_loss: 0.0491798783 MMD_loss: 0.0324515144 time: 0.8150s\n",
      "Feature: 0006 Epoch: 0800 Loss: 1.1680839220 MSE_Loss: 1.0690036092 Sparsity_loss: 0.6739109672 KL_loss: 0.0462952883 MMD_loss: 0.0324609087 time: 0.8141s\n",
      "Feature: 0006 Epoch: 0900 Loss: 1.1680892449 MSE_Loss: 1.0690095779 Sparsity_loss: 0.6738910177 KL_loss: 0.0462971027 MMD_loss: 0.0324610738 time: 0.8139s\n",
      "Feature: 0006 Epoch: 1000 Loss: 1.1680880068 MSE_Loss: 1.0690082704 Sparsity_loss: 0.6738933363 KL_loss: 0.0462985775 MMD_loss: 0.0324610372 time: 0.8133s\n",
      "Feature: 0006 Epoch: 1100 Loss: 1.1680760897 MSE_Loss: 1.0689975567 Sparsity_loss: 0.6738754511 KL_loss: 0.0462997518 MMD_loss: 0.0324608748 time: 0.8135s\n",
      "Feature: 0006 Epoch: 1200 Loss: 1.1624519780 MSE_Loss: 1.0715078908 Sparsity_loss: 0.5104350085 KL_loss: 0.0434083124 MMD_loss: 0.0324941351 time: 0.8146s\n",
      "Feature: 0006 Epoch: 1300 Loss: 1.1624574473 MSE_Loss: 1.0715125022 Sparsity_loss: 0.5104512308 KL_loss: 0.0434076541 MMD_loss: 0.0324941640 time: 0.8151s\n",
      "Feature: 0006 Epoch: 1400 Loss: 1.1624554026 MSE_Loss: 1.0715100592 Sparsity_loss: 0.5104610716 KL_loss: 0.0434081593 MMD_loss: 0.0324941128 time: 0.8213s\n",
      "Feature: 0006 Epoch: 1500 Loss: 1.1624572903 MSE_Loss: 1.0715121748 Sparsity_loss: 0.5104508815 KL_loss: 0.0434075352 MMD_loss: 0.0324942581 time: 0.8203s\n",
      "Feature: 0006 Epoch: 1600 Loss: 1.1624538152 MSE_Loss: 1.0715093062 Sparsity_loss: 0.5104428296 KL_loss: 0.0434082063 MMD_loss: 0.0324941460 time: 0.8213s\n",
      "Feature: 0006 Epoch: 1700 Loss: 1.1624554999 MSE_Loss: 1.0715109299 Sparsity_loss: 0.5104409117 KL_loss: 0.0434081181 MMD_loss: 0.0324942287 time: 0.8190s\n",
      "Feature: 0006 Epoch: 1800 Loss: 1.1624488755 MSE_Loss: 1.0715052802 Sparsity_loss: 0.5104233623 KL_loss: 0.0434086033 MMD_loss: 0.0324941698 time: 0.8202s\n",
      "Feature: 0006 Epoch: 1900 Loss: 1.1624550940 MSE_Loss: 1.0715108733 Sparsity_loss: 0.5104422456 KL_loss: 0.0434082267 MMD_loss: 0.0324940195 time: 0.8210s\n",
      "Begin training feature: 0007\n",
      "Feature: 0007 Epoch: 0000 Loss: 2.0550658733 MSE_Loss: 1.5550437245 Sparsity_loss: 8.2703992264 KL_loss: 0.0685117712 MMD_loss: 0.0429085297 time: 0.8196s\n",
      "Feature: 0007 Epoch: 0100 Loss: 1.3545414270 MSE_Loss: 1.2113806024 Sparsity_loss: 1.3066632748 KL_loss: 0.0594457673 MMD_loss: 0.0386166048 time: 0.8146s\n",
      "Feature: 0007 Epoch: 0200 Loss: 1.3545419000 MSE_Loss: 1.2113924540 Sparsity_loss: 1.3064452063 KL_loss: 0.0593861619 MMD_loss: 0.0386166623 time: 0.8145s\n",
      "Feature: 0007 Epoch: 0300 Loss: 1.3545329699 MSE_Loss: 1.2113837570 Sparsity_loss: 1.3064493092 KL_loss: 0.0593926036 MMD_loss: 0.0386164075 time: 0.8147s\n",
      "Feature: 0007 Epoch: 0400 Loss: 1.3545319068 MSE_Loss: 1.2113616814 Sparsity_loss: 1.3068478077 KL_loss: 0.0594912668 MMD_loss: 0.0386164586 time: 0.8139s\n",
      "Feature: 0007 Epoch: 0500 Loss: 1.3545308143 MSE_Loss: 1.2113637207 Sparsity_loss: 1.3067847641 KL_loss: 0.0594820526 MMD_loss: 0.0386165169 time: 0.8156s\n",
      "Feature: 0007 Epoch: 0600 Loss: 1.3545333479 MSE_Loss: 1.2113684680 Sparsity_loss: 1.3067484536 KL_loss: 0.0594648860 MMD_loss: 0.0386164032 time: 0.8150s\n",
      "Feature: 0007 Epoch: 0700 Loss: 1.3545361139 MSE_Loss: 1.2113590942 Sparsity_loss: 1.3069760709 KL_loss: 0.0595221992 MMD_loss: 0.0386164945 time: 0.8148s\n",
      "Feature: 0007 Epoch: 0800 Loss: 1.3545272818 MSE_Loss: 1.2113622321 Sparsity_loss: 1.3067488323 KL_loss: 0.0594686575 MMD_loss: 0.0386164618 time: 0.8264s\n",
      "Feature: 0007 Epoch: 0900 Loss: 1.3545319612 MSE_Loss: 1.2113769884 Sparsity_loss: 1.3065615138 KL_loss: 0.0594225780 MMD_loss: 0.0386163357 time: 0.8212s\n",
      "Feature: 0007 Epoch: 1000 Loss: 1.3545305027 MSE_Loss: 1.2113574245 Sparsity_loss: 1.3069062731 KL_loss: 0.0595053365 MMD_loss: 0.0386163596 time: 0.8215s\n",
      "Feature: 0007 Epoch: 1100 Loss: 1.3545346577 MSE_Loss: 1.2113623649 Sparsity_loss: 1.3068850629 KL_loss: 0.0594964555 MMD_loss: 0.0386165322 time: 0.8226s\n",
      "Feature: 0007 Epoch: 1200 Loss: 1.3545340217 MSE_Loss: 1.2113709103 Sparsity_loss: 1.3067068269 KL_loss: 0.0594654734 MMD_loss: 0.0386165565 time: 0.8215s\n",
      "Feature: 0007 Epoch: 1300 Loss: 1.3545309562 MSE_Loss: 1.2113665071 Sparsity_loss: 1.3067360286 KL_loss: 0.0594680343 MMD_loss: 0.0386164813 time: 0.8207s\n",
      "Feature: 0007 Epoch: 1400 Loss: 1.3545237644 MSE_Loss: 1.2113561042 Sparsity_loss: 1.3067932687 KL_loss: 0.0594877842 MMD_loss: 0.0386165503 time: 0.8205s\n",
      "Feature: 0007 Epoch: 1500 Loss: 1.3545272215 MSE_Loss: 1.2113593711 Sparsity_loss: 1.3068042571 KL_loss: 0.0594876303 MMD_loss: 0.0386163772 time: 0.8206s\n",
      "Feature: 0007 Epoch: 1600 Loss: 1.3545281291 MSE_Loss: 1.2113752086 Sparsity_loss: 1.3065135841 KL_loss: 0.0594282679 MMD_loss: 0.0386164786 time: 0.8204s\n",
      "Feature: 0007 Epoch: 1700 Loss: 1.3545257079 MSE_Loss: 1.2113584838 Sparsity_loss: 1.3067905752 KL_loss: 0.0594796681 MMD_loss: 0.0386164534 time: 0.8203s\n",
      "Feature: 0007 Epoch: 1800 Loss: 1.3545332913 MSE_Loss: 1.2113836445 Sparsity_loss: 1.3064472358 KL_loss: 0.0594044049 MMD_loss: 0.0386166229 time: 0.8228s\n",
      "Feature: 0007 Epoch: 1900 Loss: 1.3545334852 MSE_Loss: 1.2113609646 Sparsity_loss: 1.3068912995 KL_loss: 0.0594973873 MMD_loss: 0.0386164947 time: 0.8207s\n",
      "Begin training feature: 0008\n",
      "Feature: 0008 Epoch: 0000 Loss: 2.2408952985 MSE_Loss: 1.7393736273 Sparsity_loss: 8.2892455451 KL_loss: 0.0728190008 MMD_loss: 0.0431656043 time: 0.8211s\n",
      "Feature: 0008 Epoch: 0100 Loss: 1.2966561657 MSE_Loss: 1.1500440972 Sparsity_loss: 1.4909824480 KL_loss: 0.0607657860 MMD_loss: 0.0357276494 time: 0.8142s\n",
      "Feature: 0008 Epoch: 0200 Loss: 1.2966590841 MSE_Loss: 1.1500457782 Sparsity_loss: 1.4910031934 KL_loss: 0.0607659389 MMD_loss: 0.0357277442 time: 0.8144s\n",
      "Feature: 0008 Epoch: 0300 Loss: 1.2901342511 MSE_Loss: 1.1516924576 Sparsity_loss: 1.3275933658 KL_loss: 0.0578928773 MMD_loss: 0.0357415929 time: 0.8149s\n",
      "Feature: 0008 Epoch: 0400 Loss: 1.2901365780 MSE_Loss: 1.1516930831 Sparsity_loss: 1.3276228739 KL_loss: 0.0578921608 MMD_loss: 0.0357417187 time: 0.8154s\n",
      "Feature: 0008 Epoch: 0500 Loss: 1.2901351776 MSE_Loss: 1.1516932143 Sparsity_loss: 1.3275906210 KL_loss: 0.0578948468 MMD_loss: 0.0357417377 time: 0.8146s\n",
      "Feature: 0008 Epoch: 0600 Loss: 1.2901366949 MSE_Loss: 1.1516918925 Sparsity_loss: 1.3276535363 KL_loss: 0.0579017778 MMD_loss: 0.0357415582 time: 0.8146s\n",
      "Feature: 0008 Epoch: 0700 Loss: 1.2901453512 MSE_Loss: 1.1517013885 Sparsity_loss: 1.3276308503 KL_loss: 0.0578925506 MMD_loss: 0.0357417510 time: 0.8136s\n",
      "Feature: 0008 Epoch: 0800 Loss: 1.2838770860 MSE_Loss: 1.1535851050 Sparsity_loss: 1.1642057503 KL_loss: 0.0550146299 MMD_loss: 0.0357657818 time: 0.8169s\n",
      "Feature: 0008 Epoch: 0900 Loss: 1.2838907159 MSE_Loss: 1.1535961221 Sparsity_loss: 1.1642586144 KL_loss: 0.0550128256 MMD_loss: 0.0357657658 time: 0.8156s\n",
      "Feature: 0008 Epoch: 1000 Loss: 1.2838792469 MSE_Loss: 1.1535867996 Sparsity_loss: 1.1642135985 KL_loss: 0.0550166079 MMD_loss: 0.0357658007 time: 0.8159s\n",
      "Feature: 0008 Epoch: 1100 Loss: 1.2838836622 MSE_Loss: 1.1535923353 Sparsity_loss: 1.1641957850 KL_loss: 0.0550129061 MMD_loss: 0.0357657045 time: 0.8148s\n",
      "Feature: 0008 Epoch: 1200 Loss: 1.2838799953 MSE_Loss: 1.1535869671 Sparsity_loss: 1.1642253007 KL_loss: 0.0550198515 MMD_loss: 0.0357657839 time: 0.8143s\n",
      "Feature: 0008 Epoch: 1300 Loss: 1.2838756344 MSE_Loss: 1.1535818350 Sparsity_loss: 1.1642377558 KL_loss: 0.0550206245 MMD_loss: 0.0357658546 time: 0.8142s\n",
      "Feature: 0008 Epoch: 1400 Loss: 1.2838753039 MSE_Loss: 1.1535834512 Sparsity_loss: 1.1642006289 KL_loss: 0.0550154368 MMD_loss: 0.0357658316 time: 0.8150s\n",
      "Feature: 0008 Epoch: 1500 Loss: 1.2838796181 MSE_Loss: 1.1535867181 Sparsity_loss: 1.1642233616 KL_loss: 0.0550181110 MMD_loss: 0.0357657786 time: 0.8242s\n",
      "Feature: 0008 Epoch: 1600 Loss: 1.2838785475 MSE_Loss: 1.1535864782 Sparsity_loss: 1.1642143530 KL_loss: 0.0550146645 MMD_loss: 0.0357655996 time: 0.8209s\n",
      "Feature: 0008 Epoch: 1700 Loss: 1.2838749674 MSE_Loss: 1.1535833621 Sparsity_loss: 1.1641982899 KL_loss: 0.0550181018 MMD_loss: 0.0357657603 time: 0.8206s\n",
      "Feature: 0008 Epoch: 1800 Loss: 1.2838777047 MSE_Loss: 1.1535850718 Sparsity_loss: 1.1642181496 KL_loss: 0.0550168945 MMD_loss: 0.0357657801 time: 0.8209s\n",
      "Feature: 0008 Epoch: 1900 Loss: 1.2838796151 MSE_Loss: 1.1535877072 Sparsity_loss: 1.1642082069 KL_loss: 0.0550172552 MMD_loss: 0.0357656654 time: 0.8207s\n",
      "Begin training feature: 0009\n",
      "Feature: 0009 Epoch: 0000 Loss: 2.0106333389 MSE_Loss: 1.5090593530 Sparsity_loss: 8.3202718300 KL_loss: 0.0860122626 MMD_loss: 0.0423501313 time: 0.8215s\n",
      "Feature: 0009 Epoch: 0100 Loss: 1.2801032934 MSE_Loss: 1.1148124284 Sparsity_loss: 1.8168064778 KL_loss: 0.0662539739 MMD_loss: 0.0368939964 time: 0.8152s\n",
      "Feature: 0009 Epoch: 0200 Loss: 1.2729111706 MSE_Loss: 1.1157876359 Sparsity_loss: 1.6537881365 KL_loss: 0.0634699966 MMD_loss: 0.0368997077 time: 0.8156s\n",
      "Feature: 0009 Epoch: 0300 Loss: 1.2729027852 MSE_Loss: 1.1157781987 Sparsity_loss: 1.6538006837 KL_loss: 0.0634874141 MMD_loss: 0.0368998417 time: 0.8153s\n",
      "Feature: 0009 Epoch: 0400 Loss: 1.2658591044 MSE_Loss: 1.1169032786 Sparsity_loss: 1.4904823952 KL_loss: 0.0606270355 MMD_loss: 0.0369127182 time: 0.8147s\n",
      "Feature: 0009 Epoch: 0500 Loss: 1.2598625671 MSE_Loss: 1.1180772970 Sparsity_loss: 1.3473588953 KL_loss: 0.0580294336 MMD_loss: 0.0369185145 time: 0.8139s\n",
      "Feature: 0009 Epoch: 0600 Loss: 1.2523740258 MSE_Loss: 1.1197577111 Sparsity_loss: 1.1639060461 KL_loss: 0.0549182901 MMD_loss: 0.0369359096 time: 0.8148s\n",
      "Feature: 0009 Epoch: 0700 Loss: 1.2523835030 MSE_Loss: 1.1197657691 Sparsity_loss: 1.1639384756 KL_loss: 0.0549161169 MMD_loss: 0.0369358255 time: 0.8153s\n",
      "Feature: 0009 Epoch: 0800 Loss: 1.2523835928 MSE_Loss: 1.1197677307 Sparsity_loss: 1.1638927052 KL_loss: 0.0549123373 MMD_loss: 0.0369360498 time: 0.8156s\n",
      "Feature: 0009 Epoch: 0900 Loss: 1.2456780974 MSE_Loss: 1.1212290925 Sparsity_loss: 1.0005399034 KL_loss: 0.0520351987 MMD_loss: 0.0369508225 time: 0.8156s\n",
      "Feature: 0009 Epoch: 1000 Loss: 1.2282277444 MSE_Loss: 1.1282102722 Sparsity_loss: 0.5104741107 KL_loss: 0.0434067279 MMD_loss: 0.0370298596 time: 0.8148s\n",
      "Feature: 0009 Epoch: 1100 Loss: 1.2282167794 MSE_Loss: 1.1282008849 Sparsity_loss: 0.5104500478 KL_loss: 0.0434069329 MMD_loss: 0.0370296676 time: 0.8143s\n",
      "Feature: 0009 Epoch: 1200 Loss: 1.2282209940 MSE_Loss: 1.1282048369 Sparsity_loss: 0.5104580129 KL_loss: 0.0434065025 MMD_loss: 0.0370296002 time: 0.8139s\n",
      "Feature: 0009 Epoch: 1300 Loss: 1.2282160724 MSE_Loss: 1.1282007377 Sparsity_loss: 0.5104463779 KL_loss: 0.0434075811 MMD_loss: 0.0370294842 time: 0.8154s\n",
      "Feature: 0009 Epoch: 1400 Loss: 1.2282140444 MSE_Loss: 1.1281985678 Sparsity_loss: 0.5104425957 KL_loss: 0.0434071250 MMD_loss: 0.0370296474 time: 0.8152s\n",
      "Feature: 0009 Epoch: 1500 Loss: 1.2282205164 MSE_Loss: 1.1282054367 Sparsity_loss: 0.5104444600 KL_loss: 0.0434073725 MMD_loss: 0.0370294010 time: 0.8158s\n",
      "Feature: 0009 Epoch: 1600 Loss: 1.2282171008 MSE_Loss: 1.1282009422 Sparsity_loss: 0.5104565190 KL_loss: 0.0434071352 MMD_loss: 0.0370296411 time: 0.8206s\n",
      "Feature: 0009 Epoch: 1700 Loss: 1.2282124667 MSE_Loss: 1.1281975794 Sparsity_loss: 0.5104342993 KL_loss: 0.0434074201 MMD_loss: 0.0370295585 time: 0.8211s\n",
      "Feature: 0009 Epoch: 1800 Loss: 1.2282141168 MSE_Loss: 1.1281993880 Sparsity_loss: 0.5104276591 KL_loss: 0.0434070638 MMD_loss: 0.0370296506 time: 0.8212s\n",
      "Feature: 0009 Epoch: 1900 Loss: 1.2282144314 MSE_Loss: 1.1281996777 Sparsity_loss: 0.5104328567 KL_loss: 0.0434070095 MMD_loss: 0.0370295308 time: 0.8229s\n",
      "Begin training feature: 0010\n",
      "Feature: 0010 Epoch: 0000 Loss: 2.1155127921 MSE_Loss: 1.6155375793 Sparsity_loss: 8.2979260577 KL_loss: 0.0737995333 MMD_loss: 0.0421704491 time: 0.8211s\n",
      "Feature: 0010 Epoch: 0100 Loss: 1.3520601309 MSE_Loss: 1.2194576263 Sparsity_loss: 1.1643042006 KL_loss: 0.0549950392 MMD_loss: 0.0369186701 time: 0.8224s\n",
      "Feature: 0010 Epoch: 0200 Loss: 1.3520505353 MSE_Loss: 1.2194515987 Sparsity_loss: 1.1642459299 KL_loss: 0.0550088421 MMD_loss: 0.0369182797 time: 0.8232s\n",
      "Feature: 0010 Epoch: 0300 Loss: 1.3520472389 MSE_Loss: 1.2194484857 Sparsity_loss: 1.1642395047 KL_loss: 0.0550095921 MMD_loss: 0.0369183349 time: 0.8208s\n",
      "Feature: 0010 Epoch: 0400 Loss: 1.3520496993 MSE_Loss: 1.2194517164 Sparsity_loss: 1.1642217485 KL_loss: 0.0550036699 MMD_loss: 0.0369184302 time: 0.8207s\n",
      "Feature: 0010 Epoch: 0500 Loss: 1.3520495522 MSE_Loss: 1.2194499064 Sparsity_loss: 1.1642511887 KL_loss: 0.0550093971 MMD_loss: 0.0369184994 time: 0.8207s\n",
      "Feature: 0010 Epoch: 0600 Loss: 1.3520496722 MSE_Loss: 1.2194518885 Sparsity_loss: 1.1642249536 KL_loss: 0.0550077492 MMD_loss: 0.0369182354 time: 0.8218s\n",
      "Feature: 0010 Epoch: 0700 Loss: 1.3520431134 MSE_Loss: 1.2194459717 Sparsity_loss: 1.1642102546 KL_loss: 0.0550126765 MMD_loss: 0.0369182478 time: 0.8207s\n",
      "Feature: 0010 Epoch: 0800 Loss: 1.3520465674 MSE_Loss: 1.2194490448 Sparsity_loss: 1.1642105428 KL_loss: 0.0550139525 MMD_loss: 0.0369184268 time: 0.8203s\n",
      "Feature: 0010 Epoch: 0900 Loss: 1.3520786574 MSE_Loss: 1.2194809189 Sparsity_loss: 1.1641939833 KL_loss: 0.0549643999 MMD_loss: 0.0369191928 time: 0.8188s\n",
      "Feature: 0010 Epoch: 1000 Loss: 1.3520405738 MSE_Loss: 1.2194436034 Sparsity_loss: 1.1642043349 KL_loss: 0.0550100900 MMD_loss: 0.0369183202 time: 0.8209s\n",
      "Feature: 0010 Epoch: 1100 Loss: 1.3520427459 MSE_Loss: 1.2194449781 Sparsity_loss: 1.1642093477 KL_loss: 0.0550097775 MMD_loss: 0.0369185956 time: 0.8219s\n",
      "Feature: 0010 Epoch: 1200 Loss: 1.3457320016 MSE_Loss: 1.2212948233 Sparsity_loss: 1.0007932654 KL_loss: 0.0521262242 MMD_loss: 0.0369381261 time: 0.8212s\n",
      "Feature: 0010 Epoch: 1300 Loss: 1.3457329228 MSE_Loss: 1.2212937127 Sparsity_loss: 1.0008350158 KL_loss: 0.0521269486 MMD_loss: 0.0369380948 time: 0.8315s\n",
      "Feature: 0010 Epoch: 1400 Loss: 1.3457406307 MSE_Loss: 1.2213014960 Sparsity_loss: 1.0008291202 KL_loss: 0.0521211250 MMD_loss: 0.0369382363 time: 0.8207s\n",
      "Feature: 0010 Epoch: 1500 Loss: 1.3457356654 MSE_Loss: 1.2212967586 Sparsity_loss: 1.0008299426 KL_loss: 0.0521263160 MMD_loss: 0.0369380762 time: 0.8203s\n",
      "Feature: 0010 Epoch: 1600 Loss: 1.3457345721 MSE_Loss: 1.2212963180 Sparsity_loss: 1.0008241044 KL_loss: 0.0521282933 MMD_loss: 0.0369378823 time: 0.8219s\n",
      "Feature: 0010 Epoch: 1700 Loss: 1.3457426127 MSE_Loss: 1.2213049056 Sparsity_loss: 1.0008019390 KL_loss: 0.0521226707 MMD_loss: 0.0369381880 time: 0.8214s\n",
      "Feature: 0010 Epoch: 1800 Loss: 1.3457381590 MSE_Loss: 1.2212991813 Sparsity_loss: 1.0008314712 KL_loss: 0.0521252649 MMD_loss: 0.0369380694 time: 0.8222s\n",
      "Feature: 0010 Epoch: 1900 Loss: 1.3457295842 MSE_Loss: 1.2212926338 Sparsity_loss: 1.0007887143 KL_loss: 0.0521226416 MMD_loss: 0.0369381414 time: 0.8220s\n",
      "Begin training feature: 0011\n",
      "Feature: 0011 Epoch: 0000 Loss: 1.9147183307 MSE_Loss: 1.4240889429 Sparsity_loss: 8.2301948885 KL_loss: 0.0717007506 MMD_loss: 0.0392013137 time: 0.8211s\n",
      "Feature: 0011 Epoch: 0100 Loss: 1.2243111533 MSE_Loss: 1.1165245688 Sparsity_loss: 0.7550981558 KL_loss: 0.0487783645 MMD_loss: 0.0347719449 time: 0.7841s\n",
      "Feature: 0011 Epoch: 0200 Loss: 1.2170063770 MSE_Loss: 1.1183933080 Sparsity_loss: 0.5717298532 KL_loss: 0.0453715598 MMD_loss: 0.0347864298 time: 0.7838s\n",
      "Feature: 0011 Epoch: 0300 Loss: 1.2170022017 MSE_Loss: 1.1183895469 Sparsity_loss: 0.5717281594 KL_loss: 0.0453716747 MMD_loss: 0.0347862715 time: 0.7837s\n",
      "Feature: 0011 Epoch: 0400 Loss: 1.2170124559 MSE_Loss: 1.1183954870 Sparsity_loss: 0.5718063146 KL_loss: 0.0453731233 MMD_loss: 0.0347864558 time: 0.7845s\n",
      "Feature: 0011 Epoch: 0500 Loss: 1.2170054943 MSE_Loss: 1.1183907412 Sparsity_loss: 0.5717614565 KL_loss: 0.0453716804 MMD_loss: 0.0347864811 time: 0.7838s\n",
      "Feature: 0011 Epoch: 0600 Loss: 1.2170027721 MSE_Loss: 1.1183904855 Sparsity_loss: 0.5717218843 KL_loss: 0.0453702814 MMD_loss: 0.0347862476 time: 0.7848s\n",
      "Feature: 0011 Epoch: 0700 Loss: 1.2170069882 MSE_Loss: 1.1183926425 Sparsity_loss: 0.5717613976 KL_loss: 0.0453736979 MMD_loss: 0.0347862625 time: 0.7833s\n",
      "Feature: 0011 Epoch: 0800 Loss: 1.2170007568 MSE_Loss: 1.1183883367 Sparsity_loss: 0.5717237396 KL_loss: 0.0453732199 MMD_loss: 0.0347862555 time: 0.7832s\n",
      "Feature: 0011 Epoch: 0900 Loss: 1.2170337944 MSE_Loss: 1.1184155005 Sparsity_loss: 0.5718160589 KL_loss: 0.0453671824 MMD_loss: 0.0347869089 time: 0.7828s\n",
      "Feature: 0011 Epoch: 1000 Loss: 1.2170064615 MSE_Loss: 1.1183939146 Sparsity_loss: 0.5717199075 KL_loss: 0.0453714966 MMD_loss: 0.0347864179 time: 0.7831s\n",
      "Feature: 0011 Epoch: 1100 Loss: 1.2170132957 MSE_Loss: 1.1183985396 Sparsity_loss: 0.5717654877 KL_loss: 0.0453713807 MMD_loss: 0.0347863792 time: 0.7836s\n",
      "Feature: 0011 Epoch: 1200 Loss: 1.2170058798 MSE_Loss: 1.1183925429 Sparsity_loss: 0.5717362173 KL_loss: 0.0453722363 MMD_loss: 0.0347863988 time: 0.7840s\n",
      "Feature: 0011 Epoch: 1300 Loss: 1.2170068388 MSE_Loss: 1.1183925128 Sparsity_loss: 0.5717533706 KL_loss: 0.0453745878 MMD_loss: 0.0347864534 time: 0.7836s\n",
      "Feature: 0011 Epoch: 1400 Loss: 1.2169989604 MSE_Loss: 1.1183873619 Sparsity_loss: 0.5717077881 KL_loss: 0.0453733023 MMD_loss: 0.0347862419 time: 0.7833s\n",
      "Feature: 0011 Epoch: 1500 Loss: 1.2170038314 MSE_Loss: 1.1183903165 Sparsity_loss: 0.5717449075 KL_loss: 0.0453726549 MMD_loss: 0.0347862676 time: 0.7829s\n",
      "Feature: 0011 Epoch: 1600 Loss: 1.2170005660 MSE_Loss: 1.1183884551 Sparsity_loss: 0.5717225505 KL_loss: 0.0453737579 MMD_loss: 0.0347861227 time: 0.7827s\n",
      "Feature: 0011 Epoch: 1700 Loss: 1.2170068335 MSE_Loss: 1.1183927904 Sparsity_loss: 0.5717516421 KL_loss: 0.0453710910 MMD_loss: 0.0347863764 time: 0.7837s\n",
      "Feature: 0011 Epoch: 1800 Loss: 1.2170056354 MSE_Loss: 1.1183920186 Sparsity_loss: 0.5717469846 KL_loss: 0.0453720045 MMD_loss: 0.0347862776 time: 0.7839s\n",
      "Feature: 0011 Epoch: 1900 Loss: 1.2170030188 MSE_Loss: 1.1183899173 Sparsity_loss: 0.5717336845 KL_loss: 0.0453741743 MMD_loss: 0.0347863326 time: 0.7840s\n",
      "Begin training feature: 0012\n",
      "Feature: 0012 Epoch: 0000 Loss: 2.1157358583 MSE_Loss: 1.6062080430 Sparsity_loss: 8.4104281800 KL_loss: 0.0807822978 MMD_loss: 0.0440993026 time: 0.7827s\n",
      "Feature: 0012 Epoch: 0100 Loss: 1.3652288054 MSE_Loss: 1.2388838786 Sparsity_loss: 0.9390462644 KL_loss: 0.0523708745 MMD_loss: 0.0394344583 time: 0.7817s\n",
      "Feature: 0012 Epoch: 0200 Loss: 1.3652288212 MSE_Loss: 1.2388808244 Sparsity_loss: 0.9391015715 KL_loss: 0.0523774186 MMD_loss: 0.0394345635 time: 0.7825s\n",
      "Feature: 0012 Epoch: 0300 Loss: 1.3652282848 MSE_Loss: 1.2388730117 Sparsity_loss: 0.9392312494 KL_loss: 0.0524075670 MMD_loss: 0.0394348174 time: 0.7814s\n",
      "Feature: 0012 Epoch: 0400 Loss: 1.3652259353 MSE_Loss: 1.2388761549 Sparsity_loss: 0.9391310683 KL_loss: 0.0523867473 MMD_loss: 0.0394346756 time: 0.7819s\n",
      "Feature: 0012 Epoch: 0500 Loss: 1.3652384492 MSE_Loss: 1.2388875160 Sparsity_loss: 0.9391500867 KL_loss: 0.0523931276 MMD_loss: 0.0394347469 time: 0.7822s\n",
      "Feature: 0012 Epoch: 0600 Loss: 1.3652242814 MSE_Loss: 1.2388662356 Sparsity_loss: 0.9392791232 KL_loss: 0.0524276457 MMD_loss: 0.0394349079 time: 0.7820s\n",
      "Feature: 0012 Epoch: 0700 Loss: 1.3652305022 MSE_Loss: 1.2388927114 Sparsity_loss: 0.9389052859 KL_loss: 0.0523396883 MMD_loss: 0.0394345629 time: 0.7800s\n",
      "Feature: 0012 Epoch: 0800 Loss: 1.3652271877 MSE_Loss: 1.2388685504 Sparsity_loss: 0.9392946710 KL_loss: 0.0524297408 MMD_loss: 0.0394348034 time: 0.7807s\n",
      "Feature: 0012 Epoch: 0900 Loss: 1.3652262024 MSE_Loss: 1.2388710779 Sparsity_loss: 0.9392211905 KL_loss: 0.0524120302 MMD_loss: 0.0394349719 time: 0.7808s\n",
      "Feature: 0012 Epoch: 1000 Loss: 1.3652275476 MSE_Loss: 1.2388713843 Sparsity_loss: 0.9392424445 KL_loss: 0.0524157727 MMD_loss: 0.0394349409 time: 0.7815s\n",
      "Feature: 0012 Epoch: 1100 Loss: 1.3748531334 MSE_Loss: 1.2540157747 Sparsity_loss: 0.8373736581 KL_loss: 0.0492000229 MMD_loss: 0.0392383312 time: 0.7820s\n",
      "Feature: 0012 Epoch: 1200 Loss: 1.3700325964 MSE_Loss: 1.2573586053 Sparsity_loss: 0.6738631899 KL_loss: 0.0463034616 MMD_loss: 0.0392589030 time: 0.7821s\n",
      "Feature: 0012 Epoch: 1300 Loss: 1.3700384052 MSE_Loss: 1.2573646843 Sparsity_loss: 0.6738660358 KL_loss: 0.0463007110 MMD_loss: 0.0392587006 time: 0.7803s\n",
      "Feature: 0012 Epoch: 1400 Loss: 1.3700310724 MSE_Loss: 1.2573564015 Sparsity_loss: 0.6738745004 KL_loss: 0.0463043084 MMD_loss: 0.0392589503 time: 0.7803s\n",
      "Feature: 0012 Epoch: 1500 Loss: 1.3700298893 MSE_Loss: 1.2573553211 Sparsity_loss: 0.6738729484 KL_loss: 0.0463035889 MMD_loss: 0.0392589396 time: 0.7801s\n",
      "Feature: 0012 Epoch: 1600 Loss: 1.3700348350 MSE_Loss: 1.2573600525 Sparsity_loss: 0.6738771766 KL_loss: 0.0463034424 MMD_loss: 0.0392589422 time: 0.7805s\n",
      "Feature: 0012 Epoch: 1700 Loss: 1.3700293484 MSE_Loss: 1.2573546647 Sparsity_loss: 0.6738752406 KL_loss: 0.0463044395 MMD_loss: 0.0392589326 time: 0.7816s\n",
      "Feature: 0012 Epoch: 1800 Loss: 1.3700304778 MSE_Loss: 1.2573566195 Sparsity_loss: 0.6738587588 KL_loss: 0.0463021091 MMD_loss: 0.0392589531 time: 0.7817s\n",
      "Feature: 0012 Epoch: 1900 Loss: 1.3700313063 MSE_Loss: 1.2573573310 Sparsity_loss: 0.6738635234 KL_loss: 0.0463021047 MMD_loss: 0.0392588881 time: 0.7804s\n",
      "Begin training feature: 0013\n",
      "Feature: 0013 Epoch: 0000 Loss: 2.0109242050 MSE_Loss: 1.5047703370 Sparsity_loss: 8.3920338667 KL_loss: 0.0801213822 MMD_loss: 0.0428754819 time: 0.7816s\n",
      "Feature: 0013 Epoch: 0100 Loss: 1.2800532187 MSE_Loss: 1.1411993277 Sparsity_loss: 1.3067379601 KL_loss: 0.0594614063 MMD_loss: 0.0364611913 time: 0.7804s\n",
      "Feature: 0013 Epoch: 0200 Loss: 1.2893108295 MSE_Loss: 1.1572966900 Sparsity_loss: 1.1643141131 KL_loss: 0.0550331127 MMD_loss: 0.0366240513 time: 0.7821s\n",
      "Feature: 0013 Epoch: 0300 Loss: 1.2893111359 MSE_Loss: 1.1572979817 Sparsity_loss: 1.1642948812 KL_loss: 0.0550318092 MMD_loss: 0.0366240485 time: 0.7819s\n",
      "Feature: 0013 Epoch: 0400 Loss: 1.2893076509 MSE_Loss: 1.1572958322 Sparsity_loss: 1.1642712656 KL_loss: 0.0550309596 MMD_loss: 0.0366239697 time: 0.7827s\n",
      "Feature: 0013 Epoch: 0500 Loss: 1.2893082145 MSE_Loss: 1.1572961204 Sparsity_loss: 1.1642781511 KL_loss: 0.0550319742 MMD_loss: 0.0366239359 time: 0.7815s\n",
      "Feature: 0013 Epoch: 0600 Loss: 1.2893061947 MSE_Loss: 1.1572932012 Sparsity_loss: 1.1642925483 KL_loss: 0.0550334487 MMD_loss: 0.0366240170 time: 0.7810s\n",
      "Feature: 0013 Epoch: 0700 Loss: 1.2893063675 MSE_Loss: 1.1572948377 Sparsity_loss: 1.1642726147 KL_loss: 0.0550318590 MMD_loss: 0.0366237867 time: 0.7807s\n",
      "Feature: 0013 Epoch: 0800 Loss: 1.2893042979 MSE_Loss: 1.1572920212 Sparsity_loss: 1.1642829723 KL_loss: 0.0550344116 MMD_loss: 0.0366238897 time: 0.7812s\n",
      "Feature: 0013 Epoch: 0900 Loss: 1.2893067493 MSE_Loss: 1.1572939331 Sparsity_loss: 1.1642928410 KL_loss: 0.0550323982 MMD_loss: 0.0366239212 time: 0.7817s\n",
      "Feature: 0013 Epoch: 1000 Loss: 1.2893088490 MSE_Loss: 1.1572953961 Sparsity_loss: 1.1643084318 KL_loss: 0.0550324450 MMD_loss: 0.0366238529 time: 0.7822s\n",
      "Feature: 0013 Epoch: 1100 Loss: 1.2893081836 MSE_Loss: 1.1572963807 Sparsity_loss: 1.1642750321 KL_loss: 0.0550340122 MMD_loss: 0.0366238557 time: 0.7830s\n",
      "Feature: 0013 Epoch: 1200 Loss: 1.2893038347 MSE_Loss: 1.1572923336 Sparsity_loss: 1.1642701867 KL_loss: 0.0550375122 MMD_loss: 0.0366238048 time: 0.7811s\n",
      "Feature: 0013 Epoch: 1300 Loss: 1.2893076803 MSE_Loss: 1.1572955326 Sparsity_loss: 1.1642808401 KL_loss: 0.0550350985 MMD_loss: 0.0366238816 time: 0.7806s\n",
      "Feature: 0013 Epoch: 1400 Loss: 1.2893083511 MSE_Loss: 1.1572946159 Sparsity_loss: 1.1643078101 KL_loss: 0.0550345347 MMD_loss: 0.0366240016 time: 0.7812s\n",
      "Feature: 0013 Epoch: 1500 Loss: 1.2893116806 MSE_Loss: 1.1572993609 Sparsity_loss: 1.1642843968 KL_loss: 0.0550348309 MMD_loss: 0.0366238737 time: 0.7814s\n",
      "Feature: 0013 Epoch: 1600 Loss: 1.2893051671 MSE_Loss: 1.1572928829 Sparsity_loss: 1.1642819009 KL_loss: 0.0550367985 MMD_loss: 0.0366239065 time: 0.7829s\n",
      "Feature: 0013 Epoch: 1700 Loss: 1.2893034091 MSE_Loss: 1.1572921148 Sparsity_loss: 1.1642725030 KL_loss: 0.0550351885 MMD_loss: 0.0366236615 time: 0.7817s\n",
      "Feature: 0013 Epoch: 1800 Loss: 1.2893030568 MSE_Loss: 1.1572912728 Sparsity_loss: 1.1642785510 KL_loss: 0.0550367646 MMD_loss: 0.0366237475 time: 0.7806s\n",
      "Feature: 0013 Epoch: 1900 Loss: 1.2893054621 MSE_Loss: 1.1572936094 Sparsity_loss: 1.1642750245 KL_loss: 0.0550358129 MMD_loss: 0.0366238682 time: 0.7810s\n",
      "Begin training feature: 0014\n",
      "Feature: 0014 Epoch: 0000 Loss: 1.8361852863 MSE_Loss: 1.3446818892 Sparsity_loss: 8.2635910300 KL_loss: 0.0752180256 MMD_loss: 0.0387858275 time: 0.7809s\n",
      "Feature: 0014 Epoch: 0100 Loss: 1.1927559466 MSE_Loss: 1.0566119318 Sparsity_loss: 1.3271528166 KL_loss: 0.0577380717 MMD_loss: 0.0346044938 time: 0.7812s\n",
      "Feature: 0014 Epoch: 0200 Loss: 1.1863226860 MSE_Loss: 1.0583332400 Sparsity_loss: 1.1639411570 KL_loss: 0.0549128441 MMD_loss: 0.0346216350 time: 0.7828s\n",
      "Feature: 0014 Epoch: 0300 Loss: 1.1863154279 MSE_Loss: 1.0583286300 Sparsity_loss: 1.1638950230 KL_loss: 0.0549044674 MMD_loss: 0.0346215041 time: 0.7830s\n",
      "Feature: 0014 Epoch: 0400 Loss: 1.1863197496 MSE_Loss: 1.0583317016 Sparsity_loss: 1.1639192285 KL_loss: 0.0549119278 MMD_loss: 0.0346214890 time: 0.7822s\n",
      "Feature: 0014 Epoch: 0500 Loss: 1.1863146726 MSE_Loss: 1.0583278559 Sparsity_loss: 1.1638953459 KL_loss: 0.0549059061 MMD_loss: 0.0346214920 time: 0.7820s\n",
      "Feature: 0014 Epoch: 0600 Loss: 1.1863178762 MSE_Loss: 1.0583303194 Sparsity_loss: 1.1639111117 KL_loss: 0.0549132703 MMD_loss: 0.0346214313 time: 0.7814s\n",
      "Feature: 0014 Epoch: 0700 Loss: 1.1863191852 MSE_Loss: 1.0583319619 Sparsity_loss: 1.1639008296 KL_loss: 0.0549140441 MMD_loss: 0.0346215207 time: 0.7828s\n",
      "Feature: 0014 Epoch: 0800 Loss: 1.1863149654 MSE_Loss: 1.0583288451 Sparsity_loss: 1.1638799996 KL_loss: 0.0549100485 MMD_loss: 0.0346215094 time: 0.7833s\n",
      "Feature: 0014 Epoch: 0900 Loss: 1.1863442803 MSE_Loss: 1.0583548968 Sparsity_loss: 1.1639336408 KL_loss: 0.0548889929 MMD_loss: 0.0346219017 time: 0.7830s\n",
      "Feature: 0014 Epoch: 1000 Loss: 1.1796511653 MSE_Loss: 1.0598424847 Sparsity_loss: 1.0006479342 KL_loss: 0.0520613030 MMD_loss: 0.0346278346 time: 0.7834s\n",
      "Feature: 0014 Epoch: 1100 Loss: 1.1734621140 MSE_Loss: 1.0618054452 Sparsity_loss: 0.8372554711 KL_loss: 0.0491772209 MMD_loss: 0.0346510612 time: 0.7824s\n",
      "Feature: 0014 Epoch: 1200 Loss: 1.1734563105 MSE_Loss: 1.0618003750 Sparsity_loss: 0.8372442028 KL_loss: 0.0491790025 MMD_loss: 0.0346509665 time: 0.7821s\n",
      "Feature: 0014 Epoch: 1300 Loss: 1.1734613716 MSE_Loss: 1.0618051788 Sparsity_loss: 0.8372437441 KL_loss: 0.0491774063 MMD_loss: 0.0346511182 time: 0.7818s\n",
      "Feature: 0014 Epoch: 1400 Loss: 1.1734515859 MSE_Loss: 1.0617947103 Sparsity_loss: 0.8372625535 KL_loss: 0.0491863028 MMD_loss: 0.0346509454 time: 0.7830s\n",
      "Feature: 0014 Epoch: 1500 Loss: 1.1734572694 MSE_Loss: 1.0618026226 Sparsity_loss: 0.8372229406 KL_loss: 0.0491775576 MMD_loss: 0.0346508586 time: 0.7835s\n",
      "Feature: 0014 Epoch: 1600 Loss: 1.1734583649 MSE_Loss: 1.0618018863 Sparsity_loss: 0.8372538044 KL_loss: 0.0491828096 MMD_loss: 0.0346509805 time: 0.7835s\n",
      "Feature: 0014 Epoch: 1700 Loss: 1.1734545585 MSE_Loss: 1.0617990449 Sparsity_loss: 0.8372380718 KL_loss: 0.0491800581 MMD_loss: 0.0346509020 time: 0.7826s\n",
      "Feature: 0014 Epoch: 1800 Loss: 1.1734537528 MSE_Loss: 1.0617966916 Sparsity_loss: 0.8372643266 KL_loss: 0.0491836452 MMD_loss: 0.0346510054 time: 0.7818s\n",
      "Feature: 0014 Epoch: 1900 Loss: 1.1734592960 MSE_Loss: 1.0618035031 Sparsity_loss: 0.8372403059 KL_loss: 0.0491799815 MMD_loss: 0.0346509854 time: 0.7816s\n",
      "Begin training feature: 0015\n",
      "Feature: 0015 Epoch: 0000 Loss: 2.0823704578 MSE_Loss: 1.5842468822 Sparsity_loss: 8.2321201155 KL_loss: 0.0695619779 MMD_loss: 0.0429109656 time: 0.7820s\n",
      "Feature: 0015 Epoch: 0100 Loss: 1.3333852253 MSE_Loss: 1.2087465955 Sparsity_loss: 0.9390500505 KL_loss: 0.0523723353 MMD_loss: 0.0385812021 time: 0.7837s\n",
      "Feature: 0015 Epoch: 0200 Loss: 1.3333884063 MSE_Loss: 1.2087456546 Sparsity_loss: 0.9391359649 KL_loss: 0.0523763561 MMD_loss: 0.0385810884 time: 0.7839s\n",
      "Feature: 0015 Epoch: 0300 Loss: 1.3333787790 MSE_Loss: 1.2087346097 Sparsity_loss: 0.9391510434 KL_loss: 0.0523965716 MMD_loss: 0.0385813283 time: 0.7845s\n",
      "Feature: 0015 Epoch: 0400 Loss: 1.3333759632 MSE_Loss: 1.2087153045 Sparsity_loss: 0.9394606382 KL_loss: 0.0524662508 MMD_loss: 0.0385814807 time: 0.7826s\n",
      "Feature: 0015 Epoch: 0500 Loss: 1.3333923930 MSE_Loss: 1.2087544097 Sparsity_loss: 0.9390454949 KL_loss: 0.0523626177 MMD_loss: 0.0385810346 time: 0.7821s\n",
      "Feature: 0015 Epoch: 0600 Loss: 1.3333811722 MSE_Loss: 1.2087270406 Sparsity_loss: 0.9393189768 KL_loss: 0.0524510839 MMD_loss: 0.0385818359 time: 0.7827s\n",
      "Feature: 0015 Epoch: 0700 Loss: 1.3333778261 MSE_Loss: 1.2087328939 Sparsity_loss: 0.9391641609 KL_loss: 0.0524055551 MMD_loss: 0.0385813328 time: 0.7839s\n",
      "Feature: 0015 Epoch: 0800 Loss: 1.3333812552 MSE_Loss: 1.2087271297 Sparsity_loss: 0.9393321706 KL_loss: 0.0524438538 MMD_loss: 0.0385815403 time: 0.7831s\n",
      "Feature: 0015 Epoch: 0900 Loss: 1.3333855053 MSE_Loss: 1.2087415638 Sparsity_loss: 0.9391516507 KL_loss: 0.0523934968 MMD_loss: 0.0385812149 time: 0.7843s\n",
      "Feature: 0015 Epoch: 1000 Loss: 1.3333794444 MSE_Loss: 1.2087264227 Sparsity_loss: 0.9393154813 KL_loss: 0.0524319656 MMD_loss: 0.0385814698 time: 0.7849s\n",
      "Feature: 0015 Epoch: 1100 Loss: 1.3333762711 MSE_Loss: 1.2087251627 Sparsity_loss: 0.9392873456 KL_loss: 0.0524356145 MMD_loss: 0.0385811970 time: 0.7840s\n",
      "Feature: 0015 Epoch: 1200 Loss: 1.3333805060 MSE_Loss: 1.2087310356 Sparsity_loss: 0.9392500052 KL_loss: 0.0524207455 MMD_loss: 0.0385813747 time: 0.7831s\n",
      "Feature: 0015 Epoch: 1300 Loss: 1.3333852495 MSE_Loss: 1.2087325574 Sparsity_loss: 0.9393082072 KL_loss: 0.0524343112 MMD_loss: 0.0385814656 time: 0.7826s\n",
      "Feature: 0015 Epoch: 1400 Loss: 1.3333738786 MSE_Loss: 1.2087262416 Sparsity_loss: 0.9392142628 KL_loss: 0.0524165147 MMD_loss: 0.0385813824 time: 0.7837s\n",
      "Feature: 0015 Epoch: 1500 Loss: 1.3333756893 MSE_Loss: 1.2087280999 Sparsity_loss: 0.9392124105 KL_loss: 0.0524305601 MMD_loss: 0.0385813275 time: 0.7851s\n",
      "Feature: 0015 Epoch: 1600 Loss: 1.3333855867 MSE_Loss: 1.2087316905 Sparsity_loss: 0.9393278217 KL_loss: 0.0524411723 MMD_loss: 0.0385815447 time: 0.7850s\n",
      "Feature: 0015 Epoch: 1700 Loss: 1.3334007995 MSE_Loss: 1.2087413035 Sparsity_loss: 0.9394373486 KL_loss: 0.0524414525 MMD_loss: 0.0385816075 time: 0.7846s\n",
      "Feature: 0015 Epoch: 1800 Loss: 1.3333702525 MSE_Loss: 1.2087186854 Sparsity_loss: 0.9392925674 KL_loss: 0.0524335696 MMD_loss: 0.0385812970 time: 0.7831s\n",
      "Feature: 0015 Epoch: 1900 Loss: 1.3333767826 MSE_Loss: 1.2087261330 Sparsity_loss: 0.9392807967 KL_loss: 0.0524264786 MMD_loss: 0.0385811682 time: 0.7832s\n",
      "Begin training feature: 0016\n",
      "Feature: 0016 Epoch: 0000 Loss: 1.9486914798 MSE_Loss: 1.4494244343 Sparsity_loss: 8.4112721335 KL_loss: 0.0834978289 MMD_loss: 0.0389342265 time: 0.7827s\n",
      "Feature: 0016 Epoch: 0100 Loss: 1.1873948936 MSE_Loss: 1.0384071820 Sparsity_loss: 1.6534984263 KL_loss: 0.0633741131 MMD_loss: 0.0328395253 time: 0.7834s\n",
      "Feature: 0016 Epoch: 0200 Loss: 1.1664422690 MSE_Loss: 1.0419742571 Sparsity_loss: 1.1637336891 KL_loss: 0.0548569173 MMD_loss: 0.0328663818 time: 0.7845s\n",
      "Feature: 0016 Epoch: 0300 Loss: 1.1664346472 MSE_Loss: 1.0419669027 Sparsity_loss: 1.1637313079 KL_loss: 0.0548650356 MMD_loss: 0.0328662675 time: 0.7845s\n",
      "Feature: 0016 Epoch: 0400 Loss: 1.1664367379 MSE_Loss: 1.0419676990 Sparsity_loss: 1.1637559420 KL_loss: 0.0548703530 MMD_loss: 0.0328662652 time: 0.7851s\n",
      "Feature: 0016 Epoch: 0500 Loss: 1.1664378288 MSE_Loss: 1.0419678017 Sparsity_loss: 1.1637653143 KL_loss: 0.0548713370 MMD_loss: 0.0328665168 time: 0.7829s\n",
      "Feature: 0016 Epoch: 0600 Loss: 1.1664483698 MSE_Loss: 1.0419810535 Sparsity_loss: 1.1637090716 KL_loss: 0.0548522182 MMD_loss: 0.0328666683 time: 0.7828s\n",
      "Feature: 0016 Epoch: 0700 Loss: 1.1664352372 MSE_Loss: 1.0419652111 Sparsity_loss: 1.1637765094 KL_loss: 0.0548774863 MMD_loss: 0.0328662141 time: 0.7812s\n",
      "Feature: 0016 Epoch: 0800 Loss: 1.1664322947 MSE_Loss: 1.0419619977 Sparsity_loss: 1.1637782462 KL_loss: 0.0548804613 MMD_loss: 0.0328662859 time: 0.7829s\n",
      "Feature: 0016 Epoch: 0900 Loss: 1.1664301406 MSE_Loss: 1.0419616895 Sparsity_loss: 1.1637469636 KL_loss: 0.0548761457 MMD_loss: 0.0328661720 time: 0.7833s\n",
      "Feature: 0016 Epoch: 1000 Loss: 1.1664319770 MSE_Loss: 1.0419605533 Sparsity_loss: 1.1638011585 KL_loss: 0.0548862314 MMD_loss: 0.0328662577 time: 0.7840s\n",
      "Feature: 0016 Epoch: 1100 Loss: 1.1664333223 MSE_Loss: 1.0419619525 Sparsity_loss: 1.1638041101 KL_loss: 0.0548864342 MMD_loss: 0.0328661471 time: 0.7824s\n",
      "Feature: 0016 Epoch: 1200 Loss: 1.1664337591 MSE_Loss: 1.0419633660 Sparsity_loss: 1.1637805549 KL_loss: 0.0548791101 MMD_loss: 0.0328662859 time: 0.7815s\n",
      "Feature: 0016 Epoch: 1300 Loss: 1.1664302553 MSE_Loss: 1.0419591050 Sparsity_loss: 1.1637975415 KL_loss: 0.0548862834 MMD_loss: 0.0328662011 time: 0.7818s\n",
      "Feature: 0016 Epoch: 1400 Loss: 1.1664307978 MSE_Loss: 1.0419612527 Sparsity_loss: 1.1637604916 KL_loss: 0.0548786594 MMD_loss: 0.0328663714 time: 0.7819s\n",
      "Feature: 0016 Epoch: 1500 Loss: 1.1664294238 MSE_Loss: 1.0419598791 Sparsity_loss: 1.1637607436 KL_loss: 0.0548783084 MMD_loss: 0.0328663622 time: 0.7822s\n",
      "Feature: 0016 Epoch: 1600 Loss: 1.1594453907 MSE_Loss: 1.0431463179 Sparsity_loss: 1.0005037226 KL_loss: 0.0520242193 MMD_loss: 0.0328768222 time: 0.7834s\n",
      "Feature: 0016 Epoch: 1700 Loss: 1.1594495781 MSE_Loss: 1.0431500768 Sparsity_loss: 1.0005150339 KL_loss: 0.0520292543 MMD_loss: 0.0328767243 time: 0.7835s\n",
      "Feature: 0016 Epoch: 1800 Loss: 1.1594456427 MSE_Loss: 1.0431467046 Sparsity_loss: 1.0004985936 KL_loss: 0.0520222350 MMD_loss: 0.0328768909 time: 0.7827s\n",
      "Feature: 0016 Epoch: 1900 Loss: 1.1594506728 MSE_Loss: 1.0431496595 Sparsity_loss: 1.0005391051 KL_loss: 0.0520306891 MMD_loss: 0.0328768724 time: 0.7819s\n",
      "Begin training feature: 0017\n",
      "Feature: 0017 Epoch: 0000 Loss: 2.1365622931 MSE_Loss: 1.6397989777 Sparsity_loss: 8.2000370026 KL_loss: 0.0658110950 MMD_loss: 0.0430516825 time: 0.7824s\n",
      "Feature: 0017 Epoch: 0100 Loss: 1.2200345156 MSE_Loss: 1.1020662287 Sparsity_loss: 0.9394100716 KL_loss: 0.0524475956 MMD_loss: 0.0352366595 time: 0.7822s\n",
      "Feature: 0017 Epoch: 0200 Loss: 1.2200311279 MSE_Loss: 1.1020667047 Sparsity_loss: 0.9393283453 KL_loss: 0.0524293910 MMD_loss: 0.0352368543 time: 0.7830s\n",
      "Feature: 0017 Epoch: 0300 Loss: 1.2310277186 MSE_Loss: 1.1175115961 Sparsity_loss: 0.8373570246 KL_loss: 0.0492136907 MMD_loss: 0.0355780681 time: 0.7836s\n",
      "Feature: 0017 Epoch: 0400 Loss: 1.2310250469 MSE_Loss: 1.1175087585 Sparsity_loss: 0.8373582227 KL_loss: 0.0492132677 MMD_loss: 0.0355781234 time: 0.7840s\n",
      "Feature: 0017 Epoch: 0500 Loss: 1.2310237039 MSE_Loss: 1.1175081866 Sparsity_loss: 0.8373466956 KL_loss: 0.0492166634 MMD_loss: 0.0355780089 time: 0.7832s\n",
      "Feature: 0017 Epoch: 0600 Loss: 1.2310278220 MSE_Loss: 1.1175111230 Sparsity_loss: 0.8373601248 KL_loss: 0.0492147828 MMD_loss: 0.0355782739 time: 0.7821s\n",
      "Feature: 0017 Epoch: 0700 Loss: 1.2310292230 MSE_Loss: 1.1175121424 Sparsity_loss: 0.8373712218 KL_loss: 0.0492139792 MMD_loss: 0.0355781913 time: 0.7821s\n",
      "Feature: 0017 Epoch: 0800 Loss: 1.2310192034 MSE_Loss: 1.1175052395 Sparsity_loss: 0.8373193356 KL_loss: 0.0492139454 MMD_loss: 0.0355779321 time: 0.7824s\n",
      "Feature: 0017 Epoch: 0900 Loss: 1.2310187711 MSE_Loss: 1.1175036996 Sparsity_loss: 0.8373375354 KL_loss: 0.0492168946 MMD_loss: 0.0355780121 time: 0.7830s\n",
      "Feature: 0017 Epoch: 1000 Loss: 1.2310200598 MSE_Loss: 1.1175050871 Sparsity_loss: 0.8373357993 KL_loss: 0.0492149010 MMD_loss: 0.0355780183 time: 0.7841s\n",
      "Feature: 0017 Epoch: 1100 Loss: 1.2310189522 MSE_Loss: 1.1175031647 Sparsity_loss: 0.8373548366 KL_loss: 0.0492139198 MMD_loss: 0.0355779542 time: 0.7836s\n",
      "Feature: 0017 Epoch: 1200 Loss: 1.2310236044 MSE_Loss: 1.1175075709 Sparsity_loss: 0.8373543673 KL_loss: 0.0492142356 MMD_loss: 0.0355780877 time: 0.7825s\n",
      "Feature: 0017 Epoch: 1300 Loss: 1.2310188209 MSE_Loss: 1.1175030221 Sparsity_loss: 0.8373586528 KL_loss: 0.0492168156 MMD_loss: 0.0355778499 time: 0.7822s\n",
      "Feature: 0017 Epoch: 1400 Loss: 1.2310200401 MSE_Loss: 1.1175042602 Sparsity_loss: 0.8373511607 KL_loss: 0.0492157508 MMD_loss: 0.0355780330 time: 0.7816s\n",
      "Feature: 0017 Epoch: 1500 Loss: 1.2310300605 MSE_Loss: 1.1175121076 Sparsity_loss: 0.8373900531 KL_loss: 0.0492122223 MMD_loss: 0.0355781605 time: 0.7821s\n",
      "Feature: 0017 Epoch: 1600 Loss: 1.2310205872 MSE_Loss: 1.1175054923 Sparsity_loss: 0.8373427301 KL_loss: 0.0492160731 MMD_loss: 0.0355778983 time: 0.7823s\n",
      "Feature: 0017 Epoch: 1700 Loss: 1.2310251843 MSE_Loss: 1.1175098034 Sparsity_loss: 0.8373364746 KL_loss: 0.0492139760 MMD_loss: 0.0355782077 time: 0.7835s\n",
      "Feature: 0017 Epoch: 1800 Loss: 1.2310228325 MSE_Loss: 1.1175076418 Sparsity_loss: 0.8373419197 KL_loss: 0.0492125032 MMD_loss: 0.0355779874 time: 0.7840s\n",
      "Feature: 0017 Epoch: 1900 Loss: 1.2310237409 MSE_Loss: 1.1175074049 Sparsity_loss: 0.8373636717 KL_loss: 0.0492159631 MMD_loss: 0.0355779976 time: 0.7824s\n",
      "Begin training feature: 0018\n",
      "Feature: 0018 Epoch: 0000 Loss: 1.7040728195 MSE_Loss: 1.2195886743 Sparsity_loss: 8.2063762568 KL_loss: 0.0736405398 MMD_loss: 0.0367144627 time: 0.7826s\n",
      "Feature: 0018 Epoch: 0100 Loss: 1.0315172544 MSE_Loss: 0.9039706158 Sparsity_loss: 1.3067842390 KL_loss: 0.0594747634 MMD_loss: 0.0308063402 time: 0.7823s\n",
      "Feature: 0018 Epoch: 0200 Loss: 1.0315085310 MSE_Loss: 0.9039440147 Sparsity_loss: 1.3071535962 KL_loss: 0.0595623210 MMD_loss: 0.0308056087 time: 0.7825s\n",
      "Feature: 0018 Epoch: 0300 Loss: 1.0269681706 MSE_Loss: 0.9051451721 Sparsity_loss: 1.1917908644 KL_loss: 0.0555830699 MMD_loss: 0.0308388115 time: 0.7836s\n",
      "Feature: 0018 Epoch: 0400 Loss: 1.0265020049 MSE_Loss: 0.9049489294 Sparsity_loss: 1.1866796394 KL_loss: 0.0556615604 MMD_loss: 0.0308312383 time: 0.7838s\n",
      "Feature: 0018 Epoch: 0500 Loss: 1.0265174191 MSE_Loss: 0.9049456655 Sparsity_loss: 1.1870350611 KL_loss: 0.0556658358 MMD_loss: 0.0308316688 time: 0.7841s\n",
      "Feature: 0018 Epoch: 0600 Loss: 1.0264497067 MSE_Loss: 0.9049234496 Sparsity_loss: 1.1861386314 KL_loss: 0.0556707593 MMD_loss: 0.0308313100 time: 0.7832s\n",
      "Feature: 0018 Epoch: 0700 Loss: 1.0264758347 MSE_Loss: 0.9049398763 Sparsity_loss: 1.1863414985 KL_loss: 0.0556694007 MMD_loss: 0.0308310957 time: 0.7831s\n",
      "Feature: 0018 Epoch: 0800 Loss: 1.0265380031 MSE_Loss: 0.9049638095 Sparsity_loss: 1.1870697390 KL_loss: 0.0556648621 MMD_loss: 0.0308320362 time: 0.7823s\n",
      "Feature: 0018 Epoch: 0900 Loss: 1.0264725527 MSE_Loss: 0.9049353554 Sparsity_loss: 1.1863370560 KL_loss: 0.0556674091 MMD_loss: 0.0308318351 time: 0.7825s\n",
      "Feature: 0018 Epoch: 1000 Loss: 1.0264734822 MSE_Loss: 0.9049404045 Sparsity_loss: 1.1862458323 KL_loss: 0.0556639524 MMD_loss: 0.0308320720 time: 0.7835s\n",
      "Feature: 0018 Epoch: 1100 Loss: 1.0264615975 MSE_Loss: 0.9049315204 Sparsity_loss: 1.1862259545 KL_loss: 0.0556706599 MMD_loss: 0.0308310350 time: 0.7843s\n",
      "Feature: 0018 Epoch: 1200 Loss: 1.0264657064 MSE_Loss: 0.9049344470 Sparsity_loss: 1.1862400574 KL_loss: 0.0556664216 MMD_loss: 0.0308312964 time: 0.7845s\n",
      "Feature: 0018 Epoch: 1300 Loss: 1.0264625066 MSE_Loss: 0.9049302415 Sparsity_loss: 1.1863056633 KL_loss: 0.0556719614 MMD_loss: 0.0308301300 time: 0.7836s\n",
      "Feature: 0018 Epoch: 1400 Loss: 1.0264578401 MSE_Loss: 0.9049273609 Sparsity_loss: 1.1862137076 KL_loss: 0.0556687443 MMD_loss: 0.0308315560 time: 0.7825s\n",
      "Feature: 0018 Epoch: 1500 Loss: 1.0264561637 MSE_Loss: 0.9049270296 Sparsity_loss: 1.1861995247 KL_loss: 0.0556689431 MMD_loss: 0.0308312353 time: 0.7824s\n",
      "Feature: 0018 Epoch: 1600 Loss: 1.0264680582 MSE_Loss: 0.9049336065 Sparsity_loss: 1.1862818199 KL_loss: 0.0556697554 MMD_loss: 0.0308318314 time: 0.7827s\n",
      "Feature: 0018 Epoch: 1700 Loss: 1.0264609086 MSE_Loss: 0.9049327011 Sparsity_loss: 1.1861986873 KL_loss: 0.0556684405 MMD_loss: 0.0308307943 time: 0.7829s\n",
      "Feature: 0018 Epoch: 1800 Loss: 1.0264608898 MSE_Loss: 0.9049282044 Sparsity_loss: 1.1862852453 KL_loss: 0.0556697088 MMD_loss: 0.0308308596 time: 0.7847s\n",
      "Feature: 0018 Epoch: 1900 Loss: 1.0264597761 MSE_Loss: 0.9049276853 Sparsity_loss: 1.1862715377 KL_loss: 0.0556707764 MMD_loss: 0.0308309075 time: 0.7849s\n",
      "Begin training feature: 0019\n",
      "Feature: 0019 Epoch: 0000 Loss: 1.9488648584 MSE_Loss: 1.4482291258 Sparsity_loss: 8.3654172028 KL_loss: 0.0795441778 MMD_loss: 0.0407847076 time: 0.7840s\n",
      "Feature: 0019 Epoch: 0100 Loss: 1.0973648467 MSE_Loss: 0.9955648757 Sparsity_loss: 0.6739256005 KL_loss: 0.0463109259 MMD_loss: 0.0338202911 time: 0.8139s\n",
      "Feature: 0019 Epoch: 0200 Loss: 1.0973666484 MSE_Loss: 0.9955666065 Sparsity_loss: 0.6739248550 KL_loss: 0.0463111978 MMD_loss: 0.0338203443 time: 0.8149s\n",
      "Feature: 0019 Epoch: 0300 Loss: 1.0973658705 MSE_Loss: 0.9955663183 Sparsity_loss: 0.6739205673 KL_loss: 0.0463119610 MMD_loss: 0.0338202036 time: 0.8150s\n",
      "Feature: 0019 Epoch: 0400 Loss: 1.0973685565 MSE_Loss: 0.9955683094 Sparsity_loss: 0.6739276316 KL_loss: 0.0463104381 MMD_loss: 0.0338203771 time: 0.8150s\n",
      "Feature: 0019 Epoch: 0500 Loss: 1.0973614764 MSE_Loss: 0.9955615688 Sparsity_loss: 0.6739250301 KL_loss: 0.0463121661 MMD_loss: 0.0338202655 time: 0.8149s\n",
      "Feature: 0019 Epoch: 0600 Loss: 1.0973616227 MSE_Loss: 0.9955624863 Sparsity_loss: 0.6739095593 KL_loss: 0.0463108522 MMD_loss: 0.0338202685 time: 0.8143s\n",
      "Feature: 0019 Epoch: 0700 Loss: 1.0973636900 MSE_Loss: 0.9955639175 Sparsity_loss: 0.6739207189 KL_loss: 0.0463108953 MMD_loss: 0.0338203111 time: 0.8145s\n",
      "Feature: 0019 Epoch: 0800 Loss: 1.0973719932 MSE_Loss: 0.9955708317 Sparsity_loss: 0.6739470853 KL_loss: 0.0463094932 MMD_loss: 0.0338203567 time: 0.8143s\n",
      "Feature: 0019 Epoch: 0900 Loss: 1.0973629620 MSE_Loss: 0.9955638255 Sparsity_loss: 0.6739096257 KL_loss: 0.0463112031 MMD_loss: 0.0338202749 time: 0.8142s\n",
      "Feature: 0019 Epoch: 1000 Loss: 1.0973593261 MSE_Loss: 0.9955617363 Sparsity_loss: 0.6738809136 KL_loss: 0.0463113579 MMD_loss: 0.0338202175 time: 0.8151s\n",
      "Feature: 0019 Epoch: 1100 Loss: 1.0973610946 MSE_Loss: 0.9955620532 Sparsity_loss: 0.6739108902 KL_loss: 0.0463125317 MMD_loss: 0.0338201900 time: 0.8146s\n",
      "Feature: 0019 Epoch: 1200 Loss: 1.0973604382 MSE_Loss: 0.9955613862 Sparsity_loss: 0.6739057906 KL_loss: 0.0463107214 MMD_loss: 0.0338203239 time: 0.8143s\n",
      "Feature: 0019 Epoch: 1300 Loss: 1.0973616620 MSE_Loss: 0.9955634120 Sparsity_loss: 0.6738902217 KL_loss: 0.0463115518 MMD_loss: 0.0338203126 time: 0.8138s\n",
      "Feature: 0019 Epoch: 1400 Loss: 1.0973584124 MSE_Loss: 0.9955597045 Sparsity_loss: 0.6738977176 KL_loss: 0.0463134624 MMD_loss: 0.0338203428 time: 0.8145s\n",
      "Feature: 0019 Epoch: 1500 Loss: 1.0973599493 MSE_Loss: 0.9955614820 Sparsity_loss: 0.6738954269 KL_loss: 0.0463119584 MMD_loss: 0.0338202892 time: 0.8142s\n",
      "Feature: 0019 Epoch: 1600 Loss: 1.0973693578 MSE_Loss: 0.9955684724 Sparsity_loss: 0.6739393525 KL_loss: 0.0463098093 MMD_loss: 0.0338204122 time: 0.8149s\n",
      "Feature: 0019 Epoch: 1700 Loss: 1.0973602654 MSE_Loss: 0.9955621181 Sparsity_loss: 0.6738926942 KL_loss: 0.0463126174 MMD_loss: 0.0338201957 time: 0.8145s\n",
      "Feature: 0019 Epoch: 1800 Loss: 1.0973631649 MSE_Loss: 0.9955646645 Sparsity_loss: 0.6738983091 KL_loss: 0.0463104003 MMD_loss: 0.0338202398 time: 0.8147s\n",
      "Feature: 0019 Epoch: 1900 Loss: 1.0973626413 MSE_Loss: 0.9955637425 Sparsity_loss: 0.6739022151 KL_loss: 0.0463115708 MMD_loss: 0.0338203413 time: 0.8141s\n",
      "Begin training feature: 0020\n",
      "Feature: 0020 Epoch: 0000 Loss: 1.8964043614 MSE_Loss: 1.3903081070 Sparsity_loss: 8.4467685555 KL_loss: 0.0844456585 MMD_loss: 0.0414566779 time: 0.8140s\n",
      "Feature: 0020 Epoch: 0100 Loss: 1.1847017634 MSE_Loss: 1.0699080538 Sparsity_loss: 0.8374172306 KL_loss: 0.0492195655 MMD_loss: 0.0362153242 time: 0.7838s\n",
      "Feature: 0020 Epoch: 0200 Loss: 1.1847074801 MSE_Loss: 1.0699130772 Sparsity_loss: 0.8374243258 KL_loss: 0.0492161966 MMD_loss: 0.0362155168 time: 0.7850s\n",
      "Feature: 0020 Epoch: 0300 Loss: 1.1847060904 MSE_Loss: 1.0699107104 Sparsity_loss: 0.8374451157 KL_loss: 0.0492164280 MMD_loss: 0.0362154790 time: 0.7845s\n",
      "Feature: 0020 Epoch: 0400 Loss: 1.1846945105 MSE_Loss: 1.0699029648 Sparsity_loss: 0.8373746133 KL_loss: 0.0492210891 MMD_loss: 0.0362153029 time: 0.7856s\n",
      "Feature: 0020 Epoch: 0500 Loss: 1.1847075669 MSE_Loss: 1.0699149287 Sparsity_loss: 0.8373866602 KL_loss: 0.0492133473 MMD_loss: 0.0362155839 time: 0.7842s\n",
      "Feature: 0020 Epoch: 0600 Loss: 1.1847040563 MSE_Loss: 1.0699116037 Sparsity_loss: 0.8373882122 KL_loss: 0.0492152911 MMD_loss: 0.0362154440 time: 0.7830s\n",
      "Feature: 0020 Epoch: 0700 Loss: 1.1846996063 MSE_Loss: 1.0699087728 Sparsity_loss: 0.8373602651 KL_loss: 0.0492144345 MMD_loss: 0.0362153383 time: 0.7840s\n",
      "Feature: 0020 Epoch: 0800 Loss: 1.1846970727 MSE_Loss: 1.0699056553 Sparsity_loss: 0.8373692692 KL_loss: 0.0492171526 MMD_loss: 0.0362153870 time: 0.7840s\n",
      "Feature: 0020 Epoch: 0900 Loss: 1.1847078695 MSE_Loss: 1.0699160182 Sparsity_loss: 0.8373819507 KL_loss: 0.0492131816 MMD_loss: 0.0362153068 time: 0.7832s\n",
      "Feature: 0020 Epoch: 1000 Loss: 1.1847010843 MSE_Loss: 1.0699090437 Sparsity_loss: 0.8373810143 KL_loss: 0.0492203195 MMD_loss: 0.0362153978 time: 0.7848s\n",
      "Feature: 0020 Epoch: 1100 Loss: 1.1847107667 MSE_Loss: 1.0699166965 Sparsity_loss: 0.8374140142 KL_loss: 0.0492159140 MMD_loss: 0.0362156058 time: 0.7846s\n",
      "Feature: 0020 Epoch: 1200 Loss: 1.1847030437 MSE_Loss: 1.0699119296 Sparsity_loss: 0.8373626169 KL_loss: 0.0492172761 MMD_loss: 0.0362154030 time: 0.7831s\n",
      "Feature: 0020 Epoch: 1300 Loss: 1.1846989929 MSE_Loss: 1.0699077648 Sparsity_loss: 0.8373687720 KL_loss: 0.0492190857 MMD_loss: 0.0362152983 time: 0.7825s\n",
      "Feature: 0020 Epoch: 1400 Loss: 1.1847069550 MSE_Loss: 1.0699154176 Sparsity_loss: 0.8373719092 KL_loss: 0.0492165531 MMD_loss: 0.0362153906 time: 0.7831s\n",
      "Feature: 0020 Epoch: 1500 Loss: 1.1847030437 MSE_Loss: 1.0699106598 Sparsity_loss: 0.8373815478 KL_loss: 0.0492164492 MMD_loss: 0.0362155756 time: 0.7842s\n",
      "Feature: 0020 Epoch: 1600 Loss: 1.1847021708 MSE_Loss: 1.0699102139 Sparsity_loss: 0.8373805171 KL_loss: 0.0492178067 MMD_loss: 0.0362153744 time: 0.7844s\n",
      "Feature: 0020 Epoch: 1700 Loss: 1.1847027880 MSE_Loss: 1.0699111276 Sparsity_loss: 0.8373735592 KL_loss: 0.0492149864 MMD_loss: 0.0362154110 time: 0.7843s\n",
      "Feature: 0020 Epoch: 1800 Loss: 1.1846987696 MSE_Loss: 1.0699081270 Sparsity_loss: 0.8373583027 KL_loss: 0.0492177859 MMD_loss: 0.0362152701 time: 0.7827s\n",
      "Feature: 0020 Epoch: 1900 Loss: 1.1846977487 MSE_Loss: 1.0699072850 Sparsity_loss: 0.8373496729 KL_loss: 0.0492202272 MMD_loss: 0.0362153881 time: 0.7829s\n",
      "Begin training feature: 0021\n",
      "Feature: 0021 Epoch: 0000 Loss: 1.7554929173 MSE_Loss: 1.2696535218 Sparsity_loss: 8.2606109064 KL_loss: 0.0793335922 MMD_loss: 0.0360077514 time: 0.7832s\n",
      "Feature: 0021 Epoch: 0100 Loss: 1.0691177060 MSE_Loss: 0.9682007553 Sparsity_loss: 0.7554010515 KL_loss: 0.0488689362 MMD_loss: 0.0313291046 time: 0.7839s\n",
      "Feature: 0021 Epoch: 0200 Loss: 1.0691211880 MSE_Loss: 0.9682037646 Sparsity_loss: 0.7554117811 KL_loss: 0.0488607642 MMD_loss: 0.0313291122 time: 0.7838s\n",
      "Feature: 0021 Epoch: 0300 Loss: 1.0691154992 MSE_Loss: 0.9682009435 Sparsity_loss: 0.7553578125 KL_loss: 0.0488603782 MMD_loss: 0.0313290329 time: 0.7850s\n",
      "Feature: 0021 Epoch: 0400 Loss: 1.0691171960 MSE_Loss: 0.9682021337 Sparsity_loss: 0.7553694497 KL_loss: 0.0488618546 MMD_loss: 0.0313289850 time: 0.7843s\n",
      "Feature: 0021 Epoch: 0500 Loss: 1.0691138747 MSE_Loss: 0.9681967221 Sparsity_loss: 0.7554093690 KL_loss: 0.0488733772 MMD_loss: 0.0313289754 time: 0.7835s\n",
      "Feature: 0021 Epoch: 0600 Loss: 1.0691128916 MSE_Loss: 0.9681954323 Sparsity_loss: 0.7554192950 KL_loss: 0.0488757875 MMD_loss: 0.0313288675 time: 0.7830s\n",
      "Feature: 0021 Epoch: 0700 Loss: 1.0691097220 MSE_Loss: 0.9682012593 Sparsity_loss: 0.7552411534 KL_loss: 0.0488336554 MMD_loss: 0.0313290354 time: 0.7836s\n",
      "Feature: 0021 Epoch: 0800 Loss: 1.0691217177 MSE_Loss: 0.9682019070 Sparsity_loss: 0.7554564084 KL_loss: 0.0488757679 MMD_loss: 0.0313291152 time: 0.7840s\n",
      "Feature: 0021 Epoch: 0900 Loss: 1.0691138340 MSE_Loss: 0.9681996435 Sparsity_loss: 0.7553501816 KL_loss: 0.0488587514 MMD_loss: 0.0313290416 time: 0.7838s\n",
      "Feature: 0021 Epoch: 1000 Loss: 1.0691103483 MSE_Loss: 0.9681981742 Sparsity_loss: 0.7553104336 KL_loss: 0.0488513843 MMD_loss: 0.0313290686 time: 0.7849s\n",
      "Feature: 0021 Epoch: 1100 Loss: 1.0691114853 MSE_Loss: 0.9681979882 Sparsity_loss: 0.7553417170 KL_loss: 0.0488581913 MMD_loss: 0.0313289152 time: 0.7829s\n",
      "Feature: 0021 Epoch: 1200 Loss: 1.0691181150 MSE_Loss: 0.9682025736 Sparsity_loss: 0.7553748564 KL_loss: 0.0488637175 MMD_loss: 0.0313290810 time: 0.7832s\n",
      "Feature: 0021 Epoch: 1300 Loss: 1.0691122745 MSE_Loss: 0.9681979248 Sparsity_loss: 0.7553506312 KL_loss: 0.0488604197 MMD_loss: 0.0313291084 time: 0.7829s\n",
      "Feature: 0021 Epoch: 1400 Loss: 1.0691150201 MSE_Loss: 0.9682070307 Sparsity_loss: 0.7552283806 KL_loss: 0.0488304599 MMD_loss: 0.0313291335 time: 0.7842s\n",
      "Feature: 0021 Epoch: 1500 Loss: 1.0691226585 MSE_Loss: 0.9682074106 Sparsity_loss: 0.7553497779 KL_loss: 0.0488543741 MMD_loss: 0.0313296084 time: 0.7849s\n",
      "Feature: 0021 Epoch: 1600 Loss: 1.0691190022 MSE_Loss: 0.9682037317 Sparsity_loss: 0.7553649409 KL_loss: 0.0488572884 MMD_loss: 0.0313292283 time: 0.7845s\n",
      "Feature: 0021 Epoch: 1700 Loss: 1.0691145628 MSE_Loss: 0.9682013098 Sparsity_loss: 0.7553285247 KL_loss: 0.0488510805 MMD_loss: 0.0313291586 time: 0.7843s\n",
      "Feature: 0021 Epoch: 1800 Loss: 1.0691129867 MSE_Loss: 0.9682023672 Sparsity_loss: 0.7552818060 KL_loss: 0.0488440224 MMD_loss: 0.0313290425 time: 0.7843s\n",
      "Feature: 0021 Epoch: 1900 Loss: 1.0691108877 MSE_Loss: 0.9681950404 Sparsity_loss: 0.7553852683 KL_loss: 0.0488704791 MMD_loss: 0.0313289364 time: 0.7829s\n",
      "Begin training feature: 0022\n",
      "Feature: 0022 Epoch: 0000 Loss: 2.1589661459 MSE_Loss: 1.6570734306 Sparsity_loss: 8.2826188848 KL_loss: 0.0684655280 MMD_loss: 0.0435385632 time: 0.7843s\n",
      "Feature: 0022 Epoch: 0100 Loss: 1.2838161837 MSE_Loss: 1.1716150913 Sparsity_loss: 0.7550116257 KL_loss: 0.0487771677 MMD_loss: 0.0369813620 time: 0.7846s\n",
      "Feature: 0022 Epoch: 0200 Loss: 1.2838021170 MSE_Loss: 1.1715747412 Sparsity_loss: 0.7555260304 KL_loss: 0.0488912733 MMD_loss: 0.0369810828 time: 0.7849s\n",
      "Feature: 0022 Epoch: 0300 Loss: 1.2837942839 MSE_Loss: 1.1715748204 Sparsity_loss: 0.7553752510 KL_loss: 0.0488639737 MMD_loss: 0.0369810256 time: 0.7856s\n",
      "Feature: 0022 Epoch: 0400 Loss: 1.2837942499 MSE_Loss: 1.1715730150 Sparsity_loss: 0.7554155596 KL_loss: 0.0488813105 MMD_loss: 0.0369808259 time: 0.7834s\n",
      "Feature: 0022 Epoch: 0500 Loss: 1.2925796479 MSE_Loss: 1.1845989386 Sparsity_loss: 0.6738839625 KL_loss: 0.0463096463 MMD_loss: 0.0369117083 time: 0.8162s\n",
      "Feature: 0022 Epoch: 0600 Loss: 1.2925763983 MSE_Loss: 1.1845959448 Sparsity_loss: 0.6738878164 KL_loss: 0.0463104955 MMD_loss: 0.0369114808 time: 0.8191s\n",
      "Feature: 0022 Epoch: 0700 Loss: 1.2925830068 MSE_Loss: 1.1846009327 Sparsity_loss: 0.6739150919 KL_loss: 0.0463115375 MMD_loss: 0.0369116077 time: 0.8194s\n",
      "Feature: 0022 Epoch: 0800 Loss: 1.2925824229 MSE_Loss: 1.1846005720 Sparsity_loss: 0.6739077100 KL_loss: 0.0463100034 MMD_loss: 0.0369116766 time: 0.8206s\n",
      "Feature: 0022 Epoch: 0900 Loss: 1.2925791461 MSE_Loss: 1.1845965189 Sparsity_loss: 0.6739250384 KL_loss: 0.0463095113 MMD_loss: 0.0369116472 time: 0.8198s\n",
      "Feature: 0022 Epoch: 1000 Loss: 1.2925823014 MSE_Loss: 1.1846009433 Sparsity_loss: 0.6738988229 KL_loss: 0.0463090670 MMD_loss: 0.0369116662 time: 0.8196s\n",
      "Feature: 0022 Epoch: 1100 Loss: 1.2925791665 MSE_Loss: 1.1845983425 Sparsity_loss: 0.6738902549 KL_loss: 0.0463096195 MMD_loss: 0.0369116153 time: 0.8195s\n",
      "Feature: 0022 Epoch: 1200 Loss: 1.2925762081 MSE_Loss: 1.1845959561 Sparsity_loss: 0.6738820551 KL_loss: 0.0463108758 MMD_loss: 0.0369115247 time: 0.8190s\n",
      "Feature: 0022 Epoch: 1300 Loss: 1.2925757638 MSE_Loss: 1.1845948017 Sparsity_loss: 0.6738957574 KL_loss: 0.0463112745 MMD_loss: 0.0369115255 time: 0.8212s\n",
      "Feature: 0022 Epoch: 1400 Loss: 1.2925799881 MSE_Loss: 1.1845987583 Sparsity_loss: 0.6738960441 KL_loss: 0.0463101510 MMD_loss: 0.0369116692 time: 0.8204s\n",
      "Feature: 0022 Epoch: 1500 Loss: 1.2925759388 MSE_Loss: 1.1845950235 Sparsity_loss: 0.6738908887 KL_loss: 0.0463113949 MMD_loss: 0.0369116266 time: 0.8219s\n",
      "Feature: 0022 Epoch: 1600 Loss: 1.2925805261 MSE_Loss: 1.1845984512 Sparsity_loss: 0.6739093933 KL_loss: 0.0463108528 MMD_loss: 0.0369117488 time: 0.8192s\n",
      "Feature: 0022 Epoch: 1700 Loss: 1.2925773391 MSE_Loss: 1.1845969301 Sparsity_loss: 0.6738825350 KL_loss: 0.0463101655 MMD_loss: 0.0369115951 time: 0.8196s\n",
      "Feature: 0022 Epoch: 1800 Loss: 1.2925747565 MSE_Loss: 1.1845948545 Sparsity_loss: 0.6738792575 KL_loss: 0.0463120689 MMD_loss: 0.0369114127 time: 0.8210s\n",
      "Feature: 0022 Epoch: 1900 Loss: 1.2925802311 MSE_Loss: 1.1845993226 Sparsity_loss: 0.6738900648 KL_loss: 0.0463106727 MMD_loss: 0.0369116477 time: 0.8207s\n",
      "Begin training feature: 0023\n",
      "Feature: 0023 Epoch: 0000 Loss: 1.7999409648 MSE_Loss: 1.3063081062 Sparsity_loss: 8.2858652284 KL_loss: 0.0701869600 MMD_loss: 0.0393188617 time: 0.8208s\n",
      "Feature: 0023 Epoch: 0100 Loss: 1.1171492685 MSE_Loss: 0.9500833772 Sparsity_loss: 1.9806396991 KL_loss: 0.0692795488 MMD_loss: 0.0336705588 time: 0.8161s\n",
      "Feature: 0023 Epoch: 0200 Loss: 1.1100590237 MSE_Loss: 0.9511419195 Sparsity_loss: 1.8174821485 KL_loss: 0.0664580562 MMD_loss: 0.0336892030 time: 0.8148s\n",
      "Feature: 0023 Epoch: 0300 Loss: 1.0897812670 MSE_Loss: 0.9553410230 Sparsity_loss: 1.3277010556 KL_loss: 0.0579139070 MMD_loss: 0.0337380271 time: 0.8144s\n",
      "Feature: 0023 Epoch: 0400 Loss: 1.0897786285 MSE_Loss: 0.9553399856 Sparsity_loss: 1.3276682473 KL_loss: 0.0579121348 MMD_loss: 0.0337380550 time: 0.8145s\n",
      "Feature: 0023 Epoch: 0500 Loss: 1.0897758980 MSE_Loss: 0.9553363587 Sparsity_loss: 1.3276956398 KL_loss: 0.0579144508 MMD_loss: 0.0337378071 time: 0.8150s\n",
      "Feature: 0023 Epoch: 0600 Loss: 1.0897804702 MSE_Loss: 0.9553401689 Sparsity_loss: 1.3277018447 KL_loss: 0.0579107589 MMD_loss: 0.0337380505 time: 0.8155s\n",
      "Feature: 0023 Epoch: 0700 Loss: 1.0897671671 MSE_Loss: 0.9553298513 Sparsity_loss: 1.3276537838 KL_loss: 0.0579212683 MMD_loss: 0.0337377049 time: 0.8156s\n",
      "Feature: 0023 Epoch: 0800 Loss: 1.0897705472 MSE_Loss: 0.9553316877 Sparsity_loss: 1.3276800294 KL_loss: 0.0579181961 MMD_loss: 0.0337378332 time: 0.8135s\n",
      "Feature: 0023 Epoch: 0900 Loss: 1.0897727903 MSE_Loss: 0.9553352059 Sparsity_loss: 1.3276512049 KL_loss: 0.0579129989 MMD_loss: 0.0337379475 time: 0.8144s\n",
      "Feature: 0023 Epoch: 1000 Loss: 1.0897714609 MSE_Loss: 0.9553337490 Sparsity_loss: 1.3276510465 KL_loss: 0.0579137095 MMD_loss: 0.0337380109 time: 0.8133s\n",
      "Feature: 0023 Epoch: 1100 Loss: 1.0897720554 MSE_Loss: 0.9553336169 Sparsity_loss: 1.3276744176 KL_loss: 0.0579163197 MMD_loss: 0.0337377781 time: 0.8148s\n",
      "Feature: 0023 Epoch: 1200 Loss: 1.0897721709 MSE_Loss: 0.9553344982 Sparsity_loss: 1.3276589022 KL_loss: 0.0579140004 MMD_loss: 0.0337377970 time: 0.8157s\n",
      "Feature: 0023 Epoch: 1300 Loss: 1.0897740737 MSE_Loss: 0.9553359815 Sparsity_loss: 1.3276618206 KL_loss: 0.0579170709 MMD_loss: 0.0337379132 time: 0.8157s\n",
      "Feature: 0023 Epoch: 1400 Loss: 1.0897699783 MSE_Loss: 0.9553316537 Sparsity_loss: 1.3276729222 KL_loss: 0.0579156206 MMD_loss: 0.0337377589 time: 0.8138s\n",
      "Feature: 0023 Epoch: 1500 Loss: 1.0897717778 MSE_Loss: 0.9553342680 Sparsity_loss: 1.3276575894 KL_loss: 0.0579140084 MMD_loss: 0.0337377419 time: 0.8141s\n",
      "Feature: 0023 Epoch: 1600 Loss: 1.0897698380 MSE_Loss: 0.9553310713 Sparsity_loss: 1.3276785431 KL_loss: 0.0579192413 MMD_loss: 0.0337378237 time: 0.8142s\n",
      "Feature: 0023 Epoch: 1700 Loss: 1.0897713666 MSE_Loss: 0.9553329386 Sparsity_loss: 1.3276732949 KL_loss: 0.0579172577 MMD_loss: 0.0337377962 time: 0.8150s\n",
      "Feature: 0023 Epoch: 1800 Loss: 1.0897974764 MSE_Loss: 0.9553538470 Sparsity_loss: 1.3277672363 KL_loss: 0.0579012785 MMD_loss: 0.0337381278 time: 0.8153s\n",
      "Feature: 0023 Epoch: 1900 Loss: 1.0897740292 MSE_Loss: 0.9553348083 Sparsity_loss: 1.3276817452 KL_loss: 0.0579166244 MMD_loss: 0.0337379818 time: 0.8150s\n",
      "Begin training feature: 0024\n",
      "Feature: 0024 Epoch: 0000 Loss: 1.8632320196 MSE_Loss: 1.3641596987 Sparsity_loss: 8.3006258011 KL_loss: 0.0862707065 MMD_loss: 0.0415891664 time: 0.8142s\n",
      "Feature: 0024 Epoch: 0100 Loss: 1.1948549989 MSE_Loss: 1.0727146396 Sparsity_loss: 0.9388298241 KL_loss: 0.0523125434 MMD_loss: 0.0373378700 time: 0.8199s\n",
      "Feature: 0024 Epoch: 0200 Loss: 1.1920147644 MSE_Loss: 1.0734082272 Sparsity_loss: 0.8680675950 KL_loss: 0.0497495639 MMD_loss: 0.0373528292 time: 0.8196s\n",
      "Feature: 0024 Epoch: 0300 Loss: 1.1920689480 MSE_Loss: 1.0734462127 Sparsity_loss: 0.8683604123 KL_loss: 0.0497524097 MMD_loss: 0.0373535924 time: 0.8217s\n",
      "Feature: 0024 Epoch: 0400 Loss: 1.1921010131 MSE_Loss: 1.0734426402 Sparsity_loss: 0.8691047573 KL_loss: 0.0497494058 MMD_loss: 0.0373528221 time: 0.8224s\n",
      "Feature: 0024 Epoch: 0500 Loss: 1.1920407754 MSE_Loss: 1.0734440096 Sparsity_loss: 0.8678745899 KL_loss: 0.0497590583 MMD_loss: 0.0373527236 time: 0.8216s\n",
      "Feature: 0024 Epoch: 0600 Loss: 1.1921004042 MSE_Loss: 1.0734251006 Sparsity_loss: 0.8694550652 KL_loss: 0.0497577535 MMD_loss: 0.0373524878 time: 0.8219s\n",
      "Feature: 0024 Epoch: 0700 Loss: 1.1919649885 MSE_Loss: 1.0733957992 Sparsity_loss: 0.8673619990 KL_loss: 0.0497671365 MMD_loss: 0.0373517058 time: 0.8205s\n",
      "Feature: 0024 Epoch: 0800 Loss: 1.1920212432 MSE_Loss: 1.0734455842 Sparsity_loss: 0.8674176720 KL_loss: 0.0497564479 MMD_loss: 0.0373536045 time: 0.8202s\n",
      "Feature: 0024 Epoch: 0900 Loss: 1.1920485557 MSE_Loss: 1.0734752402 Sparsity_loss: 0.8674343696 KL_loss: 0.0497524233 MMD_loss: 0.0373520366 time: 0.8137s\n",
      "Feature: 0024 Epoch: 1000 Loss: 1.1919511775 MSE_Loss: 1.0734130846 Sparsity_loss: 0.8667132402 KL_loss: 0.0497627988 MMD_loss: 0.0373524029 time: 0.8147s\n",
      "Feature: 0024 Epoch: 1100 Loss: 1.1919749334 MSE_Loss: 1.0734113153 Sparsity_loss: 0.8672074487 KL_loss: 0.0497558553 MMD_loss: 0.0373528443 time: 0.8145s\n",
      "Feature: 0024 Epoch: 1200 Loss: 1.1919692755 MSE_Loss: 1.0734117340 Sparsity_loss: 0.8671201286 KL_loss: 0.0497603101 MMD_loss: 0.0373519721 time: 0.8135s\n",
      "Feature: 0024 Epoch: 1300 Loss: 1.1919432735 MSE_Loss: 1.0734084656 Sparsity_loss: 0.8666424193 KL_loss: 0.0497620903 MMD_loss: 0.0373525338 time: 0.8139s\n",
      "Feature: 0024 Epoch: 1400 Loss: 1.1922688137 MSE_Loss: 1.0734962904 Sparsity_loss: 0.8713344388 KL_loss: 0.0497336133 MMD_loss: 0.0373542292 time: 0.8132s\n",
      "Feature: 0024 Epoch: 1500 Loss: 1.1921808946 MSE_Loss: 1.0734532302 Sparsity_loss: 0.8704866458 KL_loss: 0.0497492496 MMD_loss: 0.0373529171 time: 0.8137s\n",
      "Feature: 0024 Epoch: 1600 Loss: 1.1919332863 MSE_Loss: 1.0733856461 Sparsity_loss: 0.8668711766 KL_loss: 0.0497722264 MMD_loss: 0.0373531823 time: 0.8141s\n",
      "Feature: 0024 Epoch: 1700 Loss: 1.1921083437 MSE_Loss: 1.0734333969 Sparsity_loss: 0.8694829390 KL_loss: 0.0497538400 MMD_loss: 0.0373516303 time: 0.8144s\n",
      "Feature: 0024 Epoch: 1800 Loss: 1.1920184863 MSE_Loss: 1.0734438187 Sparsity_loss: 0.8674113894 KL_loss: 0.0497649263 MMD_loss: 0.0373532261 time: 0.8139s\n",
      "Feature: 0024 Epoch: 1900 Loss: 1.1920290039 MSE_Loss: 1.0734289824 Sparsity_loss: 0.8679300954 KL_loss: 0.0497514282 MMD_loss: 0.0373530024 time: 0.8128s\n",
      "Begin training feature: 0025\n",
      "Feature: 0025 Epoch: 0000 Loss: 2.0906002989 MSE_Loss: 1.5913235085 Sparsity_loss: 8.1807633412 KL_loss: 0.0747273136 MMD_loss: 0.0447456671 time: 0.8137s\n",
      "Feature: 0025 Epoch: 0100 Loss: 1.1594154578 MSE_Loss: 1.0379758002 Sparsity_loss: 1.0008508027 KL_loss: 0.0521147944 MMD_loss: 0.0354379830 time: 0.8135s\n",
      "Feature: 0025 Epoch: 0200 Loss: 1.1594232766 MSE_Loss: 1.0379826487 Sparsity_loss: 1.0008637905 KL_loss: 0.0521099378 MMD_loss: 0.0354381688 time: 0.8138s\n",
      "Feature: 0025 Epoch: 0300 Loss: 1.1594165322 MSE_Loss: 1.0379783519 Sparsity_loss: 1.0008204195 KL_loss: 0.0521139612 MMD_loss: 0.0354380096 time: 0.8149s\n",
      "Feature: 0025 Epoch: 0400 Loss: 1.1594203386 MSE_Loss: 1.0379792573 Sparsity_loss: 1.0008772566 KL_loss: 0.0521146024 MMD_loss: 0.0354380387 time: 0.8219s\n",
      "Feature: 0025 Epoch: 0500 Loss: 1.1594086078 MSE_Loss: 1.0379733700 Sparsity_loss: 1.0007693858 KL_loss: 0.0521124702 MMD_loss: 0.0354378189 time: 0.8218s\n",
      "Feature: 0025 Epoch: 0600 Loss: 1.1537161560 MSE_Loss: 1.0404034151 Sparsity_loss: 0.8373757125 KL_loss: 0.0492215232 MMD_loss: 0.0354758718 time: 0.8138s\n",
      "Feature: 0025 Epoch: 0700 Loss: 1.1537188790 MSE_Loss: 1.0404057103 Sparsity_loss: 0.8373834378 KL_loss: 0.0492222317 MMD_loss: 0.0354758835 time: 0.8133s\n",
      "Feature: 0025 Epoch: 0800 Loss: 1.1537136202 MSE_Loss: 1.0404018360 Sparsity_loss: 0.8373619974 KL_loss: 0.0492211023 MMD_loss: 0.0354757350 time: 0.8213s\n",
      "Feature: 0025 Epoch: 0900 Loss: 1.1537182557 MSE_Loss: 1.0404054953 Sparsity_loss: 0.8373747076 KL_loss: 0.0492219642 MMD_loss: 0.0354759005 time: 0.8217s\n",
      "Feature: 0025 Epoch: 1000 Loss: 1.1537222508 MSE_Loss: 1.0404092956 Sparsity_loss: 0.8373741598 KL_loss: 0.0492193011 MMD_loss: 0.0354760259 time: 0.8221s\n",
      "Feature: 0025 Epoch: 1100 Loss: 1.1537181841 MSE_Loss: 1.0404051203 Sparsity_loss: 0.8373760423 KL_loss: 0.0492190876 MMD_loss: 0.0354760382 time: 0.8206s\n",
      "Feature: 0025 Epoch: 1200 Loss: 1.1537154905 MSE_Loss: 1.0404040512 Sparsity_loss: 0.8373458733 KL_loss: 0.0492189776 MMD_loss: 0.0354759776 time: 0.8207s\n",
      "Feature: 0025 Epoch: 1300 Loss: 1.1537162948 MSE_Loss: 1.0404045137 Sparsity_loss: 0.8373545438 KL_loss: 0.0492204025 MMD_loss: 0.0354759254 time: 0.8209s\n",
      "Feature: 0025 Epoch: 1400 Loss: 1.1537197775 MSE_Loss: 1.0404077052 Sparsity_loss: 0.8373563056 KL_loss: 0.0492197278 MMD_loss: 0.0354760316 time: 0.8198s\n",
      "Feature: 0025 Epoch: 1500 Loss: 1.1537110436 MSE_Loss: 1.0403999784 Sparsity_loss: 0.8373429006 KL_loss: 0.0492227267 MMD_loss: 0.0354758503 time: 0.8193s\n",
      "Feature: 0025 Epoch: 1600 Loss: 1.1537133893 MSE_Loss: 1.0404018134 Sparsity_loss: 0.8373544495 KL_loss: 0.0492200226 MMD_loss: 0.0354758260 time: 0.8207s\n",
      "Feature: 0025 Epoch: 1700 Loss: 1.1537187514 MSE_Loss: 1.0404056009 Sparsity_loss: 0.8373893152 KL_loss: 0.0492215494 MMD_loss: 0.0354757318 time: 0.8195s\n",
      "Feature: 0025 Epoch: 1800 Loss: 1.1537185787 MSE_Loss: 1.0404066489 Sparsity_loss: 0.8373598788 KL_loss: 0.0492185705 MMD_loss: 0.0354758724 time: 0.8189s\n",
      "Feature: 0025 Epoch: 1900 Loss: 1.1537202144 MSE_Loss: 1.0404071687 Sparsity_loss: 0.8373760181 KL_loss: 0.0492221631 MMD_loss: 0.0354760146 time: 0.8191s\n",
      "Begin training feature: 0026\n",
      "Feature: 0026 Epoch: 0000 Loss: 2.0614088230 MSE_Loss: 1.5692252645 Sparsity_loss: 8.2281874765 KL_loss: 0.0679668146 MMD_loss: 0.0400472461 time: 0.8199s\n",
      "Feature: 0026 Epoch: 0100 Loss: 1.2873831995 MSE_Loss: 1.1878136781 Sparsity_loss: 0.5717269741 KL_loss: 0.0453699549 MMD_loss: 0.0352647391 time: 0.8206s\n",
      "Feature: 0026 Epoch: 0200 Loss: 1.2873996904 MSE_Loss: 1.1878280413 Sparsity_loss: 0.5717606997 KL_loss: 0.0453726802 MMD_loss: 0.0352649394 time: 0.8210s\n",
      "Feature: 0026 Epoch: 0300 Loss: 1.2873816807 MSE_Loss: 1.1878092198 Sparsity_loss: 0.5717821853 KL_loss: 0.0453744156 MMD_loss: 0.0352648055 time: 0.8194s\n",
      "Feature: 0026 Epoch: 0400 Loss: 1.2873843938 MSE_Loss: 1.1878148521 Sparsity_loss: 0.5717227497 KL_loss: 0.0453732746 MMD_loss: 0.0352648402 time: 0.8194s\n",
      "Feature: 0026 Epoch: 0500 Loss: 1.2873760107 MSE_Loss: 1.1878060035 Sparsity_loss: 0.5717357488 KL_loss: 0.0453747324 MMD_loss: 0.0352647455 time: 0.8193s\n",
      "Feature: 0026 Epoch: 0600 Loss: 1.2873813774 MSE_Loss: 1.1878113550 Sparsity_loss: 0.5717371506 KL_loss: 0.0453720288 MMD_loss: 0.0352647244 time: 0.8195s\n",
      "Feature: 0026 Epoch: 0700 Loss: 1.2873784756 MSE_Loss: 1.1878090026 Sparsity_loss: 0.5717216760 KL_loss: 0.0453737027 MMD_loss: 0.0352648278 time: 0.8204s\n",
      "Feature: 0026 Epoch: 0800 Loss: 1.2873793855 MSE_Loss: 1.1878097238 Sparsity_loss: 0.5717244895 KL_loss: 0.0453733834 MMD_loss: 0.0352648548 time: 0.8203s\n",
      "Feature: 0026 Epoch: 0900 Loss: 1.2873805897 MSE_Loss: 1.1878092334 Sparsity_loss: 0.5717632340 KL_loss: 0.0453746580 MMD_loss: 0.0352647263 time: 0.8203s\n",
      "Feature: 0026 Epoch: 1000 Loss: 1.2873814589 MSE_Loss: 1.1878132337 Sparsity_loss: 0.5717028500 KL_loss: 0.0453715075 MMD_loss: 0.0352646825 time: 0.8195s\n",
      "Feature: 0026 Epoch: 1100 Loss: 1.2873785503 MSE_Loss: 1.1878102135 Sparsity_loss: 0.5717036460 KL_loss: 0.0453689568 MMD_loss: 0.0352647369 time: 0.8195s\n",
      "Feature: 0026 Epoch: 1200 Loss: 1.2873787058 MSE_Loss: 1.1878105606 Sparsity_loss: 0.5717010302 KL_loss: 0.0453684628 MMD_loss: 0.0352647033 time: 0.8199s\n",
      "Feature: 0026 Epoch: 1300 Loss: 1.2873828117 MSE_Loss: 1.1878118824 Sparsity_loss: 0.5717490572 KL_loss: 0.0453681015 MMD_loss: 0.0352648946 time: 0.8204s\n",
      "Feature: 0026 Epoch: 1400 Loss: 1.2873925069 MSE_Loss: 1.1878181477 Sparsity_loss: 0.5718163606 KL_loss: 0.0453675367 MMD_loss: 0.0352649332 time: 0.8205s\n",
      "Feature: 0026 Epoch: 1500 Loss: 1.2874087314 MSE_Loss: 1.1878283311 Sparsity_loss: 0.5719214180 KL_loss: 0.0453763781 MMD_loss: 0.0352652824 time: 0.8198s\n",
      "Feature: 0026 Epoch: 1600 Loss: 1.2958318991 MSE_Loss: 1.1995693792 Sparsity_loss: 0.5104915039 KL_loss: 0.0434071585 MMD_loss: 0.0351519370 time: 0.8200s\n",
      "Feature: 0026 Epoch: 1700 Loss: 1.2958253894 MSE_Loss: 1.1995650477 Sparsity_loss: 0.5104572433 KL_loss: 0.0434074419 MMD_loss: 0.0351517087 time: 0.8194s\n",
      "Feature: 0026 Epoch: 1800 Loss: 1.2958221428 MSE_Loss: 1.1995622659 Sparsity_loss: 0.5104465273 KL_loss: 0.0434074801 MMD_loss: 0.0351517432 time: 0.8189s\n",
      "Feature: 0026 Epoch: 1900 Loss: 1.2958184926 MSE_Loss: 1.1995591076 Sparsity_loss: 0.5104350025 KL_loss: 0.0434078474 MMD_loss: 0.0351517864 time: 0.8204s\n",
      "Begin training feature: 0027\n",
      "Feature: 0027 Epoch: 0000 Loss: 2.1340008416 MSE_Loss: 1.6315129414 Sparsity_loss: 8.3476916205 KL_loss: 0.0791458369 MMD_loss: 0.0421559284 time: 0.8192s\n",
      "Feature: 0027 Epoch: 0100 Loss: 1.3595904972 MSE_Loss: 1.2358229900 Sparsity_loss: 1.0007961008 KL_loss: 0.0521037289 MMD_loss: 0.0366033301 time: 0.7852s\n",
      "Feature: 0027 Epoch: 0200 Loss: 1.3595921231 MSE_Loss: 1.2358248279 Sparsity_loss: 1.0007902655 KL_loss: 0.0521064490 MMD_loss: 0.0366033597 time: 0.7834s\n",
      "Feature: 0027 Epoch: 0300 Loss: 1.3595892334 MSE_Loss: 1.2358224928 Sparsity_loss: 1.0007878678 KL_loss: 0.0521026559 MMD_loss: 0.0366031582 time: 0.7826s\n",
      "Feature: 0027 Epoch: 0400 Loss: 1.3595885355 MSE_Loss: 1.2358224377 Sparsity_loss: 1.0007745058 KL_loss: 0.0521052306 MMD_loss: 0.0366031643 time: 0.7832s\n",
      "Feature: 0027 Epoch: 0500 Loss: 1.3595935514 MSE_Loss: 1.2358244522 Sparsity_loss: 1.0008307771 KL_loss: 0.0521052871 MMD_loss: 0.0366032516 time: 0.7833s\n",
      "Feature: 0027 Epoch: 0600 Loss: 1.3595895013 MSE_Loss: 1.2358239029 Sparsity_loss: 1.0007630572 KL_loss: 0.0521045935 MMD_loss: 0.0366032020 time: 0.7836s\n",
      "Feature: 0027 Epoch: 0700 Loss: 1.3595899902 MSE_Loss: 1.2358241738 Sparsity_loss: 1.0007673789 KL_loss: 0.0521047554 MMD_loss: 0.0366032009 time: 0.7843s\n",
      "Feature: 0027 Epoch: 0800 Loss: 1.3595836140 MSE_Loss: 1.2358186630 Sparsity_loss: 1.0007570801 KL_loss: 0.0521064666 MMD_loss: 0.0366030171 time: 0.7841s\n",
      "Feature: 0027 Epoch: 0900 Loss: 1.3595947208 MSE_Loss: 1.2358273570 Sparsity_loss: 1.0007958005 KL_loss: 0.0521032747 MMD_loss: 0.0366032710 time: 0.7830s\n",
      "Feature: 0027 Epoch: 1000 Loss: 1.3595868123 MSE_Loss: 1.2358208948 Sparsity_loss: 1.0007642326 KL_loss: 0.0521039801 MMD_loss: 0.0366033274 time: 0.7834s\n",
      "Feature: 0027 Epoch: 1100 Loss: 1.3595838675 MSE_Loss: 1.2358198007 Sparsity_loss: 1.0007420476 KL_loss: 0.0521053219 MMD_loss: 0.0366029583 time: 0.7845s\n",
      "Feature: 0027 Epoch: 1200 Loss: 1.3595917904 MSE_Loss: 1.2358246566 Sparsity_loss: 1.0007868779 KL_loss: 0.0521032593 MMD_loss: 0.0366033782 time: 0.7840s\n",
      "Feature: 0027 Epoch: 1300 Loss: 1.3595977003 MSE_Loss: 1.2358317677 Sparsity_loss: 1.0007663829 KL_loss: 0.0520980369 MMD_loss: 0.0366033103 time: 0.7848s\n",
      "Feature: 0027 Epoch: 1400 Loss: 1.3595842621 MSE_Loss: 1.2358194114 Sparsity_loss: 1.0007505990 KL_loss: 0.0521072560 MMD_loss: 0.0366031175 time: 0.7841s\n",
      "Feature: 0027 Epoch: 1500 Loss: 1.3595946891 MSE_Loss: 1.2358284442 Sparsity_loss: 1.0007786298 KL_loss: 0.0521054261 MMD_loss: 0.0366031269 time: 0.7829s\n",
      "Feature: 0027 Epoch: 1600 Loss: 1.3595873397 MSE_Loss: 1.2358215534 Sparsity_loss: 1.0007640531 KL_loss: 0.0521062151 MMD_loss: 0.0366032655 time: 0.7832s\n",
      "Feature: 0027 Epoch: 1700 Loss: 1.3595861430 MSE_Loss: 1.2358204526 Sparsity_loss: 1.0007644771 KL_loss: 0.0521068809 MMD_loss: 0.0366031999 time: 0.7832s\n",
      "Feature: 0027 Epoch: 1800 Loss: 1.3595891542 MSE_Loss: 1.2358233461 Sparsity_loss: 1.0007657039 KL_loss: 0.0521050908 MMD_loss: 0.0366032327 time: 0.7842s\n",
      "Feature: 0027 Epoch: 1900 Loss: 1.3595894258 MSE_Loss: 1.2358237399 Sparsity_loss: 1.0007616885 KL_loss: 0.0521057151 MMD_loss: 0.0366032725 time: 0.7855s\n",
      "Begin training feature: 0028\n",
      "Feature: 0028 Epoch: 0000 Loss: 1.9926173053 MSE_Loss: 1.4868129741 Sparsity_loss: 8.4094404631 KL_loss: 0.0877051316 MMD_loss: 0.0422276274 time: 0.7836s\n",
      "Feature: 0028 Epoch: 0100 Loss: 1.2871171870 MSE_Loss: 1.0988044369 Sparsity_loss: 2.3073906174 KL_loss: 0.0750298211 MMD_loss: 0.0360964627 time: 0.7827s\n",
      "Feature: 0028 Epoch: 0200 Loss: 1.2871217501 MSE_Loss: 1.0988155784 Sparsity_loss: 2.3072422939 KL_loss: 0.0750119594 MMD_loss: 0.0360969743 time: 0.7822s\n",
      "Feature: 0028 Epoch: 0300 Loss: 1.2871266830 MSE_Loss: 1.0988177657 Sparsity_loss: 2.3072997799 KL_loss: 0.0750203732 MMD_loss: 0.0360968607 time: 0.7823s\n",
      "Feature: 0028 Epoch: 0400 Loss: 1.2871170285 MSE_Loss: 1.0988109874 Sparsity_loss: 2.3072431419 KL_loss: 0.0750161540 MMD_loss: 0.0360968611 time: 0.7829s\n",
      "Feature: 0028 Epoch: 0500 Loss: 1.2871139148 MSE_Loss: 1.0988102253 Sparsity_loss: 2.3072031148 KL_loss: 0.0750158528 MMD_loss: 0.0360966827 time: 0.7829s\n",
      "Feature: 0028 Epoch: 0600 Loss: 1.2871196926 MSE_Loss: 1.0988164220 Sparsity_loss: 2.3071892956 KL_loss: 0.0750138813 MMD_loss: 0.0360968290 time: 0.7841s\n",
      "Feature: 0028 Epoch: 0700 Loss: 1.2803659756 MSE_Loss: 1.1002165607 Sparsity_loss: 2.1439248067 KL_loss: 0.0721562986 MMD_loss: 0.0361158037 time: 0.7833s\n",
      "Feature: 0028 Epoch: 0800 Loss: 1.2803660435 MSE_Loss: 1.1002216460 Sparsity_loss: 2.1438194770 KL_loss: 0.0721322077 MMD_loss: 0.0361160564 time: 0.7827s\n",
      "Feature: 0028 Epoch: 0900 Loss: 1.2803659039 MSE_Loss: 1.1002181384 Sparsity_loss: 2.1438866476 KL_loss: 0.0721521223 MMD_loss: 0.0361159493 time: 0.7823s\n",
      "Feature: 0028 Epoch: 1000 Loss: 1.2734222284 MSE_Loss: 1.1014141229 Sparsity_loss: 1.9806905276 KL_loss: 0.0693114757 MMD_loss: 0.0361402333 time: 0.7832s\n",
      "Feature: 0028 Epoch: 1100 Loss: 1.2734178101 MSE_Loss: 1.1014077565 Sparsity_loss: 1.9807279774 KL_loss: 0.0693270365 MMD_loss: 0.0361401903 time: 0.7836s\n",
      "Feature: 0028 Epoch: 1200 Loss: 1.2734177294 MSE_Loss: 1.1014107760 Sparsity_loss: 1.9806707554 KL_loss: 0.0693112225 MMD_loss: 0.0361401499 time: 0.7838s\n",
      "Feature: 0028 Epoch: 1300 Loss: 1.2734209246 MSE_Loss: 1.1014102320 Sparsity_loss: 1.9807437945 KL_loss: 0.0693308673 MMD_loss: 0.0361401039 time: 0.7831s\n",
      "Feature: 0028 Epoch: 1400 Loss: 1.2734196412 MSE_Loss: 1.1014101800 Sparsity_loss: 1.9807170448 KL_loss: 0.0693229574 MMD_loss: 0.0361401937 time: 0.7826s\n",
      "Feature: 0028 Epoch: 1500 Loss: 1.2734280183 MSE_Loss: 1.1014196428 Sparsity_loss: 1.9806986368 KL_loss: 0.0693167500 MMD_loss: 0.0361401375 time: 0.7822s\n",
      "Feature: 0028 Epoch: 1600 Loss: 1.2734153987 MSE_Loss: 1.1014009186 Sparsity_loss: 1.9808174734 KL_loss: 0.0693538052 MMD_loss: 0.0361400379 time: 0.7818s\n",
      "Feature: 0028 Epoch: 1700 Loss: 1.2664503832 MSE_Loss: 1.1025775656 Sparsity_loss: 1.8175776744 KL_loss: 0.0664996769 MMD_loss: 0.0361644690 time: 0.7827s\n",
      "Feature: 0028 Epoch: 1800 Loss: 1.2664470514 MSE_Loss: 1.1025765787 Sparsity_loss: 1.8175349658 KL_loss: 0.0664977104 MMD_loss: 0.0361643750 time: 0.7842s\n",
      "Feature: 0028 Epoch: 1900 Loss: 1.2664475267 MSE_Loss: 1.1025772215 Sparsity_loss: 1.8175358938 KL_loss: 0.0664989535 MMD_loss: 0.0361642630 time: 0.7840s\n",
      "Begin training feature: 0029\n",
      "Feature: 0029 Epoch: 0000 Loss: 2.0491838274 MSE_Loss: 1.5532606244 Sparsity_loss: 8.2116896593 KL_loss: 0.0738003233 MMD_loss: 0.0423003505 time: 0.7826s\n",
      "Feature: 0029 Epoch: 0100 Loss: 1.3986028946 MSE_Loss: 1.2542137466 Sparsity_loss: 1.3273880618 KL_loss: 0.0578238028 MMD_loss: 0.0387207581 time: 0.7819s\n",
      "Feature: 0029 Epoch: 0200 Loss: 1.3918575690 MSE_Loss: 1.2556500465 Sparsity_loss: 1.1641286898 KL_loss: 0.0549654894 MMD_loss: 0.0387257162 time: 0.7818s\n",
      "Feature: 0029 Epoch: 0300 Loss: 1.3918500060 MSE_Loss: 1.2556448534 Sparsity_loss: 1.1640820051 KL_loss: 0.0549713052 MMD_loss: 0.0387256668 time: 0.7824s\n",
      "Feature: 0029 Epoch: 0400 Loss: 1.3918524151 MSE_Loss: 1.2556462620 Sparsity_loss: 1.1641022812 KL_loss: 0.0549693527 MMD_loss: 0.0387256838 time: 0.7834s\n",
      "Feature: 0029 Epoch: 0500 Loss: 1.3918518681 MSE_Loss: 1.2556466543 Sparsity_loss: 1.1640884786 KL_loss: 0.0549695098 MMD_loss: 0.0387255472 time: 0.7833s\n",
      "Feature: 0029 Epoch: 0600 Loss: 1.3918474407 MSE_Loss: 1.2556426231 Sparsity_loss: 1.1640731036 KL_loss: 0.0549677728 MMD_loss: 0.0387257426 time: 0.7822s\n",
      "Feature: 0029 Epoch: 0700 Loss: 1.3918501176 MSE_Loss: 1.2556440830 Sparsity_loss: 1.1641035352 KL_loss: 0.0549703882 MMD_loss: 0.0387255751 time: 0.7821s\n",
      "Feature: 0029 Epoch: 0800 Loss: 1.3918576331 MSE_Loss: 1.2556497130 Sparsity_loss: 1.1641287758 KL_loss: 0.0549719941 MMD_loss: 0.0387258890 time: 0.7816s\n",
      "Feature: 0029 Epoch: 0900 Loss: 1.3918506337 MSE_Loss: 1.2556448473 Sparsity_loss: 1.1640979399 KL_loss: 0.0549725639 MMD_loss: 0.0387255793 time: 0.7827s\n",
      "Feature: 0029 Epoch: 1000 Loss: 1.3918483008 MSE_Loss: 1.2556429015 Sparsity_loss: 1.1640882417 KL_loss: 0.0549748589 MMD_loss: 0.0387256181 time: 0.7822s\n",
      "Feature: 0029 Epoch: 1100 Loss: 1.3918534223 MSE_Loss: 1.2556508930 Sparsity_loss: 1.1640319372 KL_loss: 0.0549603691 MMD_loss: 0.0387256702 time: 0.7834s\n",
      "Feature: 0029 Epoch: 1200 Loss: 1.3918502044 MSE_Loss: 1.2556449763 Sparsity_loss: 1.1640818119 KL_loss: 0.0549688533 MMD_loss: 0.0387257253 time: 0.7831s\n",
      "Feature: 0029 Epoch: 1300 Loss: 1.3918555892 MSE_Loss: 1.2556503475 Sparsity_loss: 1.1640838611 KL_loss: 0.0549666933 MMD_loss: 0.0387256883 time: 0.7815s\n",
      "Feature: 0029 Epoch: 1400 Loss: 1.3918538916 MSE_Loss: 1.2556483896 Sparsity_loss: 1.1640906711 KL_loss: 0.0549693764 MMD_loss: 0.0387256396 time: 0.7821s\n",
      "Feature: 0029 Epoch: 1500 Loss: 1.3856463010 MSE_Loss: 1.2576201852 Sparsity_loss: 1.0006960736 KL_loss: 0.0520844194 MMD_loss: 0.0387352299 time: 0.7820s\n",
      "Feature: 0029 Epoch: 1600 Loss: 1.3856439221 MSE_Loss: 1.2576173665 Sparsity_loss: 1.0007021186 KL_loss: 0.0520873365 MMD_loss: 0.0387352877 time: 0.7830s\n",
      "Feature: 0029 Epoch: 1700 Loss: 1.3856490202 MSE_Loss: 1.2576227377 Sparsity_loss: 1.0006891610 KL_loss: 0.0520863126 MMD_loss: 0.0387354808 time: 0.7838s\n",
      "Feature: 0029 Epoch: 1800 Loss: 1.3856457615 MSE_Loss: 1.2576193644 Sparsity_loss: 1.0006950083 KL_loss: 0.0520895445 MMD_loss: 0.0387353733 time: 0.7838s\n",
      "Feature: 0029 Epoch: 1900 Loss: 1.3856464655 MSE_Loss: 1.2576207119 Sparsity_loss: 1.0006833514 KL_loss: 0.0520859953 MMD_loss: 0.0387353646 time: 0.7829s\n",
      "Begin training feature: 0030\n",
      "Feature: 0030 Epoch: 0000 Loss: 1.8874735070 MSE_Loss: 1.3884604392 Sparsity_loss: 8.3834855406 KL_loss: 0.0803769536 MMD_loss: 0.0395175088 time: 0.7820s\n",
      "Feature: 0030 Epoch: 0100 Loss: 1.1693953936 MSE_Loss: 1.0537569564 Sparsity_loss: 0.9392347358 KL_loss: 0.0524032361 MMD_loss: 0.0340763355 time: 0.7817s\n",
      "Feature: 0030 Epoch: 0200 Loss: 1.1693928234 MSE_Loss: 1.0537644176 Sparsity_loss: 0.9390397713 KL_loss: 0.0523528539 MMD_loss: 0.0340764415 time: 0.7821s\n",
      "Feature: 0030 Epoch: 0300 Loss: 1.1693925816 MSE_Loss: 1.0537747039 Sparsity_loss: 0.9388509906 KL_loss: 0.0523175675 MMD_loss: 0.0340760722 time: 0.7825s\n",
      "Feature: 0030 Epoch: 0400 Loss: 1.1693975974 MSE_Loss: 1.0537650540 Sparsity_loss: 0.9391251470 KL_loss: 0.0523692650 MMD_loss: 0.0340762944 time: 0.7836s\n",
      "Feature: 0030 Epoch: 0500 Loss: 1.1693914548 MSE_Loss: 1.0537592588 Sparsity_loss: 0.9391224271 KL_loss: 0.0523852043 MMD_loss: 0.0340761129 time: 0.7821s\n",
      "Feature: 0030 Epoch: 0600 Loss: 1.1693831660 MSE_Loss: 1.0537483900 Sparsity_loss: 0.9391679470 KL_loss: 0.0523995581 MMD_loss: 0.0340761850 time: 0.7810s\n",
      "Feature: 0030 Epoch: 0700 Loss: 1.1693899152 MSE_Loss: 1.0537548590 Sparsity_loss: 0.9391746310 KL_loss: 0.0523994485 MMD_loss: 0.0340761706 time: 0.7811s\n",
      "Feature: 0030 Epoch: 0800 Loss: 1.1693780226 MSE_Loss: 1.0537578407 Sparsity_loss: 0.9388844454 KL_loss: 0.0523451122 MMD_loss: 0.0340762499 time: 0.7806s\n",
      "Feature: 0030 Epoch: 0900 Loss: 1.1693836171 MSE_Loss: 1.0537434243 Sparsity_loss: 0.9392678481 KL_loss: 0.0524210983 MMD_loss: 0.0340762997 time: 0.7816s\n",
      "Feature: 0030 Epoch: 1000 Loss: 1.1693938046 MSE_Loss: 1.0537684458 Sparsity_loss: 0.9389816960 KL_loss: 0.0523454409 MMD_loss: 0.0340764057 time: 0.7828s\n",
      "Feature: 0030 Epoch: 1100 Loss: 1.1693858961 MSE_Loss: 1.0537608647 Sparsity_loss: 0.9389699962 KL_loss: 0.0523604304 MMD_loss: 0.0340764645 time: 0.7825s\n",
      "Feature: 0030 Epoch: 1200 Loss: 1.1693870772 MSE_Loss: 1.0537495059 Sparsity_loss: 0.9392206495 KL_loss: 0.0524113674 MMD_loss: 0.0340762106 time: 0.7809s\n",
      "Feature: 0030 Epoch: 1300 Loss: 1.1693880833 MSE_Loss: 1.0537521549 Sparsity_loss: 0.9391859445 KL_loss: 0.0524042055 MMD_loss: 0.0340762959 time: 0.7805s\n",
      "Feature: 0030 Epoch: 1400 Loss: 1.1693928159 MSE_Loss: 1.0537599001 Sparsity_loss: 0.9391300693 KL_loss: 0.0523871148 MMD_loss: 0.0340762687 time: 0.7813s\n",
      "Feature: 0030 Epoch: 1500 Loss: 1.1693892192 MSE_Loss: 1.0537464675 Sparsity_loss: 0.9393169767 KL_loss: 0.0524249417 MMD_loss: 0.0340763238 time: 0.7820s\n",
      "Feature: 0030 Epoch: 1600 Loss: 1.1693924069 MSE_Loss: 1.0537745440 Sparsity_loss: 0.9388516553 KL_loss: 0.0523193596 MMD_loss: 0.0340760405 time: 0.7824s\n",
      "Feature: 0030 Epoch: 1700 Loss: 1.1693888058 MSE_Loss: 1.0537471862 Sparsity_loss: 0.9392953485 KL_loss: 0.0524267749 MMD_loss: 0.0340762963 time: 0.7819s\n",
      "Feature: 0030 Epoch: 1800 Loss: 1.1693902050 MSE_Loss: 1.0537542063 Sparsity_loss: 0.9391910494 KL_loss: 0.0524002920 MMD_loss: 0.0340762212 time: 0.7819s\n",
      "Feature: 0030 Epoch: 1900 Loss: 1.1694016121 MSE_Loss: 1.0537657598 Sparsity_loss: 0.9391860328 KL_loss: 0.0523926654 MMD_loss: 0.0340763083 time: 0.7814s\n",
      "Begin training feature: 0031\n",
      "Feature: 0031 Epoch: 0000 Loss: 2.2183815113 MSE_Loss: 1.7165785624 Sparsity_loss: 8.2804748076 KL_loss: 0.0674127600 MMD_loss: 0.0435525360 time: 0.7817s\n",
      "Feature: 0031 Epoch: 0100 Loss: 1.2723514257 MSE_Loss: 1.1599511674 Sparsity_loss: 0.8371005994 KL_loss: 0.0491279706 MMD_loss: 0.0350269739 time: 0.7818s\n",
      "Feature: 0031 Epoch: 0200 Loss: 1.2723517490 MSE_Loss: 1.1599501405 Sparsity_loss: 0.8371342571 KL_loss: 0.0491345728 MMD_loss: 0.0350267759 time: 0.7822s\n",
      "Feature: 0031 Epoch: 0300 Loss: 1.2723544206 MSE_Loss: 1.1599518521 Sparsity_loss: 0.8371502395 KL_loss: 0.0491376501 MMD_loss: 0.0350268393 time: 0.7832s\n",
      "Feature: 0031 Epoch: 0400 Loss: 1.2723523986 MSE_Loss: 1.1599508200 Sparsity_loss: 0.8371260528 KL_loss: 0.0491351954 MMD_loss: 0.0350269638 time: 0.7827s\n",
      "Feature: 0031 Epoch: 0500 Loss: 1.2656556341 MSE_Loss: 1.1614019761 Sparsity_loss: 0.6737537739 KL_loss: 0.0462671611 MMD_loss: 0.0350516525 time: 0.7816s\n",
      "Feature: 0031 Epoch: 0600 Loss: 1.2656548378 MSE_Loss: 1.1614003313 Sparsity_loss: 0.6737699939 KL_loss: 0.0462670646 MMD_loss: 0.0350516661 time: 0.7813s\n",
      "Feature: 0031 Epoch: 0700 Loss: 1.2656540520 MSE_Loss: 1.1614007889 Sparsity_loss: 0.6737446928 KL_loss: 0.0462671868 MMD_loss: 0.0350516842 time: 0.7810s\n",
      "Feature: 0031 Epoch: 0800 Loss: 1.2656512747 MSE_Loss: 1.1613974964 Sparsity_loss: 0.6737601546 KL_loss: 0.0462704359 MMD_loss: 0.0350515341 time: 0.7825s\n",
      "Feature: 0031 Epoch: 0900 Loss: 1.2656522589 MSE_Loss: 1.1614005781 Sparsity_loss: 0.6737165708 KL_loss: 0.0462621131 MMD_loss: 0.0350516205 time: 0.7829s\n",
      "Feature: 0031 Epoch: 1000 Loss: 1.2590388497 MSE_Loss: 1.1629406116 Sparsity_loss: 0.5104684860 KL_loss: 0.0434068675 MMD_loss: 0.0350703808 time: 0.7823s\n",
      "Feature: 0031 Epoch: 1100 Loss: 1.2590267300 MSE_Loss: 1.1629308462 Sparsity_loss: 0.5104309419 KL_loss: 0.0434076572 MMD_loss: 0.0350701420 time: 0.7814s\n",
      "Feature: 0031 Epoch: 1200 Loss: 1.2590310113 MSE_Loss: 1.1629346730 Sparsity_loss: 0.5104368586 KL_loss: 0.0434073588 MMD_loss: 0.0350702205 time: 0.7810s\n",
      "Feature: 0031 Epoch: 1300 Loss: 1.2590292428 MSE_Loss: 1.1629329143 Sparsity_loss: 0.5104387018 KL_loss: 0.0434077036 MMD_loss: 0.0350701684 time: 0.7812s\n",
      "Feature: 0031 Epoch: 1400 Loss: 1.2590313916 MSE_Loss: 1.1629345191 Sparsity_loss: 0.5104461908 KL_loss: 0.0434076608 MMD_loss: 0.0350702545 time: 0.7813s\n",
      "Feature: 0031 Epoch: 1500 Loss: 1.2590309464 MSE_Loss: 1.1629341746 Sparsity_loss: 0.5104397747 KL_loss: 0.0434073666 MMD_loss: 0.0350703654 time: 0.7822s\n",
      "Feature: 0031 Epoch: 1600 Loss: 1.2590294786 MSE_Loss: 1.1629332621 Sparsity_loss: 0.5104350953 KL_loss: 0.0434074633 MMD_loss: 0.0350702096 time: 0.7827s\n",
      "Feature: 0031 Epoch: 1700 Loss: 1.2590293873 MSE_Loss: 1.1629332859 Sparsity_loss: 0.5104346049 KL_loss: 0.0434078909 MMD_loss: 0.0350701620 time: 0.7823s\n",
      "Feature: 0031 Epoch: 1800 Loss: 1.2590281960 MSE_Loss: 1.1629310986 Sparsity_loss: 0.5104509087 KL_loss: 0.0434081114 MMD_loss: 0.0350702424 time: 0.7827s\n",
      "Feature: 0031 Epoch: 1900 Loss: 1.2590507831 MSE_Loss: 1.1629501438 Sparsity_loss: 0.5105038466 KL_loss: 0.0434055515 MMD_loss: 0.0350707060 time: 0.7824s\n",
      "Begin training feature: 0032\n",
      "Feature: 0032 Epoch: 0000 Loss: 1.9093193676 MSE_Loss: 1.4184025813 Sparsity_loss: 8.2545443668 KL_loss: 0.0717111631 MMD_loss: 0.0387362321 time: 0.7822s\n",
      "Feature: 0032 Epoch: 0100 Loss: 1.1251668990 MSE_Loss: 0.9761662114 Sparsity_loss: 1.6743048686 KL_loss: 0.0665097468 MMD_loss: 0.0323101739 time: 0.7829s\n",
      "Feature: 0032 Epoch: 0200 Loss: 1.1103004557 MSE_Loss: 0.9796321143 Sparsity_loss: 1.3071159019 KL_loss: 0.0595485840 MMD_loss: 0.0323585328 time: 0.7844s\n",
      "Feature: 0032 Epoch: 0300 Loss: 1.1103022272 MSE_Loss: 0.9796359147 Sparsity_loss: 1.3070772434 KL_loss: 0.0595312715 MMD_loss: 0.0323585685 time: 0.7854s\n",
      "Feature: 0032 Epoch: 0400 Loss: 1.1102978791 MSE_Loss: 0.9796332423 Sparsity_loss: 1.3070548049 KL_loss: 0.0595441532 MMD_loss: 0.0323582329 time: 0.7846s\n",
      "Feature: 0032 Epoch: 0500 Loss: 1.1102960004 MSE_Loss: 0.9796320019 Sparsity_loss: 1.3070374033 KL_loss: 0.0595423201 MMD_loss: 0.0323583474 time: 0.8160s\n",
      "Feature: 0032 Epoch: 0600 Loss: 1.1102957182 MSE_Loss: 0.9796282098 Sparsity_loss: 1.3071010369 KL_loss: 0.0595560633 MMD_loss: 0.0323584466 time: 0.7835s\n",
      "Feature: 0032 Epoch: 0700 Loss: 1.1102949728 MSE_Loss: 0.9796311908 Sparsity_loss: 1.3070341152 KL_loss: 0.0595451130 MMD_loss: 0.0323583101 time: 0.7840s\n",
      "Feature: 0032 Epoch: 0800 Loss: 1.1102948589 MSE_Loss: 0.9796293868 Sparsity_loss: 1.3070669491 KL_loss: 0.0595496370 MMD_loss: 0.0323583097 time: 0.7838s\n",
      "Feature: 0032 Epoch: 0900 Loss: 1.1102991421 MSE_Loss: 0.9796327270 Sparsity_loss: 1.3070707623 KL_loss: 0.0595543884 MMD_loss: 0.0323586653 time: 0.7844s\n",
      "Feature: 0032 Epoch: 1000 Loss: 1.1103002648 MSE_Loss: 0.9796362421 Sparsity_loss: 1.3070400983 KL_loss: 0.0595362728 MMD_loss: 0.0323583297 time: 0.7849s\n",
      "Feature: 0032 Epoch: 1100 Loss: 1.1103041519 MSE_Loss: 0.9796351051 Sparsity_loss: 1.3071286165 KL_loss: 0.0595556790 MMD_loss: 0.0323585264 time: 0.7845s\n",
      "Feature: 0032 Epoch: 1200 Loss: 1.1103010706 MSE_Loss: 0.9796337191 Sparsity_loss: 1.3070978771 KL_loss: 0.0595584193 MMD_loss: 0.0323584363 time: 0.7831s\n",
      "Feature: 0032 Epoch: 1300 Loss: 1.1102929719 MSE_Loss: 0.9796296826 Sparsity_loss: 1.3070184536 KL_loss: 0.0595400964 MMD_loss: 0.0323584798 time: 0.7835s\n",
      "Feature: 0032 Epoch: 1400 Loss: 1.1102982910 MSE_Loss: 0.9796313115 Sparsity_loss: 1.3070957178 KL_loss: 0.0595472634 MMD_loss: 0.0323583653 time: 0.7845s\n",
      "Feature: 0032 Epoch: 1500 Loss: 1.1102965866 MSE_Loss: 0.9796325911 Sparsity_loss: 1.3070360603 KL_loss: 0.0595350875 MMD_loss: 0.0323584202 time: 0.7848s\n",
      "Feature: 0032 Epoch: 1600 Loss: 1.1102900980 MSE_Loss: 0.9796243235 Sparsity_loss: 1.3070706899 KL_loss: 0.0595555868 MMD_loss: 0.0323583410 time: 0.7833s\n",
      "Feature: 0032 Epoch: 1700 Loss: 1.1102913512 MSE_Loss: 0.9796278628 Sparsity_loss: 1.3070249165 KL_loss: 0.0595505259 MMD_loss: 0.0323583712 time: 0.7836s\n",
      "Feature: 0032 Epoch: 1800 Loss: 1.1102976301 MSE_Loss: 0.9796327745 Sparsity_loss: 1.3070491130 KL_loss: 0.0595452485 MMD_loss: 0.0323584730 time: 0.8165s\n",
      "Feature: 0032 Epoch: 1900 Loss: 1.1102971782 MSE_Loss: 0.9796319159 Sparsity_loss: 1.3070579179 KL_loss: 0.0595487189 MMD_loss: 0.0323584406 time: 0.7834s\n",
      "Begin training feature: 0033\n",
      "Feature: 0033 Epoch: 0000 Loss: 1.8576011612 MSE_Loss: 1.3624143155 Sparsity_loss: 8.2317290729 KL_loss: 0.0787552127 MMD_loss: 0.0414064170 time: 0.7843s\n",
      "Feature: 0033 Epoch: 0100 Loss: 1.2347344598 MSE_Loss: 1.0700019387 Sparsity_loss: 1.8173875371 KL_loss: 0.0664407829 MMD_loss: 0.0365993677 time: 0.7856s\n",
      "Feature: 0033 Epoch: 0200 Loss: 1.2278013750 MSE_Loss: 1.0712134751 Sparsity_loss: 1.6543726152 KL_loss: 0.0636396273 MMD_loss: 0.0366164304 time: 0.7846s\n",
      "Feature: 0033 Epoch: 0300 Loss: 1.2277984981 MSE_Loss: 1.0712111057 Sparsity_loss: 1.6543691385 KL_loss: 0.0636429845 MMD_loss: 0.0366162577 time: 0.7854s\n",
      "Feature: 0033 Epoch: 0400 Loss: 1.2277868443 MSE_Loss: 1.0712020397 Sparsity_loss: 1.6543205026 KL_loss: 0.0636555602 MMD_loss: 0.0366161173 time: 0.7835s\n",
      "Feature: 0033 Epoch: 0500 Loss: 1.2277909359 MSE_Loss: 1.0712054477 Sparsity_loss: 1.6543281969 KL_loss: 0.0636492838 MMD_loss: 0.0366162931 time: 0.7833s\n",
      "Feature: 0033 Epoch: 0600 Loss: 1.2277897649 MSE_Loss: 1.0712036456 Sparsity_loss: 1.6543438284 KL_loss: 0.0636559366 MMD_loss: 0.0366161777 time: 0.7835s\n",
      "Feature: 0033 Epoch: 0700 Loss: 1.2277878146 MSE_Loss: 1.0712035676 Sparsity_loss: 1.6543059153 KL_loss: 0.0636507070 MMD_loss: 0.0366162290 time: 0.7836s\n",
      "Feature: 0033 Epoch: 0800 Loss: 1.2277925844 MSE_Loss: 1.0712061226 Sparsity_loss: 1.6543507606 KL_loss: 0.0636539954 MMD_loss: 0.0366161950 time: 0.7843s\n",
      "Feature: 0033 Epoch: 0900 Loss: 1.2277856703 MSE_Loss: 1.0712021355 Sparsity_loss: 1.6542924990 KL_loss: 0.0636466899 MMD_loss: 0.0366162192 time: 0.7833s\n",
      "Feature: 0033 Epoch: 1000 Loss: 1.2277856839 MSE_Loss: 1.0712018258 Sparsity_loss: 1.6543001540 KL_loss: 0.0636528610 MMD_loss: 0.0366161603 time: 0.7825s\n",
      "Feature: 0033 Epoch: 1100 Loss: 1.2277987675 MSE_Loss: 1.0712111023 Sparsity_loss: 1.6543693527 KL_loss: 0.0636455955 MMD_loss: 0.0366163746 time: 0.7820s\n",
      "Feature: 0033 Epoch: 1200 Loss: 1.2277871061 MSE_Loss: 1.0712025256 Sparsity_loss: 1.6543126559 KL_loss: 0.0636516692 MMD_loss: 0.0366162147 time: 0.7815s\n",
      "Feature: 0033 Epoch: 1300 Loss: 1.2277908755 MSE_Loss: 1.0712053104 Sparsity_loss: 1.6543305675 KL_loss: 0.0636527953 MMD_loss: 0.0366162497 time: 0.7822s\n",
      "Feature: 0033 Epoch: 1400 Loss: 1.2277885826 MSE_Loss: 1.0712039169 Sparsity_loss: 1.6543124975 KL_loss: 0.0636531161 MMD_loss: 0.0366162471 time: 0.7839s\n",
      "Feature: 0033 Epoch: 1500 Loss: 1.2277869529 MSE_Loss: 1.0712004251 Sparsity_loss: 1.6543513687 KL_loss: 0.0636590637 MMD_loss: 0.0366161841 time: 0.7854s\n",
      "Feature: 0033 Epoch: 1600 Loss: 1.2277873257 MSE_Loss: 1.0712010909 Sparsity_loss: 1.6543493527 KL_loss: 0.0636511499 MMD_loss: 0.0366161279 time: 0.7848s\n",
      "Feature: 0033 Epoch: 1700 Loss: 1.2277912588 MSE_Loss: 1.0712075505 Sparsity_loss: 1.6542874665 KL_loss: 0.0636473477 MMD_loss: 0.0366164297 time: 0.7832s\n",
      "Feature: 0033 Epoch: 1800 Loss: 1.2277885577 MSE_Loss: 1.0712031337 Sparsity_loss: 1.6543289136 KL_loss: 0.0636543724 MMD_loss: 0.0366162143 time: 0.7835s\n",
      "Feature: 0033 Epoch: 1900 Loss: 1.2211495117 MSE_Loss: 1.0727190545 Sparsity_loss: 1.4910236522 KL_loss: 0.0607857937 MMD_loss: 0.0366357091 time: 0.7842s\n",
      "Begin training feature: 0034\n",
      "Feature: 0034 Epoch: 0000 Loss: 1.8778974421 MSE_Loss: 1.3776638387 Sparsity_loss: 8.3687058944 KL_loss: 0.0805413351 MMD_loss: 0.0404964372 time: 0.7852s\n",
      "Feature: 0034 Epoch: 0100 Loss: 1.2061493412 MSE_Loss: 1.1037062105 Sparsity_loss: 0.5717027670 KL_loss: 0.0453745042 MMD_loss: 0.0367021376 time: 0.7844s\n",
      "Feature: 0034 Epoch: 0200 Loss: 1.2061453303 MSE_Loss: 1.1036999188 Sparsity_loss: 0.5717474879 KL_loss: 0.0453726700 MMD_loss: 0.0367021606 time: 0.7875s\n",
      "Feature: 0034 Epoch: 0300 Loss: 1.2061435641 MSE_Loss: 1.1036967235 Sparsity_loss: 0.5717808702 KL_loss: 0.0453735927 MMD_loss: 0.0367020350 time: 0.7841s\n",
      "Feature: 0034 Epoch: 0400 Loss: 1.2061456434 MSE_Loss: 1.1036996494 Sparsity_loss: 0.5717635374 KL_loss: 0.0453735706 MMD_loss: 0.0367020320 time: 0.7840s\n",
      "Feature: 0034 Epoch: 0500 Loss: 1.2061432706 MSE_Loss: 1.1036989447 Sparsity_loss: 0.5717286686 KL_loss: 0.0453737605 MMD_loss: 0.0367020787 time: 0.7828s\n",
      "Feature: 0034 Epoch: 0600 Loss: 1.2061400662 MSE_Loss: 1.1036966413 Sparsity_loss: 0.5717157337 KL_loss: 0.0453744073 MMD_loss: 0.0367019469 time: 0.7843s\n",
      "Feature: 0034 Epoch: 0700 Loss: 1.2061379793 MSE_Loss: 1.1036940443 Sparsity_loss: 0.5717236928 KL_loss: 0.0453755562 MMD_loss: 0.0367019942 time: 0.7848s\n",
      "Feature: 0034 Epoch: 0800 Loss: 1.2061404880 MSE_Loss: 1.1036958906 Sparsity_loss: 0.5717312241 KL_loss: 0.0453742296 MMD_loss: 0.0367021472 time: 0.7843s\n",
      "Feature: 0034 Epoch: 0900 Loss: 1.2061420634 MSE_Loss: 1.1036966254 Sparsity_loss: 0.5717446102 KL_loss: 0.0453762199 MMD_loss: 0.0367022240 time: 0.7833s\n",
      "Feature: 0034 Epoch: 1000 Loss: 1.2061476504 MSE_Loss: 1.1037017696 Sparsity_loss: 0.5717592443 KL_loss: 0.0453751463 MMD_loss: 0.0367020842 time: 0.7831s\n",
      "Feature: 0034 Epoch: 1100 Loss: 1.2061428616 MSE_Loss: 1.1036977738 Sparsity_loss: 0.5717453330 KL_loss: 0.0453744068 MMD_loss: 0.0367020367 time: 0.7840s\n",
      "Feature: 0034 Epoch: 1200 Loss: 1.2061393631 MSE_Loss: 1.1036957163 Sparsity_loss: 0.5717148215 KL_loss: 0.0453718900 MMD_loss: 0.0367020906 time: 0.7844s\n",
      "Feature: 0034 Epoch: 1300 Loss: 1.2061418001 MSE_Loss: 1.1036980778 Sparsity_loss: 0.5717171197 KL_loss: 0.0453721649 MMD_loss: 0.0367020761 time: 0.7847s\n",
      "Feature: 0034 Epoch: 1400 Loss: 1.2061451817 MSE_Loss: 1.1036996102 Sparsity_loss: 0.5717534257 KL_loss: 0.0453760817 MMD_loss: 0.0367020685 time: 0.7854s\n",
      "Feature: 0034 Epoch: 1500 Loss: 1.2061390447 MSE_Loss: 1.1036950478 Sparsity_loss: 0.5717231065 KL_loss: 0.0453743846 MMD_loss: 0.0367020502 time: 0.7851s\n",
      "Feature: 0034 Epoch: 1600 Loss: 1.2061399885 MSE_Loss: 1.1036962656 Sparsity_loss: 0.5717183057 KL_loss: 0.0453729754 MMD_loss: 0.0367020463 time: 0.7852s\n",
      "Feature: 0034 Epoch: 1700 Loss: 1.2061431951 MSE_Loss: 1.1036993243 Sparsity_loss: 0.5717192179 KL_loss: 0.0453715954 MMD_loss: 0.0367020968 time: 0.7844s\n",
      "Feature: 0034 Epoch: 1800 Loss: 1.2061386674 MSE_Loss: 1.1036949603 Sparsity_loss: 0.5717124743 KL_loss: 0.0453732361 MMD_loss: 0.0367021742 time: 0.7841s\n",
      "Feature: 0034 Epoch: 1900 Loss: 1.2061419593 MSE_Loss: 1.1036962044 Sparsity_loss: 0.5717609276 KL_loss: 0.0453755203 MMD_loss: 0.0367019776 time: 0.7859s\n",
      "Begin training feature: 0035\n",
      "Feature: 0035 Epoch: 0000 Loss: 2.0238921220 MSE_Loss: 1.5285523805 Sparsity_loss: 8.2202049449 KL_loss: 0.0823894387 MMD_loss: 0.0417528052 time: 0.7850s\n",
      "Feature: 0035 Epoch: 0100 Loss: 1.2018982994 MSE_Loss: 1.0259360882 Sparsity_loss: 2.1436694785 KL_loss: 0.0720598728 MMD_loss: 0.0340290713 time: 0.7857s\n",
      "Feature: 0035 Epoch: 0200 Loss: 1.1946968896 MSE_Loss: 1.0268854235 Sparsity_loss: 1.9807001307 KL_loss: 0.0692888470 MMD_loss: 0.0340417829 time: 0.7837s\n",
      "Feature: 0035 Epoch: 0300 Loss: 1.1946858515 MSE_Loss: 1.0268706868 Sparsity_loss: 1.9807828393 KL_loss: 0.0693217204 MMD_loss: 0.0340414083 time: 0.7832s\n",
      "Feature: 0035 Epoch: 0400 Loss: 1.1946906991 MSE_Loss: 1.0268774342 Sparsity_loss: 1.9807358859 KL_loss: 0.0693110709 MMD_loss: 0.0340416788 time: 0.7837s\n",
      "Feature: 0035 Epoch: 0500 Loss: 1.1946877060 MSE_Loss: 1.0268784324 Sparsity_loss: 1.9806572244 KL_loss: 0.0692944090 MMD_loss: 0.0340417373 time: 0.7842s\n",
      "Feature: 0035 Epoch: 0600 Loss: 1.1946852547 MSE_Loss: 1.0268722803 Sparsity_loss: 1.9807300371 KL_loss: 0.0693152055 MMD_loss: 0.0340416605 time: 0.7846s\n",
      "Feature: 0035 Epoch: 0700 Loss: 1.1946913064 MSE_Loss: 1.0268900990 Sparsity_loss: 1.9804959735 KL_loss: 0.0692513889 MMD_loss: 0.0340419436 time: 0.7846s\n",
      "Feature: 0035 Epoch: 0800 Loss: 1.1673302786 MSE_Loss: 1.0321333356 Sparsity_loss: 1.3276053667 KL_loss: 0.0578876641 MMD_loss: 0.0341188994 time: 0.7844s\n",
      "Feature: 0035 Epoch: 0900 Loss: 1.1673315892 MSE_Loss: 1.0321374407 Sparsity_loss: 1.3275377690 KL_loss: 0.0578826034 MMD_loss: 0.0341192175 time: 0.7837s\n",
      "Feature: 0035 Epoch: 1000 Loss: 1.1673335697 MSE_Loss: 1.0321381515 Sparsity_loss: 1.3275618010 KL_loss: 0.0578853946 MMD_loss: 0.0341192386 time: 0.7835s\n",
      "Feature: 0035 Epoch: 1100 Loss: 1.1673317401 MSE_Loss: 1.0321375117 Sparsity_loss: 1.3275486668 KL_loss: 0.0578834121 MMD_loss: 0.0341189851 time: 0.7833s\n",
      "Feature: 0035 Epoch: 1200 Loss: 1.1673343038 MSE_Loss: 1.0321382458 Sparsity_loss: 1.3275805063 KL_loss: 0.0578849562 MMD_loss: 0.0341190939 time: 0.7830s\n",
      "Feature: 0035 Epoch: 1300 Loss: 1.1673320773 MSE_Loss: 1.0321362947 Sparsity_loss: 1.3275683484 KL_loss: 0.0578843694 MMD_loss: 0.0341192608 time: 0.7840s\n",
      "Feature: 0035 Epoch: 1400 Loss: 1.1673318812 MSE_Loss: 1.0321366440 Sparsity_loss: 1.3275681070 KL_loss: 0.0578859611 MMD_loss: 0.0341189868 time: 0.7842s\n",
      "Feature: 0035 Epoch: 1500 Loss: 1.1673361840 MSE_Loss: 1.0321425298 Sparsity_loss: 1.3275307568 KL_loss: 0.0578737237 MMD_loss: 0.0341191869 time: 0.7825s\n",
      "Feature: 0035 Epoch: 1600 Loss: 1.1673308822 MSE_Loss: 1.0321351154 Sparsity_loss: 1.3275819096 KL_loss: 0.0578891018 MMD_loss: 0.0341188868 time: 0.7826s\n",
      "Feature: 0035 Epoch: 1700 Loss: 1.1673349338 MSE_Loss: 1.0321395118 Sparsity_loss: 1.3275636072 KL_loss: 0.0578787276 MMD_loss: 0.0341192280 time: 0.7834s\n",
      "Feature: 0035 Epoch: 1800 Loss: 1.1673363070 MSE_Loss: 1.0321406458 Sparsity_loss: 1.3275666161 KL_loss: 0.0578825508 MMD_loss: 0.0341192507 time: 0.7836s\n",
      "Feature: 0035 Epoch: 1900 Loss: 1.1673327586 MSE_Loss: 1.0321371427 Sparsity_loss: 1.3275719579 KL_loss: 0.0578895744 MMD_loss: 0.0341190645 time: 0.7853s\n",
      "Begin training feature: 0036\n",
      "Feature: 0036 Epoch: 0000 Loss: 2.0111459652 MSE_Loss: 1.5206992472 Sparsity_loss: 8.2136250025 KL_loss: 0.0658555151 MMD_loss: 0.0395534533 time: 0.7853s\n",
      "Feature: 0036 Epoch: 0100 Loss: 1.2554152532 MSE_Loss: 1.1434149652 Sparsity_loss: 0.8571164827 KL_loss: 0.0494121736 MMD_loss: 0.0343251696 time: 0.7837s\n",
      "Feature: 0036 Epoch: 0200 Loss: 1.2490866131 MSE_Loss: 1.1434536814 Sparsity_loss: 0.7288892246 KL_loss: 0.0476080767 MMD_loss: 0.0343561948 time: 0.7839s\n",
      "Feature: 0036 Epoch: 0300 Loss: 1.2470429728 MSE_Loss: 1.1418985966 Sparsity_loss: 0.7192197600 KL_loss: 0.0471826584 MMD_loss: 0.0343557836 time: 0.7827s\n",
      "Feature: 0036 Epoch: 0400 Loss: 1.2466853367 MSE_Loss: 1.1421141643 Sparsity_loss: 0.7075938175 KL_loss: 0.0469185599 MMD_loss: 0.0343611403 time: 0.7833s\n",
      "Feature: 0036 Epoch: 0500 Loss: 1.2468569128 MSE_Loss: 1.1425868642 Sparsity_loss: 0.7021379312 KL_loss: 0.0468005424 MMD_loss: 0.0343475749 time: 0.7841s\n",
      "Feature: 0036 Epoch: 0600 Loss: 1.2407416950 MSE_Loss: 1.1442195317 Sparsity_loss: 0.5463687491 KL_loss: 0.0442930996 MMD_loss: 0.0343804003 time: 0.7854s\n",
      "Feature: 0036 Epoch: 0700 Loss: 1.2406833044 MSE_Loss: 1.1441702216 Sparsity_loss: 0.5459077728 KL_loss: 0.0442801026 MMD_loss: 0.0343874504 time: 0.7850s\n",
      "Feature: 0036 Epoch: 0800 Loss: 1.2406031995 MSE_Loss: 1.1440876428 Sparsity_loss: 0.5463698506 KL_loss: 0.0442902347 MMD_loss: 0.0343770828 time: 0.7829s\n",
      "Feature: 0036 Epoch: 0900 Loss: 1.2404307798 MSE_Loss: 1.1439217039 Sparsity_loss: 0.5460580897 KL_loss: 0.0442793874 MMD_loss: 0.0343816878 time: 0.7830s\n",
      "Feature: 0036 Epoch: 1000 Loss: 1.2407726246 MSE_Loss: 1.1442450044 Sparsity_loss: 0.5460288057 KL_loss: 0.0443423823 MMD_loss: 0.0343913777 time: 0.7831s\n",
      "Feature: 0036 Epoch: 1100 Loss: 1.2412067984 MSE_Loss: 1.1444017042 Sparsity_loss: 0.5510612530 KL_loss: 0.0444257285 MMD_loss: 0.0344038853 time: 0.7844s\n",
      "Feature: 0036 Epoch: 1200 Loss: 1.2407271274 MSE_Loss: 1.1441532619 Sparsity_loss: 0.5471595948 KL_loss: 0.0443316985 MMD_loss: 0.0343862893 time: 0.7847s\n",
      "Feature: 0036 Epoch: 1300 Loss: 1.2407151038 MSE_Loss: 1.1443965427 Sparsity_loss: 0.5422464843 KL_loss: 0.0441484572 MMD_loss: 0.0343823748 time: 0.7851s\n",
      "Feature: 0036 Epoch: 1400 Loss: 1.2400149620 MSE_Loss: 1.1434993721 Sparsity_loss: 0.5462946952 KL_loss: 0.0443032133 MMD_loss: 0.0343789111 time: 0.7837s\n",
      "Feature: 0036 Epoch: 1500 Loss: 1.2407230094 MSE_Loss: 1.1439782415 Sparsity_loss: 0.5504479642 KL_loss: 0.0444145081 MMD_loss: 0.0343891131 time: 0.7834s\n",
      "Feature: 0036 Epoch: 1600 Loss: 1.2402506599 MSE_Loss: 1.1439319332 Sparsity_loss: 0.5421839620 KL_loss: 0.0442159411 MMD_loss: 0.0343836825 time: 0.7826s\n",
      "Feature: 0036 Epoch: 1700 Loss: 1.2401763518 MSE_Loss: 1.1437070483 Sparsity_loss: 0.5456068282 KL_loss: 0.0442462898 MMD_loss: 0.0343732453 time: 0.7828s\n",
      "Feature: 0036 Epoch: 1800 Loss: 1.2407059556 MSE_Loss: 1.1441124991 Sparsity_loss: 0.5479091655 KL_loss: 0.0442916833 MMD_loss: 0.0343775453 time: 0.7849s\n",
      "Feature: 0036 Epoch: 1900 Loss: 1.2410625597 MSE_Loss: 1.1441856223 Sparsity_loss: 0.5526954464 KL_loss: 0.0444697562 MMD_loss: 0.0343987387 time: 0.7840s\n",
      "Begin training feature: 0037\n",
      "Feature: 0037 Epoch: 0000 Loss: 1.9204102936 MSE_Loss: 1.4192509523 Sparsity_loss: 8.4537391421 KL_loss: 0.0847321270 MMD_loss: 0.0388125311 time: 0.7842s\n",
      "Feature: 0037 Epoch: 0100 Loss: 1.2033078942 MSE_Loss: 1.0873632899 Sparsity_loss: 1.0007422680 KL_loss: 0.0520801726 MMD_loss: 0.0326933499 time: 0.7831s\n",
      "Feature: 0037 Epoch: 0200 Loss: 1.2033039369 MSE_Loss: 1.0873616134 Sparsity_loss: 1.0007038946 KL_loss: 0.0520758283 MMD_loss: 0.0326931871 time: 0.7827s\n",
      "Feature: 0037 Epoch: 0300 Loss: 1.2032984488 MSE_Loss: 1.0873562988 Sparsity_loss: 1.0007093798 KL_loss: 0.0520833869 MMD_loss: 0.0326929223 time: 0.7828s\n",
      "Feature: 0037 Epoch: 0400 Loss: 1.2033013875 MSE_Loss: 1.0873575497 Sparsity_loss: 1.0007337694 KL_loss: 0.0520860999 MMD_loss: 0.0326931448 time: 0.7841s\n",
      "Feature: 0037 Epoch: 0500 Loss: 1.2032999615 MSE_Loss: 1.0873563765 Sparsity_loss: 1.0007303108 KL_loss: 0.0520847593 MMD_loss: 0.0326931116 time: 0.7842s\n",
      "Feature: 0037 Epoch: 0600 Loss: 1.2032938720 MSE_Loss: 1.0873508302 Sparsity_loss: 1.0007211408 KL_loss: 0.0520874005 MMD_loss: 0.0326930517 time: 0.7850s\n",
      "Feature: 0037 Epoch: 0700 Loss: 1.2033022702 MSE_Loss: 1.0873587358 Sparsity_loss: 1.0007360721 KL_loss: 0.0520828304 MMD_loss: 0.0326929489 time: 0.7831s\n",
      "Feature: 0037 Epoch: 0800 Loss: 1.2033005281 MSE_Loss: 1.0873581616 Sparsity_loss: 1.0007141180 KL_loss: 0.0520889208 MMD_loss: 0.0326928811 time: 0.7827s\n",
      "Feature: 0037 Epoch: 0900 Loss: 1.2033024287 MSE_Loss: 1.0873586437 Sparsity_loss: 1.0007359061 KL_loss: 0.0520884003 MMD_loss: 0.0326930573 time: 0.7833s\n",
      "Feature: 0037 Epoch: 1000 Loss: 1.2032924672 MSE_Loss: 1.0873491530 Sparsity_loss: 1.0007310019 KL_loss: 0.0520921565 MMD_loss: 0.0326929223 time: 0.7830s\n",
      "Feature: 0037 Epoch: 1100 Loss: 1.2032977561 MSE_Loss: 1.0873566179 Sparsity_loss: 1.0006929576 KL_loss: 0.0520866051 MMD_loss: 0.0326928106 time: 0.7844s\n",
      "Feature: 0037 Epoch: 1200 Loss: 1.2033006134 MSE_Loss: 1.0873579647 Sparsity_loss: 1.0007139369 KL_loss: 0.0520855762 MMD_loss: 0.0326930464 time: 0.7841s\n",
      "Feature: 0037 Epoch: 1300 Loss: 1.2032966667 MSE_Loss: 1.0873539312 Sparsity_loss: 1.0007165157 KL_loss: 0.0520892653 MMD_loss: 0.0326930105 time: 0.7841s\n",
      "Feature: 0037 Epoch: 1400 Loss: 1.2032960103 MSE_Loss: 1.0873554590 Sparsity_loss: 1.0006746537 KL_loss: 0.0520875122 MMD_loss: 0.0326929681 time: 0.7833s\n",
      "Feature: 0037 Epoch: 1500 Loss: 1.2032948257 MSE_Loss: 1.0873536807 Sparsity_loss: 1.0006910819 KL_loss: 0.0520867114 MMD_loss: 0.0326928613 time: 0.7827s\n",
      "Feature: 0037 Epoch: 1600 Loss: 1.2032973404 MSE_Loss: 1.0873552033 Sparsity_loss: 1.0007079659 KL_loss: 0.0520874166 MMD_loss: 0.0326929273 time: 0.7828s\n",
      "Feature: 0037 Epoch: 1700 Loss: 1.2032942817 MSE_Loss: 1.0873484106 Sparsity_loss: 1.0007936411 KL_loss: 0.0520925613 MMD_loss: 0.0326926325 time: 0.7837s\n",
      "Feature: 0037 Epoch: 1800 Loss: 1.2032971774 MSE_Loss: 1.0873551686 Sparsity_loss: 1.0006996763 KL_loss: 0.0520864639 MMD_loss: 0.0326930811 time: 0.7840s\n",
      "Feature: 0037 Epoch: 1900 Loss: 1.2032956572 MSE_Loss: 1.0873533910 Sparsity_loss: 1.0007006835 KL_loss: 0.0520821927 MMD_loss: 0.0326932076 time: 0.7852s\n",
      "Begin training feature: 0038\n",
      "Feature: 0038 Epoch: 0000 Loss: 1.7569853294 MSE_Loss: 1.2710551412 Sparsity_loss: 8.1926685044 KL_loss: 0.0667638346 MMD_loss: 0.0378145557 time: 0.7838s\n",
      "Feature: 0038 Epoch: 0100 Loss: 1.0803718597 MSE_Loss: 0.9326176511 Sparsity_loss: 1.6544010971 KL_loss: 0.0636580704 MMD_loss: 0.0321987867 time: 0.7837s\n",
      "Feature: 0038 Epoch: 0200 Loss: 1.0803698331 MSE_Loss: 0.9326117955 Sparsity_loss: 1.6544845300 KL_loss: 0.0636778821 MMD_loss: 0.0321985121 time: 0.7825s\n",
      "Feature: 0038 Epoch: 0300 Loss: 1.0803668922 MSE_Loss: 0.9326120079 Sparsity_loss: 1.6544162744 KL_loss: 0.0636667835 MMD_loss: 0.0321986977 time: 0.7843s\n",
      "Feature: 0038 Epoch: 0400 Loss: 1.0803687105 MSE_Loss: 0.9326140375 Sparsity_loss: 1.6544170078 KL_loss: 0.0636711413 MMD_loss: 0.0321985547 time: 0.7843s\n",
      "Feature: 0038 Epoch: 0500 Loss: 1.0803640704 MSE_Loss: 0.9326097546 Sparsity_loss: 1.6544185545 KL_loss: 0.0636748089 MMD_loss: 0.0321983220 time: 0.7847s\n",
      "Feature: 0038 Epoch: 0600 Loss: 1.0803638727 MSE_Loss: 0.9326089824 Sparsity_loss: 1.6544271828 KL_loss: 0.0636753325 MMD_loss: 0.0321983918 time: 0.7844s\n",
      "Feature: 0038 Epoch: 0700 Loss: 1.0803877055 MSE_Loss: 0.9326288180 Sparsity_loss: 1.6544741422 KL_loss: 0.0636607029 MMD_loss: 0.0321992851 time: 0.7829s\n",
      "Feature: 0038 Epoch: 0800 Loss: 1.0803644951 MSE_Loss: 0.9326087798 Sparsity_loss: 1.6544383885 KL_loss: 0.0636719862 MMD_loss: 0.0321985389 time: 0.7828s\n",
      "Feature: 0038 Epoch: 0900 Loss: 1.0803644001 MSE_Loss: 0.9326102624 Sparsity_loss: 1.6544077955 KL_loss: 0.0636722827 MMD_loss: 0.0321985140 time: 0.7835s\n",
      "Feature: 0038 Epoch: 1000 Loss: 1.0803620076 MSE_Loss: 0.9326074787 Sparsity_loss: 1.6544152785 KL_loss: 0.0636791171 MMD_loss: 0.0321984846 time: 0.7839s\n",
      "Feature: 0038 Epoch: 1100 Loss: 1.0803601500 MSE_Loss: 0.9326064564 Sparsity_loss: 1.6544037590 KL_loss: 0.0636731906 MMD_loss: 0.0321983850 time: 0.7849s\n",
      "Feature: 0038 Epoch: 1200 Loss: 1.0803658615 MSE_Loss: 0.9326103922 Sparsity_loss: 1.6544283281 KL_loss: 0.0636776509 MMD_loss: 0.0321986373 time: 0.7843s\n",
      "Feature: 0038 Epoch: 1300 Loss: 1.0803627327 MSE_Loss: 0.9326090635 Sparsity_loss: 1.6543940502 KL_loss: 0.0636739637 MMD_loss: 0.0321986091 time: 0.7832s\n",
      "Feature: 0038 Epoch: 1400 Loss: 1.0803599576 MSE_Loss: 0.9326065549 Sparsity_loss: 1.6543933832 KL_loss: 0.0636748868 MMD_loss: 0.0321984963 time: 0.7829s\n",
      "Feature: 0038 Epoch: 1500 Loss: 1.0803660245 MSE_Loss: 0.9326118212 Sparsity_loss: 1.6544023692 KL_loss: 0.0636698464 MMD_loss: 0.0321986928 time: 0.7833s\n",
      "Feature: 0038 Epoch: 1600 Loss: 1.0803636546 MSE_Loss: 0.9326068472 Sparsity_loss: 1.6544663695 KL_loss: 0.0636799962 MMD_loss: 0.0321983439 time: 0.7838s\n",
      "Feature: 0038 Epoch: 1700 Loss: 1.0803664002 MSE_Loss: 0.9326108287 Sparsity_loss: 1.6544284836 KL_loss: 0.0636782498 MMD_loss: 0.0321986853 time: 0.7846s\n",
      "Feature: 0038 Epoch: 1800 Loss: 1.0803654790 MSE_Loss: 0.9326107087 Sparsity_loss: 1.6544159349 KL_loss: 0.0636736917 MMD_loss: 0.0321986196 time: 0.7849s\n",
      "Feature: 0038 Epoch: 1900 Loss: 1.0803659581 MSE_Loss: 0.9326116043 Sparsity_loss: 1.6544155939 KL_loss: 0.0636758912 MMD_loss: 0.0321984076 time: 0.7836s\n",
      "Begin training feature: 0039\n",
      "Feature: 0039 Epoch: 0000 Loss: 1.8541488693 MSE_Loss: 1.3637408161 Sparsity_loss: 8.1957328531 KL_loss: 0.0631475680 MMD_loss: 0.0399949645 time: 0.7828s\n",
      "Feature: 0039 Epoch: 0100 Loss: 1.1601708697 MSE_Loss: 1.0508244388 Sparsity_loss: 0.7554543728 KL_loss: 0.0488780981 MMD_loss: 0.0355424700 time: 0.7824s\n",
      "Feature: 0039 Epoch: 0200 Loss: 1.1601663262 MSE_Loss: 1.0508205242 Sparsity_loss: 0.7554367170 KL_loss: 0.0488771126 MMD_loss: 0.0355425933 time: 0.7838s\n",
      "Feature: 0039 Epoch: 0300 Loss: 1.1601659852 MSE_Loss: 1.0508456672 Sparsity_loss: 0.7549906132 KL_loss: 0.0487822389 MMD_loss: 0.0355414793 time: 0.7838s\n",
      "Feature: 0039 Epoch: 0400 Loss: 1.1680905653 MSE_Loss: 1.0634143896 Sparsity_loss: 0.6739052157 KL_loss: 0.0463038799 MMD_loss: 0.0352589382 time: 0.7837s\n",
      "Feature: 0039 Epoch: 0500 Loss: 1.1680891989 MSE_Loss: 1.0634136928 Sparsity_loss: 0.6738915111 KL_loss: 0.0463066634 MMD_loss: 0.0352589358 time: 0.7825s\n",
      "Feature: 0039 Epoch: 0600 Loss: 1.1680866118 MSE_Loss: 1.0634121420 Sparsity_loss: 0.6738706442 KL_loss: 0.0463035729 MMD_loss: 0.0352589471 time: 0.7814s\n",
      "Feature: 0039 Epoch: 0700 Loss: 1.1681044283 MSE_Loss: 1.0634262913 Sparsity_loss: 0.6739363965 KL_loss: 0.0462977041 MMD_loss: 0.0352591672 time: 0.7817s\n",
      "Feature: 0039 Epoch: 0800 Loss: 1.1680874545 MSE_Loss: 1.0634120771 Sparsity_loss: 0.6738865421 KL_loss: 0.0463054820 MMD_loss: 0.0352589999 time: 0.7822s\n",
      "Feature: 0039 Epoch: 0900 Loss: 1.1680848900 MSE_Loss: 1.0634105919 Sparsity_loss: 0.6738660569 KL_loss: 0.0463054562 MMD_loss: 0.0352589682 time: 0.7830s\n",
      "Feature: 0039 Epoch: 1000 Loss: 1.1680852167 MSE_Loss: 1.0634106982 Sparsity_loss: 0.6738734419 KL_loss: 0.0463055487 MMD_loss: 0.0352588967 time: 0.7837s\n",
      "Feature: 0039 Epoch: 1100 Loss: 1.1680864677 MSE_Loss: 1.0634111819 Sparsity_loss: 0.6738873162 KL_loss: 0.0463056601 MMD_loss: 0.0352589320 time: 0.7834s\n",
      "Feature: 0039 Epoch: 1200 Loss: 1.1680879857 MSE_Loss: 1.0634127629 Sparsity_loss: 0.6738821721 KL_loss: 0.0463055378 MMD_loss: 0.0352590297 time: 0.7830s\n",
      "Feature: 0039 Epoch: 1300 Loss: 1.1680860806 MSE_Loss: 1.0634108789 Sparsity_loss: 0.6738849825 KL_loss: 0.0463051850 MMD_loss: 0.0352589471 time: 0.7826s\n",
      "Feature: 0039 Epoch: 1400 Loss: 1.1680847852 MSE_Loss: 1.0634092738 Sparsity_loss: 0.6738923713 KL_loss: 0.0463061933 MMD_loss: 0.0352589199 time: 0.7823s\n",
      "Feature: 0039 Epoch: 1500 Loss: 1.1680891846 MSE_Loss: 1.0634137939 Sparsity_loss: 0.6738885649 KL_loss: 0.0463056305 MMD_loss: 0.0352589541 time: 0.7839s\n",
      "Feature: 0039 Epoch: 1600 Loss: 1.1680850417 MSE_Loss: 1.0634104508 Sparsity_loss: 0.6738708645 KL_loss: 0.0463065770 MMD_loss: 0.0352589901 time: 0.7840s\n",
      "Feature: 0039 Epoch: 1700 Loss: 1.1680845897 MSE_Loss: 1.0634096695 Sparsity_loss: 0.6738825071 KL_loss: 0.0463062216 MMD_loss: 0.0352588679 time: 0.7839s\n",
      "Feature: 0039 Epoch: 1800 Loss: 1.1680890993 MSE_Loss: 1.0634142059 Sparsity_loss: 0.6738814591 KL_loss: 0.0463044061 MMD_loss: 0.0352588854 time: 0.7830s\n",
      "Feature: 0039 Epoch: 1900 Loss: 1.1680867702 MSE_Loss: 1.0634101011 Sparsity_loss: 0.6739212048 KL_loss: 0.0463067751 MMD_loss: 0.0352587715 time: 0.7831s\n",
      "Begin training feature: 0040\n",
      "Feature: 0040 Epoch: 0000 Loss: 1.9531993504 MSE_Loss: 1.4510393988 Sparsity_loss: 8.3478017879 KL_loss: 0.0753347751 MMD_loss: 0.0420082634 time: 0.7833s\n",
      "Feature: 0040 Epoch: 0100 Loss: 1.2125104296 MSE_Loss: 1.0661345728 Sparsity_loss: 1.4903221840 KL_loss: 0.0605703560 MMD_loss: 0.0356270220 time: 0.7835s\n",
      "Feature: 0040 Epoch: 0200 Loss: 1.2125142798 MSE_Loss: 1.0661444264 Sparsity_loss: 1.4901959972 KL_loss: 0.0605465323 MMD_loss: 0.0356272944 time: 0.7837s\n",
      "Feature: 0040 Epoch: 0300 Loss: 1.1912700670 MSE_Loss: 1.0694092769 Sparsity_loss: 1.0007032956 KL_loss: 0.0520791440 MMD_loss: 0.0356524192 time: 0.7840s\n",
      "Feature: 0040 Epoch: 0400 Loss: 1.1912673546 MSE_Loss: 1.0694055821 Sparsity_loss: 1.0007195050 KL_loss: 0.0520846997 MMD_loss: 0.0356524762 time: 0.7842s\n",
      "Feature: 0040 Epoch: 0500 Loss: 1.1792568881 MSE_Loss: 1.0736517997 Sparsity_loss: 0.6739505937 KL_loss: 0.0463167668 MMD_loss: 0.0357221887 time: 0.7832s\n",
      "Feature: 0040 Epoch: 0600 Loss: 1.1792534333 MSE_Loss: 1.0736508415 Sparsity_loss: 0.6739043450 KL_loss: 0.0463147524 MMD_loss: 0.0357221157 time: 0.7828s\n",
      "Feature: 0040 Epoch: 0700 Loss: 1.1792521975 MSE_Loss: 1.0736498810 Sparsity_loss: 0.6738931499 KL_loss: 0.0463163696 MMD_loss: 0.0357222476 time: 0.7827s\n",
      "Feature: 0040 Epoch: 0800 Loss: 1.1792543666 MSE_Loss: 1.0736504046 Sparsity_loss: 0.6739126127 KL_loss: 0.0463159147 MMD_loss: 0.0357225809 time: 0.7837s\n",
      "Feature: 0040 Epoch: 0900 Loss: 1.1792548510 MSE_Loss: 1.0736518125 Sparsity_loss: 0.6739063331 KL_loss: 0.0463154715 MMD_loss: 0.0357222863 time: 0.7844s\n",
      "Feature: 0040 Epoch: 1000 Loss: 1.1792587238 MSE_Loss: 1.0736548644 Sparsity_loss: 0.6739199079 KL_loss: 0.0463155062 MMD_loss: 0.0357223583 time: 0.7848s\n",
      "Feature: 0040 Epoch: 1100 Loss: 1.1792691735 MSE_Loss: 1.0736610693 Sparsity_loss: 0.6740014825 KL_loss: 0.0463144365 MMD_loss: 0.0357224460 time: 0.7827s\n",
      "Feature: 0040 Epoch: 1200 Loss: 1.1792517712 MSE_Loss: 1.0736492752 Sparsity_loss: 0.6738966092 KL_loss: 0.0463160180 MMD_loss: 0.0357222523 time: 0.7822s\n",
      "Feature: 0040 Epoch: 1300 Loss: 1.1792482070 MSE_Loss: 1.0736462051 Sparsity_loss: 0.6738822641 KL_loss: 0.0463155927 MMD_loss: 0.0357223596 time: 0.7825s\n",
      "Feature: 0040 Epoch: 1400 Loss: 1.1792528697 MSE_Loss: 1.0736493551 Sparsity_loss: 0.6739163406 KL_loss: 0.0463167186 MMD_loss: 0.0357222633 time: 0.7836s\n",
      "Feature: 0040 Epoch: 1500 Loss: 1.1792536453 MSE_Loss: 1.0736509546 Sparsity_loss: 0.6739050278 KL_loss: 0.0463151015 MMD_loss: 0.0357221431 time: 0.7843s\n",
      "Feature: 0040 Epoch: 1600 Loss: 1.1792574902 MSE_Loss: 1.0736538413 Sparsity_loss: 0.6739214681 KL_loss: 0.0463150102 MMD_loss: 0.0357222153 time: 0.7839s\n",
      "Feature: 0040 Epoch: 1700 Loss: 1.1792531247 MSE_Loss: 1.0736503933 Sparsity_loss: 0.6739064938 KL_loss: 0.0463146651 MMD_loss: 0.0357221233 time: 0.7832s\n",
      "Feature: 0040 Epoch: 1800 Loss: 1.1792643304 MSE_Loss: 1.0736598440 Sparsity_loss: 0.6739287942 KL_loss: 0.0463123112 MMD_loss: 0.0357224645 time: 0.7822s\n",
      "Feature: 0040 Epoch: 1900 Loss: 1.1792533285 MSE_Loss: 1.0736512481 Sparsity_loss: 0.6738854881 KL_loss: 0.0463153777 MMD_loss: 0.0357223213 time: 0.7829s\n",
      "Begin training feature: 0041\n",
      "Feature: 0041 Epoch: 0000 Loss: 1.9907033730 MSE_Loss: 1.4914830750 Sparsity_loss: 8.3563275760 KL_loss: 0.0773703573 MMD_loss: 0.0403151009 time: 0.7833s\n",
      "Feature: 0041 Epoch: 0100 Loss: 1.1396262668 MSE_Loss: 1.0441479924 Sparsity_loss: 0.5717477565 KL_loss: 0.0453768095 MMD_loss: 0.0332185562 time: 0.7847s\n",
      "Feature: 0041 Epoch: 0200 Loss: 1.1396303207 MSE_Loss: 1.0441513378 Sparsity_loss: 0.5717520887 KL_loss: 0.0453705992 MMD_loss: 0.0332188404 time: 0.7836s\n",
      "Feature: 0041 Epoch: 0300 Loss: 1.1396250808 MSE_Loss: 1.0441466185 Sparsity_loss: 0.5717441274 KL_loss: 0.0453763663 MMD_loss: 0.0332187527 time: 0.7840s\n",
      "Feature: 0041 Epoch: 0400 Loss: 1.1396427509 MSE_Loss: 1.0441617830 Sparsity_loss: 0.5717888942 KL_loss: 0.0453777973 MMD_loss: 0.0332188693 time: 0.7827s\n",
      "Feature: 0041 Epoch: 0500 Loss: 1.1396238457 MSE_Loss: 1.0441452868 Sparsity_loss: 0.5717516308 KL_loss: 0.0453772037 MMD_loss: 0.0332185986 time: 0.7830s\n",
      "Feature: 0041 Epoch: 0600 Loss: 1.1396231138 MSE_Loss: 1.0441446553 Sparsity_loss: 0.5717456831 KL_loss: 0.0453791932 MMD_loss: 0.0332186863 time: 0.7821s\n",
      "Feature: 0041 Epoch: 0700 Loss: 1.1396197654 MSE_Loss: 1.0441415672 Sparsity_loss: 0.5717458197 KL_loss: 0.0453789265 MMD_loss: 0.0332185552 time: 0.7836s\n",
      "Feature: 0041 Epoch: 0800 Loss: 1.1396251857 MSE_Loss: 1.0441476175 Sparsity_loss: 0.5717282401 KL_loss: 0.0453766413 MMD_loss: 0.0332186960 time: 0.7841s\n",
      "Feature: 0041 Epoch: 0900 Loss: 1.1396194138 MSE_Loss: 1.0441426921 Sparsity_loss: 0.5717077866 KL_loss: 0.0453713006 MMD_loss: 0.0332188057 time: 0.7841s\n",
      "Feature: 0041 Epoch: 1000 Loss: 1.1396205629 MSE_Loss: 1.0441432248 Sparsity_loss: 0.5717309208 KL_loss: 0.0453766450 MMD_loss: 0.0332185092 time: 0.7830s\n",
      "Feature: 0041 Epoch: 1100 Loss: 1.1396230278 MSE_Loss: 1.0441448885 Sparsity_loss: 0.5717418405 KL_loss: 0.0453802374 MMD_loss: 0.0332186201 time: 0.7821s\n",
      "Feature: 0041 Epoch: 1200 Loss: 1.1523666503 MSE_Loss: 1.0595120063 Sparsity_loss: 0.5104414572 KL_loss: 0.0434082031 MMD_loss: 0.0334492531 time: 0.7830s\n",
      "Feature: 0041 Epoch: 1300 Loss: 1.1523673897 MSE_Loss: 1.0595126507 Sparsity_loss: 0.5104376530 KL_loss: 0.0434082507 MMD_loss: 0.0334493972 time: 0.7838s\n",
      "Feature: 0041 Epoch: 1400 Loss: 1.1523709003 MSE_Loss: 1.0595161357 Sparsity_loss: 0.5104352432 KL_loss: 0.0434079523 MMD_loss: 0.0334494729 time: 0.7835s\n",
      "Feature: 0041 Epoch: 1500 Loss: 1.1523820781 MSE_Loss: 1.0595238503 Sparsity_loss: 0.5105030937 KL_loss: 0.0434075096 MMD_loss: 0.0334495055 time: 0.7833s\n",
      "Feature: 0041 Epoch: 1600 Loss: 1.1523677103 MSE_Loss: 1.0595131637 Sparsity_loss: 0.5104346705 KL_loss: 0.0434080872 MMD_loss: 0.0334493784 time: 0.7830s\n",
      "Feature: 0041 Epoch: 1700 Loss: 1.1523642895 MSE_Loss: 1.0595105637 Sparsity_loss: 0.5104236301 KL_loss: 0.0434086469 MMD_loss: 0.0334492401 time: 0.7826s\n",
      "Feature: 0041 Epoch: 1800 Loss: 1.1523688209 MSE_Loss: 1.0595143513 Sparsity_loss: 0.5104327594 KL_loss: 0.0434084686 MMD_loss: 0.0334493818 time: 0.7825s\n",
      "Feature: 0041 Epoch: 1900 Loss: 1.1523702937 MSE_Loss: 1.0595151835 Sparsity_loss: 0.5104422637 KL_loss: 0.0434079492 MMD_loss: 0.0334494629 time: 0.7821s\n",
      "Begin training feature: 0042\n",
      "Feature: 0042 Epoch: 0000 Loss: 1.9691798068 MSE_Loss: 1.4760293048 Sparsity_loss: 8.2819838946 KL_loss: 0.0763876755 MMD_loss: 0.0391437111 time: 0.7843s\n",
      "Feature: 0042 Epoch: 0100 Loss: 1.2144446516 MSE_Loss: 1.0905213696 Sparsity_loss: 1.1230649646 KL_loss: 0.0559581308 MMD_loss: 0.0336052240 time: 0.7835s\n",
      "Feature: 0042 Epoch: 0200 Loss: 1.2068917193 MSE_Loss: 1.0921398688 Sparsity_loss: 0.9395147185 KL_loss: 0.0524868682 MMD_loss: 0.0336256203 time: 0.7834s\n",
      "Feature: 0042 Epoch: 0300 Loss: 1.2068941804 MSE_Loss: 1.0921456037 Sparsity_loss: 0.9394543858 KL_loss: 0.0524725494 MMD_loss: 0.0336255676 time: 0.7818s\n",
      "Feature: 0042 Epoch: 0400 Loss: 1.2068866552 MSE_Loss: 1.0921370651 Sparsity_loss: 0.9394828141 KL_loss: 0.0524842477 MMD_loss: 0.0336253096 time: 0.7826s\n",
      "Feature: 0042 Epoch: 0500 Loss: 1.2068839979 MSE_Loss: 1.0921355991 Sparsity_loss: 0.9394589112 KL_loss: 0.0524773158 MMD_loss: 0.0336253356 time: 0.7832s\n",
      "Feature: 0042 Epoch: 0600 Loss: 1.2068826760 MSE_Loss: 1.0921348115 Sparsity_loss: 0.9394419140 KL_loss: 0.0524757125 MMD_loss: 0.0336255063 time: 0.7843s\n",
      "Feature: 0042 Epoch: 0700 Loss: 1.2068795064 MSE_Loss: 1.0921340419 Sparsity_loss: 0.9394018280 KL_loss: 0.0524755545 MMD_loss: 0.0336253109 time: 0.7848s\n",
      "Feature: 0042 Epoch: 0800 Loss: 1.2068826715 MSE_Loss: 1.0921356090 Sparsity_loss: 0.9394424618 KL_loss: 0.0524712722 MMD_loss: 0.0336251132 time: 0.7843s\n",
      "Feature: 0042 Epoch: 0900 Loss: 1.2068833332 MSE_Loss: 1.0921350823 Sparsity_loss: 0.9394539240 KL_loss: 0.0524812731 MMD_loss: 0.0336253713 time: 0.7849s\n",
      "Feature: 0042 Epoch: 1000 Loss: 1.2068861572 MSE_Loss: 1.0921378075 Sparsity_loss: 0.9394513006 KL_loss: 0.0524853870 MMD_loss: 0.0336254652 time: 0.7831s\n",
      "Feature: 0042 Epoch: 1100 Loss: 1.2068811783 MSE_Loss: 1.0921290026 Sparsity_loss: 0.9395292024 KL_loss: 0.0524954666 MMD_loss: 0.0336253826 time: 0.7833s\n",
      "Feature: 0042 Epoch: 1200 Loss: 1.2068887557 MSE_Loss: 1.0921407561 Sparsity_loss: 0.9394514523 KL_loss: 0.0524762521 MMD_loss: 0.0336253309 time: 0.7846s\n",
      "Feature: 0042 Epoch: 1300 Loss: 1.2068823025 MSE_Loss: 1.0921358398 Sparsity_loss: 0.9394227794 KL_loss: 0.0524717314 MMD_loss: 0.0336253064 time: 0.7855s\n",
      "Feature: 0042 Epoch: 1400 Loss: 1.2068872580 MSE_Loss: 1.0921379901 Sparsity_loss: 0.9394707936 KL_loss: 0.0524866725 MMD_loss: 0.0336254265 time: 0.7851s\n",
      "Feature: 0042 Epoch: 1500 Loss: 1.2068873757 MSE_Loss: 1.0921391468 Sparsity_loss: 0.9394470634 KL_loss: 0.0524865697 MMD_loss: 0.0336255056 time: 0.7847s\n",
      "Feature: 0042 Epoch: 1600 Loss: 1.2068795539 MSE_Loss: 1.0921312405 Sparsity_loss: 0.9394601161 KL_loss: 0.0524836607 MMD_loss: 0.0336252374 time: 0.7827s\n",
      "Feature: 0042 Epoch: 1700 Loss: 1.2068866091 MSE_Loss: 1.0921391981 Sparsity_loss: 0.9394364863 KL_loss: 0.0524750116 MMD_loss: 0.0336254156 time: 0.7826s\n",
      "Feature: 0042 Epoch: 1800 Loss: 1.2068839103 MSE_Loss: 1.0921377253 Sparsity_loss: 0.9394194031 KL_loss: 0.0524779106 MMD_loss: 0.0336252189 time: 0.7829s\n",
      "Feature: 0042 Epoch: 1900 Loss: 1.2068947531 MSE_Loss: 1.0921450529 Sparsity_loss: 0.9394824603 KL_loss: 0.0524802609 MMD_loss: 0.0336253877 time: 0.7844s\n",
      "Begin training feature: 0043\n",
      "Feature: 0043 Epoch: 0000 Loss: 1.7631466215 MSE_Loss: 1.2642219606 Sparsity_loss: 8.3434784443 KL_loss: 0.0800558472 MMD_loss: 0.0404750952 time: 0.8148s\n",
      "Feature: 0043 Epoch: 0100 Loss: 1.1601905555 MSE_Loss: 1.0042386304 Sparsity_loss: 1.6543399805 KL_loss: 0.0636279643 MMD_loss: 0.0362993198 time: 0.8157s\n",
      "Feature: 0043 Epoch: 0200 Loss: 1.1534108787 MSE_Loss: 1.0056138921 Sparsity_loss: 1.4909913691 KL_loss: 0.0607763108 MMD_loss: 0.0363198241 time: 0.8154s\n",
      "Feature: 0043 Epoch: 0300 Loss: 1.1534142543 MSE_Loss: 1.0056152298 Sparsity_loss: 1.4910296715 KL_loss: 0.0607758459 MMD_loss: 0.0363198939 time: 0.8143s\n",
      "Feature: 0043 Epoch: 0400 Loss: 1.1534090815 MSE_Loss: 1.0056135281 Sparsity_loss: 1.4909656033 KL_loss: 0.0607770596 MMD_loss: 0.0363197532 time: 0.8142s\n",
      "Feature: 0043 Epoch: 0500 Loss: 1.1534107501 MSE_Loss: 1.0056144656 Sparsity_loss: 1.4909783632 KL_loss: 0.0607774368 MMD_loss: 0.0363197910 time: 0.8150s\n",
      "Feature: 0043 Epoch: 0600 Loss: 1.1534094916 MSE_Loss: 1.0056129302 Sparsity_loss: 1.4909855248 KL_loss: 0.0607791502 MMD_loss: 0.0363197423 time: 0.8150s\n",
      "Feature: 0043 Epoch: 0700 Loss: 1.1534102729 MSE_Loss: 1.0056151235 Sparsity_loss: 1.4909589396 KL_loss: 0.0607765372 MMD_loss: 0.0363197174 time: 0.8148s\n",
      "Feature: 0043 Epoch: 0800 Loss: 1.1534111764 MSE_Loss: 1.0056157825 Sparsity_loss: 1.4909634741 KL_loss: 0.0607735774 MMD_loss: 0.0363197381 time: 0.8143s\n",
      "Feature: 0043 Epoch: 0900 Loss: 1.1534104389 MSE_Loss: 1.0056133199 Sparsity_loss: 1.4909976947 KL_loss: 0.0607766977 MMD_loss: 0.0363197355 time: 0.8142s\n"
     ]
    }
   ],
   "source": [
    "for idx in range(50):\n",
    "    print('Begin training feature: {:04d}'.format(idx + 1))\n",
    "    decoder_file = 'decoder' + str(idx) + '.pt'\n",
    "    decoder_file = os.path.join('/home/jing_xuzijian/crf/Intrer_VAE_result/fmri_sim4', decoder_file)\n",
    "    encoder_file = 'encoder' + str(idx) + '.pt'\n",
    "    encoder_file = os.path.join('/home/jing_xuzijian/crf/Intrer_VAE_result/fmri_sim4', encoder_file)\n",
    "\n",
    "    Inter_decoder = decoder(args.dims, args.decoder_hidden, args.time_step - 1, args.num_nodes, args.decoder_dropout, args.decoder_alpha)\n",
    "    Inter_decoder.load_state_dict(torch.load(decoder_file))\n",
    "    Inter_decoder = Inter_decoder.cuda()\n",
    "    Inter_decoder.eval()\n",
    "\n",
    "    Inter_encoder = encoder(init_adj, args.dims, args.encoder_hidden, args.dims, args.time_step - 1, args.encoder_dropout, args.encoder_alpha)\n",
    "    Inter_encoder = Inter_encoder.cuda()\n",
    "\n",
    "    optimizer = optim.Adam(Inter_encoder.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
    "    loss_mse = nn.MSELoss()\n",
    "    best_loss = np.inf\n",
    "    \n",
    "    for epoch in range(2000):\n",
    "        t = time.time()\n",
    "        Loss = []\n",
    "        MSE_loss = []\n",
    "        SPA_loss = []\n",
    "        KL_loss = []\n",
    "        MMD_loss = []\n",
    "        for batch_idx, data in enumerate(data_loader):\n",
    "            optimizer.zero_grad()\n",
    "            data = data.cuda()\n",
    "            target = data[:, idx, 1:, :]\n",
    "            inputs = data[:, :, :-1, :]\n",
    "\n",
    "            mu, log_var = Inter_encoder(inputs)  #Inter_encoder(inputs, adj)\n",
    "            sigma = torch.exp(log_var / 2)\n",
    "            # sigma2 = torch.exp(log_var2 / 2)\n",
    "            gamma = torch.randn(size = mu.size()).cuda()\n",
    "            # theta = torch.randn(size = mu2.size()).cuda()\n",
    "            gamma = mu + sigma * gamma\n",
    "            # theta = mu2 + sigma2 * theta\n",
    "            mask = torch.sigmoid(gamma) #* torch.sigmoid(theta) #* torch.sigmoid(theta + gamma)\n",
    "            # gamma = torch.sigmoid(gamma)\n",
    "            # theta = torch.sigmoid(theta)\n",
    "\n",
    "            inputs = mask_inputs(mask, inputs)\n",
    "            pred = Inter_decoder(inputs, idx)   #Inter_decoder(inputs, adj, idx)\n",
    "\n",
    "\n",
    "\n",
    "            mse_loss = loss_mse(pred, target)\n",
    "            spa_loss = loss_sparsity(mask, 'log_sum')\n",
    "            kl_loss = loss_divergence(mask, 'JS')\n",
    "            mmd_loss = loss_mmd(data[:, :, 1:, :], pred, idx)\n",
    "\n",
    "            loss = mse_loss + args.beta_sparsity * spa_loss + args.beta_kl * kl_loss + args.beta_mmd * mmd_loss\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            Loss.append(loss.item())\n",
    "            MSE_loss.append(mse_loss.item())\n",
    "            SPA_loss.append(spa_loss.item())\n",
    "            KL_loss.append(kl_loss.item())\n",
    "            MMD_loss.append(mmd_loss.item())\n",
    "        \n",
    "        # if epoch == 500:\n",
    "        #     optimizer.param_groups[0]['lr'] = args.lr/10\n",
    "\n",
    "        if epoch % 100 == 0:\n",
    "            print(  'Feature: {:04d}'.format(idx + 1),\n",
    "                    'Epoch: {:04d}'.format(epoch),\n",
    "                    'Loss: {:.10f}'.format(np.mean(Loss)),\n",
    "                    'MSE_Loss: {:.10f}'.format(np.mean(MSE_loss)),\n",
    "                    'Sparsity_loss: {:.10f}'.format(np.mean(SPA_loss)),\n",
    "                    'KL_loss: {:.10f}'.format(np.mean(KL_loss)),\n",
    "                    'MMD_loss: {:.10f}'.format(np.mean(MMD_loss)),\n",
    "                    'time: {:.4f}s'.format(time.time() - t))\n",
    "                \n",
    "        if np.mean(Loss) < best_loss:\n",
    "            best_loss = np.mean(Loss)\n",
    "            #M[idx, :] = \n",
    "            # gamma_matrix[idx, :] = gamma.squeeze().mean(dim=2).mean(dim=1)\n",
    "            # theta_matrix[idx, :] = theta.squeeze().mean(dim=2).mean(dim=1)\n",
    "            # torch.save({\n",
    "            #             'encoder_state_dict': Inter_encoder.state_dict(),\n",
    "            #             'decoder_state_dict': Inter_decoder.state_dict(),\n",
    "            #                 # 'adj' : adj\n",
    "\n",
    "            #             }, encoder_file)\n",
    "            torch.save(Inter_encoder.state_dict(), encoder_file)\n",
    "                # np.save(save_file + str(idx) + '.npy', mask.cpu().detach().numpy())\n",
    "\n",
    "            # print('Feature: {:04d}'.format(idx + 1),\n",
    "            #       'Epoch: {:04d}'.format(epoch),\n",
    "            #       'Loss: {:.10f}'.format(np.mean(Loss)),\n",
    "            #       'mse_loss: {:.10f}'.format(np.mean(MSE_loss)),\n",
    "            #       'Sparsity_loss: {:.10f}'.format(np.mean(SPA_loss)),\n",
    "            #       'KL_loss: {:.10f}'.format(np.mean(KL_loss)),\n",
    "            #       #'mmd_loss: {:.10f}'.format(np.mean(mmd_loss)),\n",
    "            #       # 'time: {:.4f}s'.format(time.time() - t), file=log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "causality_matrix = []\n",
    "total_gamma_matrix = []\n",
    "total_theta_matrix = []\n",
    "#init_adj = torch.eye(20)\n",
    "for idx in range(2):\n",
    "    encoder_file = 'encoder' + str(idx) + '.pt'\n",
    "    encoder_file = os.path.join('/home/omnisky/Public/ChenRongfa/fmri_sim4', encoder_file)\n",
    "    est_net = encoder(init_adj, args.dims, args.encoder_hidden, args.dims, args.time_step - 1, args.encoder_dropout, args.encoder_alpha)\n",
    "    est_net.load_state_dict(torch.load(encoder_file))\n",
    "    # est_net = est_net.to('cuda:1')\n",
    "    est_net.eval()\n",
    "    inputs = X_np[:, :, :-1, :]#.to('cuda:1')   #:-1和1:有什么区别\n",
    "    mu, log_var = est_net(inputs)\n",
    "    # mu = mu.cpu().detach()\n",
    "    # log_var = log_var.cpu().detach()\n",
    "    sigma = torch.exp(log_var / 2)\n",
    "    # sigma2 = torch.exp(log_var2 / 2)\n",
    "    gamma = torch.randn(size = mu.size())\n",
    "    # theta = torch.randn(size = mu1.size())\n",
    "    gamma = mu + sigma * gamma\n",
    "    # theta = mu2 + sigma2* theta\n",
    "    mask_matrix = torch.sigmoid(gamma) #* torch.sigmoid(theta)\n",
    "    mask_matrix = mask_matrix.squeeze()\n",
    "    causality_matrix.append(mask_matrix)\n",
    "    # gamma_matrix = torch.sigmoid(gamma)\n",
    "    # gamma_matrix = gamma_matrix.squeeze()\n",
    "    # total_gamma_matrix.append(gamma_matrix)\n",
    "    # theta_matrix = torch.sigmoid(theta)\n",
    "    # theta_matrix = theta_matrix.squeeze()\n",
    "    # total_theta_matrix.append(theta_matrix)\n",
    "\n",
    "# np.save('/home/omnisky/Public/ChenRongfa/Intrer_VAE_result/result_fmri_concat/causality_matrix_10',causality_matrix)\n",
    "\n",
    "causality_matrix = torch.stack(causality_matrix, dim=1)\n",
    "\n",
    "# total_gamma_matrix = torch.stack(total_gamma_matrix, dim=1)\n",
    "# total_theta_matrix = torch.stack(total_theta_matrix, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 50])\n"
     ]
    }
   ],
   "source": [
    "adj_gca = causality_matrix.mean(dim=3).mean(dim=0)\n",
    "np.save('/home/omnisky/Public/ChenRongfa/Intrer_VAE_result/result_fmri_concat/adj_gca_10',adj_gca.detach().numpy())\n",
    "# gamma_adj = total_gamma_matrix.mean(dim=3).mean(dim=0)\n",
    "# theta_adj = total_theta_matrix.mean(dim=3).mean(dim=0)\n",
    "print(adj_gca.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 50)\n"
     ]
    }
   ],
   "source": [
    "adj_gca = np.load('/home/omnisky/Public/ChenRongfa/Intrer_VAE_result/result_fmri_concat/adj_gca_1.npy')\n",
    "for idx in range(2,11):\n",
    "    file = '/home/omnisky/Public/ChenRongfa/Intrer_VAE_result/result_fmri_concat/adj_gca_' + str(idx) + '.npy'\n",
    "    adj = np.load(file)\n",
    "    adj_gca = np.concatenate((adj_gca, adj))\n",
    "\n",
    "print(adj_gca.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.9332, 'precision': 0.39928057553956836, 'recall': 1.0, 'F1': 0.5706940874035991, 'ROC_AUC': 0.9834696563453366, 'PR_AUC': 0.6693590152422391}\n"
     ]
    }
   ],
   "source": [
    "result, _ = evaluate_result(GC, adj_gca, 0.5)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9.9999756e-01 9.9985868e-01 9.7211050e-06 9.2710952e-06 9.9985385e-01\n",
      " 9.3774725e-06 9.3284125e-06 9.7708935e-06 9.5543246e-06 8.9184632e-06\n",
      " 9.5961177e-06 9.3959152e-06 9.6411231e-06 8.6593700e-06 9.5046353e-06\n",
      " 8.5324618e-06 9.4927373e-06 8.8207071e-06 9.2803066e-06 9.4160778e-06\n",
      " 9.1965385e-06 9.1515576e-06 9.2759246e-06 9.6836839e-06 9.8320315e-06\n",
      " 9.2732207e-06 9.3022054e-06 9.2657292e-06 9.5832602e-06 9.3807803e-06\n",
      " 9.1514676e-06 1.0003901e-05 8.9867190e-06 9.2027703e-06 9.3525477e-06\n",
      " 9.2896389e-06 9.1430720e-06 8.7794806e-06 9.5598398e-06 8.9996784e-06\n",
      " 8.7186336e-06 8.8284305e-06 9.1528018e-06 9.0607837e-06 9.3686122e-06\n",
      " 9.4386169e-06 9.4497600e-06 8.9849527e-06 9.0072836e-06 9.6226840e-06]\n"
     ]
    }
   ],
   "source": [
    "print(adj_gca[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(GC[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------Iter = 1000----------\n",
      "Loss = 8.797492\n",
      "Variable usage = 100.00%\n",
      "----------Iter = 2000----------\n",
      "Loss = 7.815337\n",
      "Variable usage = 100.00%\n",
      "----------Iter = 3000----------\n",
      "Loss = 6.890838\n",
      "Variable usage = 100.00%\n",
      "----------Iter = 4000----------\n",
      "Loss = 6.137646\n",
      "Variable usage = 40.00%\n"
     ]
    }
   ],
   "source": [
    "from contrast_models.clstm import cLSTM, train_model_ista\n",
    "X = torch.FloatTensor(X_np_ori[np.newaxis])\n",
    "X = X.cuda()\n",
    "clstm = cLSTM(X.shape[-1], hidden=100).cuda()\n",
    "train_loss_list = train_model_ista(\n",
    "    clstm, X, context=10, lam=0.30, lam_ridge=1e-2, lr=1e-3, max_iter=4000,\n",
    "    check_every=1000)\n",
    "adj_clstm = clstm.GC(threshold=False).cpu().data.numpy()\n",
    "result_clstm, _ = evaluate_result(GC, adj_clstm, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.79, 'precision': 0.5, 'recall': 0.9523809523809523, 'F1': 0.6557377049180327, 'ROC_AUC': 0.9171187462326702, 'PR_AUC': 0.7998405051892206}\n"
     ]
    }
   ],
   "source": [
    "result_clstm, _ = evaluate_result(GC, adj_clstm, 0)\n",
    "print(result_clstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0000,  0.1223,  0.0164, -0.0233,  0.1127, -0.0018, -0.0035,  0.0089,\n",
      "         -0.0077, -0.0020],\n",
      "        [ 0.1087,  0.0000,  0.1149,  0.0153,  0.0224,  0.0024, -0.0021,  0.0169,\n",
      "          0.0092,  0.0034],\n",
      "        [ 0.0107,  0.0833,  0.0000,  0.0898,  0.0205,  0.0006, -0.0168,  0.0802,\n",
      "          0.0171,  0.0068],\n",
      "        [-0.0295,  0.0133,  0.1332,  0.0000,  0.1441, -0.0039, -0.0055,  0.0334,\n",
      "          0.0107,  0.0026],\n",
      "        [ 0.1025,  0.0163,  0.0291,  0.1180,  0.0000, -0.0009,  0.0013,  0.0130,\n",
      "          0.0054,  0.0020],\n",
      "        [ 0.0033, -0.0040, -0.0006, -0.0124, -0.0041,  0.0000,  0.1226,  0.0161,\n",
      "         -0.0154,  0.1225],\n",
      "        [-0.0032, -0.0062, -0.0191, -0.0062,  0.0035,  0.1099,  0.0000,  0.1111,\n",
      "          0.0191,  0.0170],\n",
      "        [ 0.0071,  0.0206,  0.0835,  0.0219,  0.0126,  0.0144,  0.0867,  0.0000,\n",
      "          0.0969,  0.0197],\n",
      "        [-0.0039,  0.0053,  0.0193,  0.0080,  0.0010, -0.0139,  0.0170,  0.1131,\n",
      "          0.0000,  0.1099],\n",
      "        [-0.0013,  0.0053,  0.0064,  0.0053,  0.0053,  0.1157,  0.0176,  0.0264,\n",
      "          0.1202,  0.0000]])\n"
     ]
    }
   ],
   "source": [
    "print(init_adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_value = 1\n",
    "for i in range(len(GC)):\n",
    "    for j in range(i+1):\n",
    "        if i == j or j == len(GC[0])-1:\n",
    "            GC[i][j] = new_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 1. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 1. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 1. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "print(GC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = scio.loadmat('/home/jing_xuzijian/crf/fmri模拟/sims/sim2.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 10)\n"
     ]
    }
   ],
   "source": [
    "GC = data['net']\n",
    "shuju = data['ts']\n",
    "cc = GC.mean(axis=0)\n",
    "print(shuju.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "cc[cc>0] = 1\n",
    "cc[cc<=0] = 0\n",
    "print(cc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "ae5e1b78a42d3a00ffbad3028f2882e576ba2589a97bb9b799d13c9e22855bf5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
